Notes
=====

AWs Fundamentals
=================

The building Blocks of AWS : Availability Zones and Regions
============================================================

Okay.
Hello, Cloud Gurus, and welcome to this lecture.
In this lecture we're going to look
at the building blocks of AWS.
So we're going to look at things
like Availability Zones and Regions.
So just what we're going to look at in this lesson,
we're going to start with the AWS Global Infrastructure.
We'll then explore what Availability Zones are,
what data centers are, what Regions are,
what edge locations are,
and then we're going to go in and have a look
at the AWS Management Console as it looks today.
So why don't we start with the AWS Global Infrastructure?
And the Global Infrastructure consists of Regions
and Availability Zones.
And at the time of recording,
there's currently 24 Regions and 77 Availability Zones.
Now you never get tested on this in the exam.
It changes all the time.
There'll be more and more Regions added all the time,
as well as Availability Zones.
And you might be wondering what is the difference
between a Region and an Availability Zone?
Well, Regions are really simple.
A Region is a location, so it could be Cape Town,
in South Africa; or London, in England.
Or it could be us-east-1, in the United States,
or Sydney, Australia, etc., etc.
So you might then be wondering,
"Well, what's an Availability Zone?"
Well, think of an Availability Zone as a data center.
Now, if you haven't worked in tech before,
if you've never been to a data center,
you might be wondering what a data center is.
Well, a data center is simply a building
that's filled with servers.
That's all it is.
It's a building with rows and rows and rows of servers.
They're all inside data halls,
and you'll have multiple data halls per data center,
depending on the data center architecture.
Now an Availability Zone may be several
different data centers, but because they're close together,
they will be within a certain mile radius between each 1,
they will be counted as 1 Availability Zone.
So it doesn't necessarily mean it's just 1 data center--
there's always going to be multiple data centers--
but they will be close enough together to be grouped
into 1 Availability Zone.
So what's a Region?
Well, we covered it off,
but a Region is just a geographical area,
and each Region consists of 2 or more Availability Zones.
So in this Region, we've got 3 Availability Zones.
We've got Availability Zone A, Availability Zone B,
Availability Zone C, etc., etc.
So if you Google Regions and Availability Zones with AWS,
you'll get the following landing page.
And you can see here,
we've got all the different Regions inside North America.
And you can click up here and see it inside South America,
Europe and the Middle East, and Asia Pacific.
So there are Regions and Availability Zones everywhere.
So if we go over to North America, we can scroll down here,
and we can see that us-east-1 is Northern Virginia.
So this has 6 Availability Zones in it.
And it was launched in 2006.
And we can scroll down and have a read
about our Availability Zones.
And you'll see here, right at the end it says,
"Availability Zones are physically separated
by a meaningful distance, many kilometers,
from any other Availability Zone,
although all are within 100 kilometers,
or 60 miles, of each other."
So every Availability Zone--so take, for example,
in here we've got 6 Availability Zones.
They'll all be within 60 miles,
or 100 kilometers, of each other
because they're in the Northern Virginia Region.
So that's how Availability Zones work.
And you'll notice here, we've got edge locations,
and you'll notice our edge locations,
we've got many more than we do have Regions.
And you can actually see the edge locations
listed here for North America.
So what are edge locations?
Well, an edge location are basically just endpoints for AWS
that are used for caching content.
And typically this consists of things like CloudFront,
which is Amazon's content delivery network,
which we're going to learn about
a little bit later on in the course.
Essentially, all you need to know going into your exam
is that there are many more edge locations
than there are Regions.
And currently there are over 215 edge locations
around the world.
So that's all an edge location is.
So these are all the different service types that sit
on top of the AWS Global Infrastructure,
and I'm not going to read them all out,
but there's an awful lot.
And these are the service types.
So these are actually just the groups.
Underneath each of these service types
you have individual services.
So it can get a little bit overwhelming.
So what we'll do is we'll have a look
at the Amazon Management Console when I first started.
So I first started learning AWS in about 2014 is
when I started really looking at it.
And this is what it looked like in 2014.
And it was very simple.
So we had compute services,
storage and content delivery services, database services,
networking services, etc., etc.
And under Compute, we only had 2 services.
We had EC2 and Lambda.
And under storage content and delivery,
we just had 4 services.
So we had S3, Storage Gateway, Glacier, and CloudFront.
Let's have a look at what the AWS Management Console
looks like today.
So this is what the AWS Management Console looks like today.
And if we click on Services,
you can see Compute already has way more
than just 2 services.
We've now got EC2, Lightsail, Lambda, Batch,
Elastic Beanstalk, Serverless Application Repository,
AWS Outposts, and EC2 Image Builder.
And I can't even say it in 1 breath,
and that is just Compute.
If we scroll down, you can see all
these different AWS services.
So this is what the console looks like today.
Now, I'm a huge SpaceX fan. This is a real picture.
This is a picture of Elon Musk's Roadster
orbiting the earth.
It was launched by the Falcon Heavy.
I was actually lucky enough
to be at that launch with my son.
And I want you to look at the center console
of the Roadster, and it says, "DON'T PANIC!"
And that is from Hitchhiker's Guide to the Galaxy,
Douglas Adams' famous book.
So when you were looking at the AWS Console
and you saw all those different services,
please don't panic.
You only need to know a few of those services
to pass your exam.
You certainly don't need to know
all of those services in-depth.
To be honest, there's quite a few services that are there
that I don't even know what they do.
They're just names to me.
So you'll never know all services inside out with AWS.
It's just too vast.
But if we have a look at the different service types
that you need to know in order to pass your exam,
it's going to be these ones.
So we'll start from the bottom.
Compute is a massive area
and we will cover that in a lot of detail.
Same with storage and databases.
They are the bread and butter
of the Certified Solutions Architect - Associate exam.
You also need to know migration and transfer;
network and content delivery;
a little bit of management and governance;
analytics; security, identity, and compliance,
which is the very first service we're going to cover off
in the next section.
We then have application integration,
AWS cost management, and containers.
So you really just need to know
these different service types in order to pass your exam.
You don't need to know things like quantum technologies,
or satellite, or blockchain, or robotics.
That's just not covered
in the Solutions Architect - Associate exam.
So please don't panic. We're going to make it super easy.
We're going to cover each service type individually.
And then we're going to drill down
into each individual service in-depth
and you'll learn everything that you need to know
in order to pass your exam.
So at the end of every lecture
we're going to have our exam tips
and we're just going to reinforce what you've learned
in the end of each lecture.
So in this lecture let's have a look
at what we learned about.
So we learned that a Region is a physical location
in the world that consists
of 2 or more Availability Zones.
An Availability Zone is 1 or more discrete data centers,
each with redundant power, networking, and connectivity,
which can sometimes be housed in separate facilities.
And then we have our edge locations, which are endpoints
for AWS that are used for caching content,
and typically this consists of CloudFront,
which is Amazon's content delivery network.
And we will cover that off in more detail
as we go on for the rest of the course.
So that is it for this lecture, everyone.
You've done really well.
Congratulations on passing your first lecture.
If you have any questions, please let me know.
If not, feel free to move on to the next lecture. Thank you.


Who owns what in the clous ?
=============================

Okay. Hello, Cloud Gurus,
and welcome to this lecture.
In this lecture we're going to look at who owns
what in the cloud, and by owns,
we're talking about who is responsible
for what in the cloud.
So the first thing I'm going to do is give you an analogy
of renting a car,
then we're going to look at renting resources in the cloud,
and then we're going to look at the crux of this lecture,
which is the shared responsibility model.
And it comes up time and time again
in the Certified Solutions Architect -
Associate exam as well as the security specialty
and a whole bunch of other exams.
So if you do want to go on and do other exams with AWS,
you need to know the shared responsibility model inside out.
We're just going to touch upon it briefly in this lecture.
I would highly recommend by the time
you finish the end of this course,
that you go and actually read
the shared responsibility model on AWS's website,
and we'll have a link to it in the resources section.
And then we'll cover off our exam tips.
So I want you to think, when you go and rent a car,
who is responsible for what?
What is your responsibility,
and what is the car rental company's responsibility?
Well, at a high level, you're responsible for not to damage
the vehicle when you're in control of it.
You're responsible not to speed and get any speeding fines.
And you're responsible for paying
things like tolls, for example,
whereas the rental car's responsibility is the car itself.
So they have to be in charge of things
like the tire pressure.
They have to make sure there's enough air
in the tires to responsibly lend it to you,
and they have to make sure it has a full tank
of gas when they give it to you.
And their overall responsibility
is in the mechanics of the car.
There's no point giving you a car
if it's suddenly going to break down.
Let's say the, you know, the exhaust is about to fall off.
So they're responsible for the physical parts of the car,
and you're responsible for what you do with the car.
And it's very similar in the cloud.
So who is responsible for what
when you rent resources in the cloud?
What are you responsible for,
and what is Amazon responsible for?
And that's where the shared responsibility model comes in.
So let's talk about Amazon's responsibility.
They're responsible for security of the cloud.
So they're responsible for the Regions,
the Availability Zones, the edge locations.
They basically don't advertise, you know,
where the Availability Zones are,
where the data centers are,
because they don't want people
just rocking up at their data centers trying to get in.
They're responsible for the physical security
of the data centers.
So to get in, you have to go through security checkpoints.
There'll be chain link fencing
around the data centers, etc, etc.
They're responsible for the hardware
and physical assets within the data center.
They're responsible for the computers,
the storage, the database, the network,
and the overall software that runs the entire stack.
So things like the hypervisor, for example,
and this is basically just controls the virtualization
on top of the physical components of AWS's cloud.
So what are you responsible for?
You're responsible for what you do with the cloud.
You're responsible for the security in the cloud.
So you're responsible for your customer data.
You don't want to store that in a place
that's publicly accessible where anyone can download it.
If you do that, that's your fault--that's not Amazon's.
You're responsible for things like your platform,
the applications, identity and access management,
which we're going to cover off in the next section.
You're responsible for the configuration
of your operating systems on your virtual machines,
the network and firewall configurations.
You're responsible for client-side encryption
as well as server-side encryption.
And you're also responsible
for generally just protecting your network traffic.
If you don't come from a tech background,
a lot of these are new words to you.
Don't worry as we go throughout the course
it'll make a lot more sense.
That's why I recommend reading the shared
responsibility model at the end of the course.
But going into the exam, basically, this is our exam tips.
I want you to think, when you get a scenario question and
they're talking about who's responsible for what,
ask yourself can you do this yourself
in the AWS Management Console?
And if you can, then you're likely responsible.
For example, security groups, IAM users,
patching your EC2 operating systems,
patching your databases running on EC2, etc.
Because you can do this in the AWS console,
you are going to be responsible.
And again, if you don't understand
what any of those words mean, don't worry about it.
We are going to cover it off later on,
but essentially you're responsible
for the things that you can control. If you
can't control it, then AWS is most likely responsible.
So you can't control how AWS manage their data centers.
You can't control what security cameras are turned on.
You can't control the cabling
that they've put into their data centers
or the patching of their database servers
or their operating systems, etc.,
the security patching of that.
You can't control that.
So if you can't control that,
then most likely AWS is responsible.
So as you go into your exam,
if you do get a scenario-based question
talking about the shared responsibility model,
who's responsible for what,
I just want you to ask this 1 question in your head:
Can I do this myself in the AWS Management Console?
And if you can, then you're probably going to be responsible
and if not, then AWS is likely to be responsible.
And we have a shared responsibility
between the 2 of you, is encryption.
So you can do encryption. AWS can do encryption.
For you, you can encrypt your clients' data.
You can make sure that where you store
all your clients' data, that it is encrypted.
AWS, when you basically, you go in into the console
and click a button saying, please encrypt this volume.
When you click that, you are doing the encryption
on your side. AWS actually has to follow through
and make sure that that is encrypted.
So that's why encryption is a shared responsibility.
So keep all this in mind going into the exam.
If you have any questions, please let me know.
If not, feel free to move on to the next lecture.
Thank you.


Compute, storage, Databases, and Networking
===========================================

Okay. Hello, Cloud Gurus,
and welcome to this lecture.
In this lecture we're going to talk
about the key different services you need to know
in order to pass your exam.
So we're going to talk about compute, storage, databases,
and networking, and we'll start with compute.
So essentially compute, you wouldn't be able
to build an application without compute power.
So, you're going to need something to crunch the data.
And typically we think of compute as servers
or virtual machines.
It can also be things like serverless, things like Lambda,
which we're going to look at in a lot more detail,
but compute is just the way that we process our information.
So, some key services in compute
that we're going to cover off in this course
is EC2, which are essentially just virtual machines;
Lambda, which is a way of doing it serverlessly,
so there are no virtual machines--
all you worry about is your code.
We have Elastic Beanstalk, which is a provisioning engine.
So this is a way of basically automating the deployment
of your applications within AWS.
So you don't need to know a lot about AWS
to use Elastic Beanstalk.
Essentially, you just throw your code at it
and it will basically grow out the web servers,
load balancers, etc., that you need.
And those are the key compute services
for a Solutions Architect - Associate exam.
Once we've computed our data, we need somewhere to store it.
So storage. So think of storage
as a giant disk in the cloud.
It's a safe place to leave your information.
And 1 of the key services (in fact,
1 of the very first services with AWS)
is our Simple Storage Service,
which is sometimes--well, it's always abbreviated to S3.
You also have Elastic Block Store.
This is basically a virtual hard disk that we attach
to our virtual machines.
We're going to learn a lot more about that.
We have Elastic File Service,
which is a way of storing our files centrally.
We then have Fsx, and we have Storage Gateway.
And we're going to cover all this off in a lot more detail
as we go throughout the course.
So moving on to the next key technology
you need to understand going into your exam, are databases.
And the easiest way to think of a database
is simply a spreadsheet. It's a way of storing information.
It's a reliable way to store and retrieve information.
And there are various different services that come
under databases that we're going to cover off.
First is Relational Database Service (or RDS).
We then have DynamoDB, which is a non-relational database,
and we'll cover off what the differences are
in the database section.
And we then have Redshift, which is basically a database
warehousing technology.
Moving on from databases, we're going
to learn about networking.
Networking is just a way for our compute storage
and databases to communicate with each other
and even a place for them to live in.
And this is where networking comes in.
So we're going to learn about VPCs,
and VPCs are basically virtual data centers in the cloud.
This is where all our resources are going to live.
So our compute, our storage, our databases, etc.
We then have Direct Connect. Direct Connect is a way
of directly connecting your headquarters
or your on-premises data centers to AWS.
And we're going to learn about that in more detail.
Route 53 is a way of doing DNS.
So it's a way of registering domain names
and then pointing them at our web servers, for example.
We have API Gateway.
This is basically a serverless way
of replacing your web servers.
We're going to learn a lot more about that.
And then we have AWS Global Accelerator,
and this is a way of accelerating your audiences
towards your applications within AWS.
So that is networking.
So the key services to know for the exams are compute,
which consists of EC2, Lambda, and Elastic Beanstalk.
Storage, which consists of S3, EBS, EFS, Fsx,
and Storage Gateway.
Databases, we're going to cover off RDS, DynamoDB,
and Redshift.
And networking, we're going to cover off VPCs, Direct
Connect, Route 53, API Gateway, and AWS Global Accelerator.
So this will be throughout the entire course.
The first technology we're going to cover off
is Identity and Access Management.
And this is essential to know
because it's how you create users and groups
and grant access to people to use AWS.
Once we've learned that, we're going to go into storage.
So we're going to start with storage
and we're going to start with S3.
So that is it for this lecture, everyone.
If you have any questions, please let me know.
If not, feel free to move on to the next lecture. Thank you.


What is the Well-Architected Framework ?
========================================

Okay. Hello, Cloud Gurus.
And welcome to this lecture.
In this lecture, we're just going to briefly look
at the Well-Architected Framework.
So we're going to discuss whitepapers first
and then we'll go on
to the pillars of the Well-Architected Framework.
So you can get whitepapers from the following address.
Just go to aws.amazon.com/whitepapers
and there's literally hundreds of whitepapers available.
So this is the whitepaper website.
You can see in here we've got 427 whitepapers currently.
Don't worry. You don't need to read every whitepaper
in order to pass the exam.
And here you've got the methodology
and you can actually click
on the Well-Architected Framework
and that will load up 13 different whitepapers
that will talk about it.
The whitepaper you probably want to start with
is the AWS Well-Architected Framework Overview.
And then the Well-Architected Framework consists
of 5 different pillars that we're going to cover off
in a second.
Each pillar has its own whitepaper.
Now going into your exam, you really only need to read this.
I wouldn't read this just yet
because you won't understand what any
of the technologies are
because you haven't gone through the course.
You haven't learned what EC2 is.
You haven't learned what Direct Connect is
or Storage Gateway, etc., etc.
So I would leave this, reading this,
until the very end of the course,
but I would definitely 100% read this whitepaper
before going into your exam
because the exam is really structured
around the Well-Architected Framework.
If you want to be a Solutions Architect,
well, of course you want to architect things well
and this is the go-to whitepaper
in terms of architecting things well on the AWS platform.
So like I said, the most important whitepaper
for the exam is the Well-Architected Framework.
And you can get to it
by visiting that website we just looked at.
The Well-Architected Framework consists
of 5 different pillars.
So we've got operational excellence, and this focuses
on running and monitoring systems to deliver business value
and continually improving processes and procedures.
We've then got security,
which focuses on protecting information and systems.
We've then got reliability,
which focuses on ensuring a workload
performs its intended functions correctly and consistently
when it's expected to do so.
We've got performance efficiency, which focuses on using IT
and computing resources efficiently.
And then we have cost optimization,
which focuses on avoiding unnecessary costs.
So those are the 5 pillars
of the Well-Architected Framework.
So my exam tip is read the whitepaper,
but don't read it yet.
You won't understand half
of the technologies in there just yet.
Read it once you've finished this course
and you're about to take the exam.
It really is worth reading pretty much the day
or night before for the exam.
So that is it for this lecture, everyone.
If you have any questions, please let me know.
If not, feel free to move on to the next lecture.
Thank you.


AWS Fundamentals Exam Tips
===========================

Okay. Hello, Cloud Gurus,
and welcome to this lecture.
In this lecture we're just going to cover off everything
that we've just learned in section 2.
So, let's start with the building blocks of AWS.
What are our 3 tips for the AWS building blocks?
Well, we learned what a Region is.
A Region is a physical location in the world
that consists of 2 or more Availability Zones.
We then have an Availability Zone,
which is 1 or more discrete data centers,
each with redundant power, networking, and connectivity,
which is housed in separate facilities.
And then we learned about edge locations,
and edge locations are endpoints for AWS
that are used for caching content.
Typically, this consists of CloudFront,
which is Amazon's content delivery network.
And there's going to always be more edge locations
than there are Regions.
Edge locations are basically closer to your user.
A Region is, basically just makes up
a whole bunch of different Availability Zones.
You always have a minimum of 2 Availability Zones
in a Region.
Next, we're going to move on
to the shared responsibility model.
So who owns what in the cloud?
If you get a scenario-based question talking
about the shared responsibility model,
I just need you to ask yourself,
can you do this yourself in the AWS Management Console?
If yes, then you're likely responsible.
So things like security groups, creating users,
patching your EC2 operating system.
So patching your virtual machines with security patches,
patching your databases
that you're running on EC2 etc.
If you can't do this stuff in the console,
then AWS is most likely responsible for it.
So this could be things like management of the data centers,
preventing people from going into the data centers
and walking around.
It could be the installation of security cameras.
It could be the actual cabling within the data centers
connecting up your servers to your storage, for example.
It could be things like patching RDS or operating systems
that you can't actually touch.
It could be the operating systems
that run Lambda, for example.
If you can't do this yourself,
then most likely AWS is going to be responsible.
And then encryption is a shared responsibility.
So it's shared between you and AWS.
And then you also need to know the key services
to know for the exam.
So we covered off compute, and the key services
within compute are EC2, Lambda, and Elastic Beanstalk.
Storage, we're going to learn about S3, EBS, EFS, FSx,
and Storage Gateway.
Databases, we're gonna learn about RDS, DynamoDB,
and Redshift.
And networking, we're going to learn
about VPCs, Direct Connect, Route 53, API Gateway,
and AWS Global Accelerator.
And don't worry if all of this just sounds
like gibberish to you.
At the end of this course,
you understand what each service is intimately
and you'll be able to pass your exam, hopefully
on the first go.
So we're gonna cover off all these services
in the course as we move on.
And then finally, before you go and sit your exam,
remember to read the whitepaper.
This is the Well-Architected Framework whitepaper.
I would not read the whitepaper just yet.
The reason is you don't know half the services just yet.
However, you should definitely read the Well-Architected
Framework whitepaper when you finish this course
and you're about to take the exam.
So that is it for this section.
Let's move on to section 3,
where we're going to learn about identity access management,
which is a way of creating users and granting permissions
to use the AWS console.
So if you've got time,
please join me in the next section.
Thank you.



Identity and Access Management (IAM)
=====================================

Securing the Root Account
==========================

Okay. Hello, Cloud Gurus,
and welcome to this lecture.
In this lecture, we're going to look at
how we can secure our root account to AWS.
So we'll start with covering off what is IAM.
IAM just stands for identity access management.
Next, we'll go on to understanding what IAM is
and then we'll look at what the root account is.
Then, we'll look at how we can actually secure
the root account using the console, and then
we'll review everything that we've learned in this lesson.
So what is IAM?
Well, like I said,
IAM stands for identity access management,
and essentially it allows you to manage users
and their level of access to the AWS console.
It allows you to go ahead
and create users and grant permissions to those users.
You can also create groups and roles,
which we'll cover off in a little bit.
And you basically use IAM to control access
to your AWS resources.
So that's all that IAM is.
It's a way of granting access to AWS resources,
and, really, understanding IAM is essential
both for passing the exam
but also for working with AWS in real life.
If you're administrating a company's AWS account
in real life, you need to have
an intimate understanding of IAM.
And IAM all starts with the root account.
So what is the root account?
Well, this is just the email address you use
to sign up for AWS.
So the root account has full administrative access to AWS.
And for this reason, it's really important to
secure this account because if somebody gets your password
to this account and knows the email address,
they can go in and mine as much Bitcoin as they want.
They can literally use any service within the AWS console.
For that very reason,
it's super important to secure your root account.
So let's go ahead and have a look
at how we can do that in the AWS console.
Okay, so here I am in the AWS console.
Now you'll notice up here, we've got our Regions
and I'm going to start in us-east-1.
And throughout the rest of this course,
I'd recommend that you use us-east-1
simply because it is the default Region.
It's the very first Region
that AWS essentially roll out all their services to.
So if we are using a new service
and you can't see it, it probably is
because it hasn't been deployed in your Region.
If we all use us-east-1, then we have some consistency.
So make sure you select us-east-1.
So what we're going to do is we're going to
go over to Services and we're going to go ahead
and try and find IAM.
Now, if you ever get stuck,
you can type up here in the search button
for the service that you're looking for.
I know that IAM is under Security, Identity, and Compliance
and it's the very top one.
So I'm going to go ahead and click in here.
And if you've never used AWS before,
you're going to get a dashboard that looks like this.
You'll have zero users, zero groups,
and we'll cover that off a little bit later on.
And what we want to do is go in and secure the root account.
Now you can do it by clicking on some links in here.
If you just scroll down, I think it's on
rotate credentials regularly.
You could go in and click on that
and that will load you up with this page.
The easier way to get to it, though,
is by clicking on your username up here
and then clicking on My Security Credentials.
And this is the security credentials for your root account.
So this is what controls your root account.
Now the very first thing we're going to do
is go ahead and turn on multi-factor authentication.
So let's go ahead and hit Activate MFA, and we're
going to use a virtual multi-factor authentication device.
So this could be Google Authenticator.
I actually use Authy, which is a third-party platform.
Google Authenticator is owned by Google.
You can use whatever you want,
but you will need to download this to your smartphone.
So go ahead and hit Continue.
And then what you're going to do is
once you've downloaded it on your smartphone,
so it could be Google authenticator or Authy,
which is spelled A-U-T-H-Y,
You're going to go in and add an account.
And then what you're going to do is just point your phone
at this QR code, go ahead and scan it,
and that will give you 2 codes.
You'll get 1 code, and then you have to wait
for the timer to basically time out,
which is normally about 30 seconds,
and then you enter in the second code in here.
So I'm just going to go ahead and do that now.
So my first code is 828400,
and then my second code is 526310.
And then I'm going to go ahead and hit Assign MFA.
And that has now successfully assigned MFA,
or multi-factor authentication,
and I now have this enabled on my root account.
Now, if you ever want to go in and remove it,
you just click on Manage and you can Remove.
You can also go in and re-sync if you need to.
So I'm just going to leave this as default.
And so that's it.
It's very simple.
Let's move on to my exam tips.
So onto my exam tips.
And if you're asked in the exam
how to go ahead and secure the root account,
always remember to turn on multi-factor authentication.
That's critical because remember the root account
has full administrative access to the AWS console.
So you always want to turn on 2-factor
or multi-factor authentication on the root account.
The other ways you can secure the root account is
by creating an administrative group for your administrators
and then assigning the appropriate permissions to the group.
You can also create user accounts
for your administrators and then add them to the group.
And we are going to look at how to do that
in a couple of lectures from now, but in the next lecture
we're just going to learn about permissions
and how they work in identity access management.
Once you get a handle on how permissions work,
then we'll learn how we can assign permissions
to things like groups, users, and roles.
So if you've got the time,
please join me in the next lecture.
Thank you.


Controlling User's Actions with IAM Policy Documents
====================================================

Okay. Hello, Cloud Gurus,
and welcome to this lecture.
In this lecture, we're going to look at
how we can control our users' actions
with IAM policy documents.
So the first thing we're going to look at
is how permissions work with IAM,
then we're going to look at some JSON examples
of policy documents,
then we're going to look at how policy documents
actually work.
Then we'll go in and view some policy documents
within the console,
and then we'll do our exam tips.
So let's start with permissions.
How do we control permissions
using identity access management?
Well, we assign our permissions using policy documents,
which are made up of JSON,
which stands for JavaScript Object Notation.
And this is a good example of a policy document,
and you will need to know how to read and understand
policy documents in your exam.
So in here, we've got a statement,
and this is a JSON statement.
So you've got the version,
and then we've got the statement itself.
And then inside the statement,
we've got a number of different key pairs.
So we've got the effect, which is allow.
So basically we're allowing,
the action is what,
and then it's a wildcard,
so we're allowing everything,
and the resource is a wildcard as well.
So that just simply means everything.
So this gives you full administrator access.
This is saying, allow the action of everything
on every resource.
So this allows
the person who this policy document's attached to,
or the group that this policy document's attached to,
the ability to do everything with every resource.
So that's the most basic policy document that you'll see.
This gives you full admin access to the AWS console.
And what we typically will do,
is we will take our policy documents
and we'll assign them to groups.
We can also assign them to users,
and we can also assign them to roles.
And we'll look at best practices going forward.
Typically, you would not assign a policy document
directly to a user,
because that can become difficult to manage.
What you would do is add the user into a group,
and then assign the policy documents to a group,
and what happens is,
is the user inherits the permissions from the group.
But all the permissions that are controlled
within AWS come back to IAM policy documents.
So let's go ahead and have a look at
how this works in the AWS console.
So I'm back in the AWS console.
Again, note up here, my Region is Northern Virginia.
If we go over to Services,
and we go down to Identity Access Management
(which is under Security, Identity, and Compliance),
and we click in here, note what happens to the Region.
It changes to global.
And you see here it says that,
IAM does not require Region selection.
So when you go ahead and create things like
users, and groups, and policies, etc., etc.,
that's happening on a global level.
You can't have a user
that just sits within a particular Region.
Users, groups, policies--they're all global.
Now you'll be able to find policies here,
under Access Management.
If we click in policies, you'll see that I have no custom
policies, but I've got all these default policies,
which are default from Amazon.
The reason you can tell it's an Amazon managed policy
is using this little icon in here.
So you can see it's the AWS logo.
And that means that
Amazon manage and control these policies.
Now you can go ahead and create your own custom policies,
but to be honest, Amazon had so many different policies
that you can pretty much use one of their managed policies
and you can actually type up here,
and search for the policy that you're after.
So if you are trying to do a policy for S3, for example,
you type in the S3 service in here,
and then you could go in and click on S3 full access.
You can also search by roles.
So in here we've got our administrator.
And you can see in here,
it's actually, they have it by type.
So you can see that this is by a job function.
So our administrator needs full administrative access.
So you can click in here,
and if you actually click on that policy
you'll see the one that we were just looking at
in the slide before,
and this is all the permissions.
This is the way it's done in JSON.
And the effect is allow, the action is allow everything,
and the resource is everything.
And this will give you full administrator access
to anything that you assign this policy to.
Now in the next lecture,
we're going to go ahead and have a look
at how we can assign policies to users,
to groups, etc., etc.
First, let's go ahead and have a look at my exam tips.
So onto my exam tips,
just remember going into your exam
that you assign permissions using IAM policy documents.
These consists of JSON, which is JavaScript Object Notation,
and it looks like this.
Now you will have more complicated policy documents
going into your exam,
so we will cover that off later on in the course--how to
read policy documents and how to understand them.
Typically your policy documents will be centered around S3,
but it could be anything, to be honest--
they change the exam questions all the time.
Really, you just need to get comfortable reading this.
And what I would do, honestly, before going into your exam,
is just go into IAM
and randomly click through different policies
and have a look at how the language works,
how the JSON works,
how you can grant access to particular resources.
So that is it for this lecture everyone.
In the next lecture,
let's go ahead and create our users, our groups,
and we'll look at how we can assign policies
to our users and groups.
So if you've got the time,
please join me in the next lecture.
Thank you.


Permanent IAM Credentials
==========================

Okay. Hello, Cloud Gurus,
and welcome to this lecture.
In this lecture we're going to look at
permanent identity access management credentials.
So in this lecture we're going to look at
the building blocks of IAM.
We're going to look at users, groups, and roles.
Then we're going to run through our best practices
in terms of policies,
our best practices in terms of users and peoples.
And then we're going to talk about
the principle of least privilege.
Then we'll go in and look at how we can set all this up
in IAM and we'll move on to my exam tip.
So let's start with the building blocks
of identity access management.
So we have our users,
and our user should be a physical person.
One user should always equal 1 human being
and we should group our users into groups
and this should be by job function.
So essentially your job function might be
your administrators, your developers,
human resources, finance--whatever job function you have
within your organization,
you should create a group for that
and then put your, the relevant users into those groups.
And then we have roles.
And roles are basically used for internal usage within AWS,
and it allows 1 part of AWS to access another part of AWS.
We're going to cover this off in a lot more detail
in the EC2 section of the course.
Essentially, what we're going to do
is allow our virtual machines to access our S3 buckets.
Roles are very easy to understand,
but only once you start using them.
So I'm going to cover that off
in the EC2 section of the course.
Right now we're really going to focus
on users, groups, and policies.
So like I was saying before, it's best practice
for users to inherit their permissions from groups.
You can definitely assign IAM policy documents
directly to users to give them permissions.
But that can be really hard to manage,
especially if you have users coming and going
from the organization--
if you've got contractors, etc.
It's much easier to basically apply
an IAM policy document to a group.
And then what will happen is the users
in that group will inherit those permissions.
So when you're making changes, you do it at a group level
rather than a user level.
So it is best practice to always apply IAM policies
to groups rather than to individual users.
And remember that users and people
basically should be equal to each other.
So 1 user should always equal 1 physical person.
You should always work on that principle.
You should never share user accounts across multiple people.
So you shouldn't have an administrator user
and then you share that with a team of 5
because you basically wouldn't be able
to track who did what.
You should always make 1 user
equal to 1 physical person.
And then following on from that,
we have the principle of least privilege.
And you hear this a lot when you're talking about
security discussions within AWS and within tech in general.
And this is where you only assign a user
the minimum amount of privileges they need to do their job.
So your developers, you want to give them developer access,
maybe full administrator access to the AWS platform.
What you don't want to do is give that to your finance team
or to your human resources team.
Your finance team and your human resources team
don't need to spin up virtual machines.
They probably don't need to go in
and use some of the artificial intelligence services.
That's something only your devs
potentially should need to do.
So just remember, you should only assign a user or a group
the minimum amount of privileges they need
in order to do their job.
So let's go ahead and log in to the AWS console
and see all of this in action.
Here I am in the AWS console,
I'm going to go over to identity access management--
which, as always, is under Security, Identity,
and Compliance, and IAM is the very first one.
And here we can see I've got zero users and zero groups.
Now the first thing,
we'll just go ahead and create our user.
And in here, I'm going to add our user and the username.
I'm just going to put as my name.
So Ryan Kroonenburg,
and then in here we've got our access type.
So we've got programmatic access or console access.
Now I'm going to go ahead and grant console access.
If you do programmatic access,
it's going to generate an access key ID
and secret access key.
In fact, let's click on that as well.
And then in here we've got our console password.
It's going to be auto-generated
and then we're going to require
that our users reset their password as they sign in.
So let's go ahead and add our permissions.
Now you can see that we haven't added any groups yet.
So it says you haven't created any groups yet.
Let's go ahead and create a group.
Now, my group name--
I'm going to give this administrator access,
so admin, or I'll just call it administrators, for example.
So administrators.
And in here we can apply a policy to the group.
Now these are AWS managed policies
and you can see here that this one's by job function.
So I'm going to give my administrator group
administrator access, and you can click in here
to have a look at the policy summary,
which is basically saying, allow everything.
So let's go ahead and hit Create Group.
And that will go ahead and create my group.
And now I'm going to add my user to this group.
So I'm going to be the administrator of my group.
So in here I have my tags.
In here, we could put in our department, for example,
so our department would be administrators.
So this person is in my administrator group.
We could have employee ID.
So employee_ID
and then 1, 2, 3, 4, 5, etc., etc.
And all these tags will be key-value pairs
that are added to our user.
So it allows us to track our users
and you can sort by tags later on.
Let's go ahead and hit Next to review.
And we're going to go in and go ahead and create our user.
Okay, so we've successfully created our user.
And we can see 3 things here.
We've got our access key ID,
our secret access key, and our password.
Now our access key ID and secret access key
is used for programmatic access to the AWS console.
We're going to cover that off later on
in the course when we configure the command line.
But the key point to take away here
is you're only ever going to see all this once.
So you should either send an email or download it as a CSV.
And you can see in here my password as well.
So I'm going to go ahead and delete this
as soon as I finish this lecture
so you guys can't access my account,
but you will only ever see all of this information once.
So let's go ahead and hit Close.
And that has now created my user.
And if I go into my groups,
I'll be able to see my groups in here.
And we've got our administrator.
Now, I'm not going to create any more groups or users,
but I am going to go down to policies.
And I want to show you--
you can see here, we've got our job function.
So we've got administrator access.
Now you can keep scrolling down.
And, honestly, because everything begins with Amazon,
it takes an awful long time.
So I'm going to pause the video
and scroll all the way to the bottom.
So I've just gone past the Amazon part,
but you can see in here, we've got database administrator
and data scientist, and these are job functions.
So Amazon already have pre-populated policies
for particular job functions.
We scroll all the way to the bottom.
You can now see quite easily,
we've got view-only access.
We've got system administrator.
We've got support users.
We've got security audit.
We've got our power user access,
network administrator access.
And over here is the description.
So power user provides full access
to AWS services and resources
but does not allow management of users and groups.
So a power user can go in and do everything
that a normal administrator can,
but they can't create a user and can't create a group
and they can't grant administrator access
to users or groups.
They basically are sort of like an administrator
except they don't have the ability
to create users and groups.
And if you read the different job functions,
it will give you a breakdown as to what they can do.
But you can also click in the policy document.
So data scientist
grants permission to AWS data analytic services.
And if you want to actually go in and view the policy,
you can just click the down arrow
and you can actually read the policy here.
So in here, we've got our statement.
Our action is all of these services.
It's going to essentially affect allow and then resource.
And then these are all the stuff that it allows,
and you can see it can get quite complicated in here.
So it will allow access to some AWS services that it thinks,
or that AWS thinks, are involved with data scientists.
So things like data pipeline, for example,
CloudWatch or CloudFormation, etc., etc.
Don't worry if you don't know
what any of these services are, we'll cover them off later.
But the point I'm trying to make
is Amazon pre-populate policies based on the job function
to make your life easy.
Now, of course, you can go in and create your own policies,
that's totally up to you.
Then you basically give them a name
and it will be saved here.
But basically it will be saved down the bottom.
It will be under the AWS management policies
and you won't have the icon.
If you've got a customer managed policy,
like you can see in here,
it's going to be--essentially,
it won't have this little icon here,
which means it's not an AWS managed policy.
So now that we've created our users and groups,
let's go ahead and have a look at our account settings.
And in here, we can basically define our password policy.
So right now it uses the following custom password policy.
So we've got a minimum of 8 characters.
We need at least 1 uppercase letter,
one lowercase letter, at least 1 number,
and 1 non-alphanumeric character.
And in here you can actually change the password policy.
So you can enable a password expiration.
You can say, Hey, I want my passwords
to expire every 90 days.
We can now allow our users to change their own passwords.
We can also prevent password reuse.
So we can make it super, super secure.
And all you have to do is hit Save Changes
and that will then save our password policy and update it.
So now it's gone from default.
Now, it will essentially expire every 90 days.
We can allow our users to change their own passwords.
And we can remember our last 5 passwords
and prevent reuse.
Another thing I want to show you
is if we just create a new user
and let's just add this user,
we'll call it a test_user.
So test_user.
I'm going to give them console access.
I'm just going to go ahead and hit Next.
I'm not going to assign them to a group.
I'm just going to go ahead and hit Next.
And I'm going to leave my tags and hit Review.
Now you can see, it says this user has no permissions.
You haven't given this user any permissions.
This means that the user has no access
to any AWS service or resources.
Consider returning to the previous step
and adding some type of permissions.
So every time we create a user, by default
that user has no permissions, has no privileges whatsoever.
So remember that going into your exam.
And finally, the last thing I want to show you
is identity providers.
So if we click in here,
we'll be able to see our identity providers.
And in here you can see it says
it makes it easy to centrally manage access
to multiple AWS accounts and provide users
with a single sign-on access
to all their accounts from 1 place.
You can go in here and add a policy.
And the most common type of policy is a SAML policy
that's tied into something
like Active Directory Federation Services.
So if you work in a Windows environment,
when you log on to your Windows machine at work every day,
you're going to enter in your SSO,
which stands for single sign-on.
And that's basically your username and password.
So the same username and password that you use
to log in to your work PC within a Windows environment,
you can also use to log in to the AWS platform.
But in order to configure that, essentially,
you need to establish a trust between your AWS accounts
and Active Directory Federation Services.
And that's what you do here.
Now, I'm not going to show you how to do that.
It's well beyond the scope
of the Solutions Architect - Associate exam.
But the important thing to note is
if you ever see a scenario question
where it talks about making your username and password
the same as when you log in the morning to your AWS account,
that's just basically called federation--
Active Directory Federation.
And you can do that.
You just create an identity provider within IAM
and it's using SAML.
So let's go on to my exam tips.
So what have we learned so far
about identity access management?
Well, we've learned that IAM is universal.
It does not apply to Regions at this time.
When you create a user or a group,
it's created across the world globally.
It's not tied down to a specific Region.
We've also learned that the root account
is the account that's created
when you first set up your AWS account,
which has complete administrator access.
And you want to secure it as soon as possible,
and you shouldn't be using it
for your day-to-day management of AWS.
When you create a new user,
there's no permissions when they're first created.
We had to go in and essentially add permissions to our user.
We did that in this lecture by adding our user into a group.
And we then assigned our group with AWS permission.
So we gave our group administrator access.
Our access key ID and secret access key
are not the same as usernames and passwords.
Access key ID and secret access keys
are used for programmatic access to the AWS console.
And like I said, we're going to look at that in more detail
when we cover off the command line.
You will only get to view your access key ID
and secret access key once.
So make sure you save them,
download them, and send them on to your users.
If not, you are going to have to regenerate them.
And we'll have a look at how we can do that again
in the EC2 section of the course.
And you should always set up password rotation,
so you can create and customize
your own password rotation policies.
And then finally, we learned about IAM federation.
So we learned that you can combine
your existing user accounts with AWS.
For example, when you log on to your PC,
usually using Microsoft Active Directory,
you can use the same credentials to log in to AWS
if you set up federation.
And to do that, you use identity federation,
and this uses SAML as the standard,
which is essentially Active Directory.
So that is it for this lecture, everyone.
In the next lecture we're just going to review
everything that we learned in this section of the course,
and then we're ready to move on.
So if you've got the time,
please join me in the next lecture.
Thank you.


IAM Exam Tips
=================

Okay.
Hello, Cloud Gurus, and welcome to this lecture.
Well, congratulations on completing
your first practical section of the course.
In this section we learned all about
identity access management,
which is a fundamental part of AWS.
So you need to know how identity access management works
not just for the exam
but also for using AWS in real life.
So let's cover off what we learned
in this section of the course.
The first thing was we looked at 4 steps
to securing our root account.
And in the very first lecture
we enabled multi-factor authentication on the root account.
Once multi-factor authentication
is turned on for our root account,
we need to take some more steps
enable to secure our root account.
And our end goal is to stop logging in
using the root account to administrate our AWS account.
We basically don't want to ever log in
to our root account again if we can avoid it.
And so to do that what we need to do
is create an administrator group for our administrators
and assign the appropriate permissions to that group.
We then need to create user accounts for our administrators.
And once we've created those user accounts,
we need to add those users to our administrator group.
And once we've done that, we then, in theory,
should no longer have to log in to AWS
using the root account again.
So we then learned about policy documents
and we can assign permissions using IAM policy documents.
And these consists of JavaScript Object Notation (or JSON).
And we looked at the most simplest policy document,
most universal.
This gives us administrator access to all AWS services.
And you can see that
because basically it says effect allow,
action which is wildcards.
So allow any action to any resource
and that will give us full admin access.
Now going into the exam,
you will get some IAM policy documents in JSON format
that you will need to interpret.
I would just recommend going through your policy documents
and looking at individual ones
and trying to figure out what it is they do.
And also to read the documentation.
We will have in-depth lectures
on IAM policy documents later on in the course as well.
Going into your exam, remember the following.
So, IAM is universal.
Like I said, it doesn't apply to Regions at this time.
When you create a user or a group, they're not created
in the Northern Virginia Region or the London Region--
they're created universally.
The root account is the account that's created
when you first set up your AWS account
and it has complete administrator access.
You should secure the root account as soon as possible.
You should never use it to log in from day to day.
And when you create a new user,
that user has no permissions when they're first created.
So you have to go in and add them to a group
where they will inherit permissions.
Or you could apply policy documents to a user
to give them permissions.
But like I said, that's not best practice
because that can be very difficult to manage.
When you create a user, it generates an access key
and secret access key,
when you essentially allow programmatic access
to the AWS console.
Remember that the access key and secret access key
are not the same as usernames and passwords.
These are used only for programmatic access--
so when you're using the command line.
We will cover that off later on in the course.
And you're only ever going to view your access key ID
and secret access key as well as your user password once.
So you need to save them in a secure location.
And you should always set up password rotation policies.
So you can create and customize
your own password rotation policies,
which is what we looked at in the last lecture.
And then finally, IAM federation.
You can combine your existing user accounts with AWS.
For example, when you log in to your PC,
usually using Microsoft Active Directory,
you can use the same credentials to log in to AWS
if you set up federation.
This is done using SAML, and that's the standard,
which is essentially Active Directory.
So that is it for this lecture, everyone.
Congratulations, you've completed the very first
technical section of the course.
In the next section of the course,
we're going to do an in-depth
look at S3 (Simple Storage Service).
And this is one of the oldest services in AWS.
And because of that, it comes up on the exam an awful lot.
The good news is it's very simple
and it is lots of fun to learn.
So if you've got the time,
please join me in the next section.
Thank you.


Simple Storage Service
=========================

S3 Overview
===========

Okay.
Hello, Cloud Gurus.
And welcome to this section of the course.
In this lecture, we're going to do an overview of S3.
S3 is 1 of the oldest services with AWS.
And for that reason,
it features very heavily in a lot of exams actually.
But certainly in the Certified Solutions
Architect Associate exam.
This is 1 of the bread-and-butter services
you need to know inside out in order to pass your exam.
So what are we going to learn in this lesson?
Well, we're going to learn what S3 is.
We're going to learn the basics around S3.
We're going to learn how we can work with S3 buckets.
We'll look at key-value stores.
We'll look at availability and durability.
We'll look at characteristics of S3,
and then we will look at how we can secure our data.
We'll look at the consistency model
when we upload data to S3,
and then we'll go on to our exam tips.
And then in the next lecture, we're going to have a look
at how we can create an S3 bucket.
So what is S3?
S3 stands for Simple Storage Service
and essentially it's object storage in the cloud.
It provides secure, durable,
and highly scalable object storage.
And S3 allows you to store and retrieve any amount
of data from anywhere on the web at a very low cost.
So it's extremely scalable.
And the other cool thing about S3
is it's really, really simple to use.
It comes with this really easy-to-use web interface,
which we will look at in the next lecture.
So S3 is object-based storage, and basically it manages data
as objects rather than in file systems or data blocks.
And all that means is S3 is basically a place
where you can store your files.
So you can upload any file type that you can think of to S3.
So you can upload things like image files, text files,
videos, web pages--you name it.
And that's what you can store in S3.
What you don't want to do with S3
is install an operating system.
So you can't run or install Windows or Linux on S3.
You can't run databases off S3.
Basically S3 is just a place to store your static files.
So your files that don't change,
and that's all S3 is: object-based storage.
So in terms of the basics,
you get unlimited storage with S3.
The total volume of data
and the number of objects you can store is unlimited.
And objects can be up to 5 terabytes in size.
So you can start with 0 bytes
and go all the way up to 5 terabytes.
So you can store some pretty big files in S3.
So where do we store our files?
Well, we store our files in a thing called a bucket.
An S3 bucket is basically a folder inside S3.
So you might have a bucket for your test and dev.
You might have a bucket for your finance department.
You might have a bucket for your vacation photos from 2015.
Basically everywhere you store your files
is in these things called buckets.
Now working with S3 buckets,
what you need to understand is it's a universal namespace.
So if you try and register the name test-bucket,
I guarantee you that was taken, you know, over a decade ago.
Somebody owns the bucket test-bucket.
So all AWS accounts share the S3 namespace.
And for that reason,
your S3 bucket name has to be globally unique.
You won't be able to register acloudguru, for example,
because we already own that.
You won't be able to register test-bucket.
But you might be able to register a bucket in your name
with your date of birth or something like that.
And you'll need to understand how the S3 URLs work
because this can come up in the exam.
So when you create a bucket--
let's say it's called acloudguru--
the URL that will be generated will always
be https:// then the name of the bucket, then .s3,
and then . the region that it's going to be in.
So in here we've got acloudguru.s3.us-east-1
and then it's always .amazonaws.com,
and then / and then essentially the key name,
so this is your file name,
so it could be Ralphie.jpg.
Ralphie is the A Cloud Guru office dog.
So when you upload a file to S3,
if that upload is successful,
your browser will receive a HTTP 200 code
and that can be a very popular exam topic.
So just remember that going into your exam as well.
So I mentioned before that S3 basically works
off a key-value store.
So the key is simply the name of the object.
So if we were to upload a photo of the A Cloud Guru
office dog, Ralphie, the key would be Ralphie.jpg.
We then have the value, which is the data itself.
And this is just made up of a sequence of bytes.
We also can have the version ID.
This is when we have versioning enabled on the bucket
and we're going to have a look at that later on
in this section of the course.
But this is important for storing
multiple versions of the same object.
So you can have multiple versions of the file within S3.
And then we have our metadata.
And throughout this course,
you're always going to hear the word metadata.
It relates to various things within AWS.
Just remember metadata is simply data about data.
So in this case with S3,
the data about the data that we're storing.
So this could be the content type,
this could be the last time that the file was modified,
etc., etc.
So that's what we have at a high level
when we upload our objects to S3.
We have the key, we have the value, the version ID,
and then the metadata as well.
So just remember S3 is a safe place to store your files.
The data is always spread across multiple devices
and facilities in order to ensure
availability and durability.
So it's not just in a single data center in a single server.
It's always spread across multiple devices
and multiple facilities.
It can be across multiple Availability Zones,
but we'll come to that in a second.
But just remember it's done this way to ensure availability
and durability as well.
So S3 is highly available and highly durable
and it's built for availability.
It's built for 99.95% to 99.99%
service availability depending on the S3 tier.
We're going to cover off the tiers in a second.
And then it's also designed for durability.
And durability just means
is your files going to be lost at some point?
And it's designed for what's called 11 9's durability.
So this is 99.999999999.
I'm not going to read it all out,
but it goes to 9 decimal places.
So there's 11 9's in that number.
So 11 9's durability for data stored in S3.
So you should not lose any objects
stored on standard S3.
So what is standard S3?
Well, that's just normal S3.
Again, it's designed for high availability and durability.
Standard S3 is essentially the default version
of S3 when you store your objects.
So data's stored redundantly
across multiple devices in multiple facilities.
So at least 3 Availability Zones.
It's designed for 99.99% availability
and 11 9's durability.
It's designed for frequently accessed data.
So if you're frequently accessing it,
maybe you're reading and writing
to it every second or even every hour or every day.
That's what S3 is built for,
and it's suitable for most workloads.
So like I said, it's the default storage class,
and it's use cases include things like websites,
content distribution, mobile and gaming applications,
big data analytics, etc., etc.
So that's S3 Standard.
We do have tiered storage within S3.
So S3 offers a range of storage classes
designed for different use cases.
So S3 Standard fits most use cases,
but it doesn't fit all use cases.
And we'll cover off the different tiers in a second.
One thing you do need to know is you get a thing
called lifecycle management,
and this is where you define rules
to automatically transition your objects
or files to a cheaper storage tier
or to delete objects that are no longer required
after a set period of time.
We're going to have a whole lecture on that coming up.
And then, like I said earlier, we also have versioning.
And with versioning,
all versions of an object are stored in S3
and they can be retrieved, including deleted objects.
So we'll have a look at that in further detail later on
in this section of the course.
In terms of securing our data,
we can secure our data in a number of different ways.
So we can have server-side encryption.
And this is where we can set default encryption
on a bucket to encrypt all new objects
when they're stored in the bucket.
So as soon as you upload an object to your bucket
that object will be encrypted.
So if somebody breaks into the AWS data center
and starts stealing hard drives--which is highly unlikely,
but you never know--
that means that they would not be able to access
your objects because it would be encrypted.
We then also have access control lists,
and this defines which AWS accounts or groups
are granted access and the type of access
that they're granted.
And you can attach S3 access control lists
to individual objects, to individual files within a bucket.
It gives you that much of a fine-grained approach.
And then we have bucket policies.
And these are policies that are specific
to what actions are allowed or denied
for that particular bucket.
So you could allow Alice to put
but not delete objects into a bucket.
And bucket policies are bucket wide.
It's very similar to what we looked at with IAM policies.
Bucket policies are simply JSON policies,
and you attach them to buckets
and they will apply across the bucket as a whole.
Whereas with access control lists,
you can actually drill down your permissions
to individual objects and individual files
within your bucket.
In terms of the data consistency model with S3.
Right now it's strong read-after-write consistency.
So as soon as you upload a new object
or put a new object or overwrite an existing object in S3,
any subsequent read request immediately
receives the latest version of the object.
So if I was to create a new text file and I upload it to S3
and I was trying to read that text file, like, you know,
a couple of milliseconds
after it had finished writing to S3,
I'm going to get a successful read of that object.
Likewise, if I went in and updated a file
and then I immediately went to read that object
after I've done the update,
I'm going to get the newest version of the file.
And you get strong consistency for list operations.
So after you write objects to your S3 bucket,
you can immediately perform a listing object
or a listing command, which basically just lists
all the objects in your bucket.
And you'll be able to see that new file
being created immediately.
So just remember going into your exam,
the consistency model for S3 is strong read after write.
As soon as you've written something to S3,
it should be immediately available.
So in terms of what you need to know for your exam.
Just remember that S3 is object-based storage.
So objects are simply files.
You can't install an operating system or a database on S3.
It's not suitable for that.
It's simply for flat or static files.
Your files can be 5 terabytes in size,
up to 5 terabytes in size.
Starting from 0 bytes and you get unlimited storage.
So the total volume of data
and the number of objects you can store
within S3 is unlimited.
And just remember that your files are stored in buckets
and that S3 is a universal namespace.
So when you create a bucket, it has to be a unique name.
And the web address that's created will always
be https:// the bucket name, then .s3, and then . the
region. So it could be us-east-1, for example,
and then .amazonaws.com,
and then it will be / and then the key value pair
or any folders that you create within the bucket.
And remember that when you do a successful command line
or API upload, you're always going to get back
a HTTP page 200 status code
saying that your upload has been successful.
And then finally some 4 S3 object tips.
Just remember that S3 consists of a key.
So the key is simply the object name.
So in our example, we used Ralphie,
the A Cloud Guru office dog.
So Ralphie.jpg.
The value is the data itself,
which is made up of a sequence of bytes.
We then have our version ID, which allows you
to store multiple versions of the same object.
And then we have our metadata,
which is simply data about data.
So this could be data about the data that you are storing--
for example, the content type
or the last time it was modified, etc.
So that is it for this lecture, everyone.
It's very high level and theoretical right now.
In the next lecture,
we're going to go ahead and create our very first S3 bucket.
And we're going to go ahead and upload some objects to it
and try and make them public.
We're going to look at the security implications
behind that as well.
So if you've got the time,
please join me in the next lecture.
Thank you.


Securing Your Bucket with S3 Block Public Access
================================================

Okay, hello, Cloud Gurus, and welcome to this lecture.
In this lecture, we're going to look at how we can secure
our buckets with S3 using the Block Public Access setting.
So what we're going to learn about in this lesson,
we're just going to recap what we learned in the last
lesson: object ACLs versus bucket policies.
We're then going to go into the console
and we're going to create our very first S3 bucket.
We're going to upload some files to it
and try and make those files public.
We going to see how we can do that.
And then we'll go on to my exam tips.
So in the last lecture we looked at object ACLs
(or access control lists) versus bucket policies.
So access control lists work on an individual object level.
So you can basically make an individual file
public or readable.
You can adjust the permissions so that certain AWS users
could access and read that file or download it
or write to that file or delete it.
And then deny other AWS users the right to do that.
And you can do that on an individual object or file level.
However, if you want to make everything public
in your bucket, for example,
you would do that using a bucket policy, and we
are going to do that in a couple of lectures coming up.
So bucket policies are bucket wide.
They apply on an entire bucket,
whereas object ACLs go down to the individual object level.
And it's really important you remember
that going into your exam.
So now that you know that,
let's go ahead and log in to the AWS console.
Okay, so here I am in the AWS console.
You notice my default region is Northern Virginia.
If I go over to All Services,
I'll be able to see S3 under Storage.
And it was 1 of the very first storage services.
So it's right under Storage.
And in here, you can see I've got no buckets.
Now note up here the region has changed to global.
Now I know what you're thinking, we've looked at IAM.
Now we've looked at S3, and both seem to have a global view.
Unfortunately, there's only a few services in AWS
that have a global view.
Almost every single service is regional.
It just so happens that we've just started on the few
exceptions to the rule.
So just bear that in mind going into your exam.
So let's go ahead and create a bucket.
Now, if you remember, we learned that bucket
namespace is global.
So you have to have a unique bucket name.
So here you can see it's defaulting at saying, "Hey,
"call your bucket name myawsbucket."
Well, let's try that: myawsbucket.
I'll tell you, if I can register this, I'm going to keep it.
And we'll just leave everything as default
and just go ahead and hit Create Bucket.
Now straightaway, you can see it says, "Bucket
"with the same name already exists."
It's probably somebody in the S3 team who owns this bucket.
So let's give the bucket a more unique name.
So I'm going to say ryan, let's do pi3141
and then kroonenburg--something like that.
So that's quite a unique name.
And that should hopefully be completely unique.
So in here, we define our region.
Now we did just say that S3 is a global view.
And that's true when you're looking at S3 as a whole,
but your buckets are actually stored in individual regions.
So although you get a global view for S3,
the buckets themselves are regional.
So in here I'm just going to select us-east-1.
So Northern Virginia.
And then down here we've got our Block Public Access
Settings for Buckets.
And you can see, by default, this is ticked.
So by default, we're going to block all public access.
So essentially this bucket is private.
It doesn't matter.
Every file that you put in there is going to be private.
It's not going to be publicly accessible.
I'm going to leave that as default, and we'll scroll down.
You can see here we've got our bucket versioning.
We're going to cover that off in another lecture.
So just leave it as disabled.
And here we've got our encryption.
Again, we're going to cover that off in another lecture.
So just leave everything as default,
and go ahead and hit Create Bucket.
Now that will create my bucket.
You can see that that is a unique bucket name.
And we can see that it's been created in us-east-1.
And if we have a look over here,
it says Access. Our bucket and objects are not public.
So everything in this bucket is going to be private
and that's by default.
So let's go ahead and upload some files or objects
to our bucket.
So in here we can drag and drop our folders or files.
I'm just going to go ahead and hit Add Files.
And I'm going to go over to my desktop
and I'm going to add 2 files.
I'm going to add 1 of our office dog, Ralphie.
And I'm going to add another 1,
which is just a picture of me and my colleague,
or coworker, Faye at New York Summit.
So let's go ahead and upload these 2 pictures
to our S3 bucket.
You can see here, this is the destination.
And I'm just going to go ahead and hit Upload.
And once we hit Upload,
we will be able to see if it's been successful.
And you can see here it says Succeeded.
And here it's uploading.
It's quite a big file.
So it's going to take a little bit more time.
Here it says Succeeded.
So both have succeeded.
And our browser will be getting back a HTTP 200 code
because that has now successfully been uploaded.
So let's go ahead and hit Exit.
And now we can see our objects are in our bucket.
So if we go back to our global view.
You can see it's global up here.
Here's our bucket and it's in us-east-1
in Northern Virginia.
So let's click in our bucket.
We've got our 2 objects in here.
Let's try and make this object public.
So I want to be able to view this just by clicking on a URL.
And you can actually, if you click on the object itself,
you'll always be able to get the URL just over here.
We've got our object URL.
And if I click on that link,
I'm going to get an access denied.
So you can see in here, there's some XML
and it's just saying access denied.
I cannot view this object publicly
because I haven't enabled permissions for it.
So let's go ahead and go back.
So if I go back to my bucket,
let's go ahead and click in here.
And you can see that we have our Actions over here.
So we've got Actions, and I can click in here
and I can go ahead and I can scroll all the way down
and there's an option that says Make Public.
So let's go ahead and make this public.
And, what we'll do is we'll go ahead and hit Make Public.
And it does give us a little info thing here,
saying, "Public access is blocked
"because block public access settings are turned on
"for this bucket."
Let's go ahead and try and make it public anyway.
And we're going to get an error message.
And that's because we blocked public access
to this bucket.
So the first thing we want to do is make this bucket public
on a bucket level--
enable public access to this bucket.
So to make it public, we need to go over to Permissions.
And you can see here,
these are our Block Public Access settings.
So let's go ahead and hit Edit.
And what we're going to do is we're going to uncheck this
and we're going to leave everything unchecked.
And this will essentially allow us to make objects
within our bucket public.
So let's go ahead and hit Save.
And then we just have to type in here "confirm."
So we have to make sure, you know,
we're not doing this accidentally.
We have to physically go in and hit Confirm.
And now you can see that this is turned off.
And if we actually go back to S3,
we can now see that the access level has changed.
And it says objects can be public.
So we can make objects in here public.
So let's go ahead and do that.
We'll click on our object, we'll go over to Actions,
and then we're going to go over here and hit Make Public.
And let's go ahead and make that public.
And now, you can see that we've got a success code.
So, it has gone ahead and made that public.
So if we exit out of here
and if we click on our New York Summit.
And if we actually just click on the link,
you'll be able to get the object URL in here.
Note the URL structure.
So it's the name of the bucket .s3.amazonaws.com
and then / and then the name of the object.
So summit.JPG.
So let's click on that.
And there we go.
It is now loading it and we can share that URL
all over the internet and everyone
will be able to access it.
So that object is now public.
So come back here, let's go back into our bucket.
If we were to click on Ralphie and click on this,
we're going to get that same XML error,
which is basically just saying access denied.
And that's because we haven't made this object public.
But if we go back and we click on the object itself.
So we click in here and we go over to our Actions,
we can then go ahead and make the picture of Ralphie public.
And then you can see that that has now
successfully happened.
And then if we click in on Ralphie
and then click on the link,
we should now get a picture of Ralphie.
And there he is.
And he's got a cute little collar saying,
"Keep being awesome, Cloud Gurus."
So that's Ralphie, our office dog.
So that is S3 buckets in a nutshell.
By default, when you create an S3 bucket,
everything will be private.
It will block all public access.
You can even go in there and try and make objects public,
but it won't work
because the bucket is blocked at the bucket level.
So you won't be able to go in there
and make an individual object public.
You first have to go into Permissions
and uncheck the Block Public Access settings
and then go in and make your objects public.
So let's go ahead and have a look at my exam tips.
Okay, so going into the exam,
just remember that buckets are private by default.
So when you create an S3 bucket, it is private by default
and that includes any objects within it.
And you have to allow public access on both the bucket
and its objects in order to make the bucket
and those objects public.
Object access control lists (or ACLs),
you can make individual objects public using object ACLs.
Bucket policies, you can make entire buckets public
using bucket policies.
And every time you upload an object to S3
and it's successful, you will receive a HTTP 200 code.
So that is it for this lecture, everyone.
If you have any questions, please let me know.
If not, feel free to move on to the next lecture.
Thank you.


Hosting a Static Website using S3
====================================

Okay, hello, Cloud Gurus,
and welcome to this lecture.
In this lecture, we're going to look at
how we can host a static website using S3.
So we're going to explore what static websites on S3 are,
then we're going to talk about automated scaling,
then I'll show you how to actually go in and do this,
and then we'll move on to my exam tips.
So you can use S3 to host static websites,
and static just means they're not dynamic
so they basically don't change.
So things like HTML websites, for example.
Dynamic websites are those that require a database
connection, and they can't be hosted on S3,
but S3 is great for static web pages.
And prior to me working at A Cloud Guru,
one of my jobs as a solutions architect was to help
the film industry basically release trailers.
And we would always put these trailers on S3
and the websites themselves on S3.
That way it could basically handle the load.
We didn't have to worry about load balancers
and, you know, capacity planning, etc., etc.
S3 did it all for us.
And that's because S3 scales automatically to meet demand.
And many enterprises will put their static websites
on S3 when they think there's going to be a large number
of requests, and like I said, for a movie preview
so that's where I've used it in real life.
So let's go ahead and have a look at how
we can host our websites on S3.
Okay, so in the Resources section of this course,
you'll find a zip file.
And inside that zip file are 3 files.
So we've got our bucket permissions,
our index.html, and our error.html.
So this is JavaScript Object Notation
and this is our bucket permissions.
And basically we're going to use this
to make our bucket public,
and we will need to change this line of code here, on line
12, so that it points to our bucket.
So we'll need to put in our bucket name in there.
And in terms of our index.html,
it's just a very straightforward HTML page.
It basically just says, Hello Cloud Gurus,
and then it has a picture of the A Cloud Guru logo.
And then if we were to go to our website
and type in a address that doesn't exist,
we want it to come up with an error page.
And this will be the error page just says,
"Sorry, Cloud Gurus, there has been an error."
So a really, really basic website.
So go ahead and download this in the Resources section,
unzip it, and then let's head over to the AWS console.
Okay, so here I am in the AWS console,
I'm going to go over to Services,
and I'm going to go over to Storage and click on S3.
And in S3, we'll have the bucket that we created earlier.
So it's the ryan3141kroonenburgbucket.
Let's go ahead and create a new bucket.
I'm going to call this bucket myacloudguruwebsite2021,
and then I'll just do 12345.
So it should be, hopefully be random enough.
Now I am going to uncheck Block All Public Access
because we do want people to be able to access this bucket
because it is a website,
so we need it to have public accessibility.
You will have to click in here--
"I acknowledge that the current settings might result
"in the objects becoming public"--
and then just go ahead and hit Create Bucket.
And that has now successfully created my bucket.
So if I click in here, I'll be able to see my bucket.
So the first thing we need to do is go over to Properties,
and we scroll all the way down to the bottom,
and you can see in here it says Static Website Hosting.
And what we want to do is we want to go ahead and hit Edit
and we want to enable it, so go ahead and enable it.
In here, we specify our index document,
so it's going to be called index.html,
and in here we're going to type error.html.
So this will basically specify our index page,
and what page to load if there is an error.
So let's go ahead and hit Save Changes.
And that has now enabled static website hosting
on our bucket.
Again, if we scroll all the way down,
we now have a URL that we can visit in order to,
you know, to view this website.
Right now, I'm just going to leave it,
and what we're going to do is we're going to go in
and upload our objects.
So let's go ahead and hit Upload,
and we're going to add our files.
So my files are saved in here.
Three files, actually, we don't need the JSON,
so we're just going to add our 2 files:
index.html and error.html.
I'm going to go ahead and hit Open,
and then I'm going to go ahead and hit Upload.
So that has now uploaded our files to S3.
The last thing we need to do--let's just click on Exit.
So we need to go in and we could make these files public
on an individual basis.
So we could click in here and go ahead
and go over to our Actions and go Make Public.
But if you've got a website with thousands of files,
that's very, very time consuming.
Instead, what we want to do is we want to go ahead
and basically make this entire bucket public.
And if I just come back out of S3,
you can see in here right now
it says my Access settings, Objects Can Be Public.
So let's click in here and let's go over to our Permissions.
And in Permissions, we've got our bucket policy in here.
And we can go ahead and hit Edit.
So here is our bucket policy.
So what I'm going to do is I'm going to copy and paste that
JSON from my downloads directory into here.
So here we go, this is my JSON.
I'm going to copy that into my clipboard,
going to paste it over into my policy,
so I pasted it in.
And then what we need to do is
we just need to update this line,
this arn, and arn stands for Amazon Resource Name.
And this is a unique way of referencing objects within AWS.
So every time you create a service
or some kind of thing within AWS--
so it could be an EC2 instance,
it could be a bucket, could be a security group--
you'll always get an ARN.
And basically it's a unique name to address
that particular service within AWS.
So what I'm going to do is I'm going to paste it in here.
Make sure you keep the / and then the wildcard. Once I've
pasted it in here, I'm going to go ahead and hit Save.
And we can now see that this now is publicly accessible.
So it is giving us this little UI
that's showing us that it's publicly accessible.
If we actually click on S3,
you can also see in here that the access is public.
So you can now see
that it is completely publicly accessible.
So every file I put in here will now be publicly accessible.
If we go over to our Properties
and we scroll all the way down and click on this link,
we'll now be able to see our static website.
So, "Hello Cloud Gurus."
And if we change the name of the URL,
so if we just click up here--
I'll just minimize this--and I'll go /
and then just type in some junk, we'll get our error page.
So this is our error page:
"Sorry Cloud Gurus, there has been an error."
So that's how we create websites using S3.
It's very, very simple.
Essentially, all we need to do is apply a bucket policy
to our S3 buckets,
and that will make every object in there public.
And then all we need to do is go over to Properties
and enable static website hosting.
So let's move on to my exam tips.
So going into the exam,
if you see any scenario-based questions
about static websites,
I want you to instantly associate
the word "static website" with S3.
So just remember going into the exam,
bucket policies are used to make entire buckets public,
so you don't need to keep making individual objects public.
And that will save you a lot of time when you're dealing
with websites that have thousands of different HTML pages.
Also remember that it's static content only,
so you can only use S3 to host static content, not dynamic,
and dynamic is essentially where
it's got a database connection.
And then finally, automated scaling.
Just remember that S3 scales automatically with demand.
So it can be a great tool in a scenario-based question
where you're not sure of the demand.
It's a static website,
you may as well put it up on S3 because then
you don't have to deal with things like load balancers,
and EC2 instances, and provisioning, etc., etc.
So that is it for this lecture, everyone,
if you have any questions, please let me know.
If not, feel free to move on to the next lecture.
Thank you.


Versioning Objects in S3
========================

Okay, hello, Cloud Gurus,
and welcome to this lecture.
In this lecture, we're going to look
at versioning objects in S3.
So the first thing we're going to explore
is what is versioning,
then we'll look at the advantages of versioning,
and then I'll show you how to do it in the console,
and then we will go on to my exam tips.
So what is versioning?
Well, you can enable versioning in S3,
so you can have multiple versions of an object
within your S3 bucket.
It's a way of doing version control
with your objects or files.
So it's very, very easy to set up
and we'll look at how we can do it.
There is quite a few nuances with versioning.
So I'll show you some tips and tricks as well.
So the advantages of versioning is it allows you
to have all versions of your objects stored in S3.
So this includes all writes
and even if you delete an object.
It's a great backup tool.
And once versioning is enabled,
versioning cannot be disabled.
It can only be suspended.
So you can't get rid of versioning on your bucket--
all you can do is suspend versioning.
It integrates with these things called lifecycle rules,
which we will cover off in a later lecture,
and it supports multi-factor authentication,
and this can come up in your examine an awful lot.
So what the scenario question will basically be around
is, you need to protect your objects in S3
from being accidentally deleted,
and that includes all versions of your objects,
and how can you prevent that?
Well, you can enable multi-factor authentication
to delete objects, so you need 2 forms of authentication
in order to delete your objects.
So let's go over to the AWS console
and have a look at how we can turn on versioning.
Okay, so here I am in the AWS console.
What we're going to do is we're going to go over
to our website bucket,
and we're going to go over to Properties
and you'll see we've got Bucket Versioning in here,
and all we want to do is go in and hit the Edit button.
I'm going to go ahead and enable bucket versioning.
And as soon as we do that, versioning will be turned on.
Then what we want to do is let's go over to our bucket.
So go back over to our objects in our bucket,
and you can see our objects are in here.
I'm just going to go back and come back into this bucket
because the UI is a bit funny.
And so now you can see our 2 objects in here,
and we've actually got this little tab over here,
which says List Versions.
So you can see it in there.
So let's click on that and now we'll be able to see
the different versions of our objects.
So right now there is no versions of our objects.
We've just got our index.html, we've got our error.html.
These are all basically the first versions of our objects.
Let's go ahead and upload--or let's make a change
to index.html, and then upload it to the bucket.
So here's our index.html.
I'm just going to add a line of code in here.
I'm going to change it from Hello Cloud Gurus! to
Hello Cloud Gurus! THIS IS VERSION 2.
And I'm going to go ahead and save that.
And now what I want to do is I want to go over
and hit Upload,
and then I'm going to go in and add my files.
I can see my index.html in here.
So I'm going to go ahead and hit Open,
and I'm going to go ahead and hit Upload.
So that has now uploaded it and if I hit Exit,
if I click on this List Versions again,
I'll be able to see that we've got 2 different versions.
So we've got a new version ID.
So this is version 1, and this is version 2.
And then what I can actually do is click
on the object itself. I can then click on this object URL
and I'll be able to see Hello Cloud Gurus!
THIS IS VERSION 2.
So if I go back and if I come back to my object,
so click in here and again, turn versioning on.
If we click on this 1,
we'll be able to get a unique URL in here.
So I wonder what will happen if we click on it.
Well, because this is a previous version,
it is now, we've got access denied.
So we don't actually have permissions
to access this version.
So even when you have a policy that enables all objects
within your bucket to be public,
it doesn't apply to previous versions of those objects.
So that is also an important thing to remember
for your exam.
So in order to be able to view this,
we can go to Object Actions,
and we can go ahead and hit Make Public.
That will then make this version of the object public.
And then we can click back in here
and we should be able to click this
and we'll be able to see the original version.
So here we go, Hello Cloud Gurus,
and we don't have THIS IS VERSION 2.
Now what we're going to do is we're going to go back
to our bucket, and what we're going to do
is we're going to upload a third version.
So let's go over and make a change.
So we've got, Hello Cloud Gurus! THIS IS VERSION 2.
What I'm going to do is going to change this to version 3
and going to go ahead and save it.
I'm back in my bucket.
I'm going to go in and hit Upload, going to add my file,
and I'm going to do it index.html.
and I'm going to go ahead and hit Open
and then I'm going to go ahead and upload this.
So we've now uploaded version 3.
If we come back to our bucket and hit List Versions,
you'll be able to see the 3 different versions here.
So I've got version 1, version 2, and version 3.
Now, if we were to click on version 2
and we click on this,
we won't be able to see it because it isn't public.
So we need to first just go in and hit Make Public.
And we just click in here, Make Public.
And so that has now made the version public.
And if we click Exit,
we should now be able to click this
and we'll be able to see version 2.
And if we go back and we click on our just main index.html,
so we click in here and click in here,
we'll be able to see version 3.
So this is version 3.
Okay, so the next thing we're going to do
is look at how delete markers work.
So let's go back over to our bucket
and what happens if we go ahead and click in here
and hit Delete?
So we're going to go in and hit Delete.
So just type delete in here,
that's going to delete this object and then we hit Exit.
Now you can't see that index.html,
but because we've got versioning turned on,
it is still there.
You just need to click on the list object versions,
and you can now see we've got 3 different versions
of our object and then a delete marker sitting over the top.
So we still have all 3 versions of our objects
and we can see all the changes there
and we've got our delete marker in here.
And the way to restore an object is to delete
the delete marker.
So if we click in here and go ahead and hit Delete,
that will delete our delete marker.
So we have to type permanently delete
and hit Delete Objects.
Now, if we go back to our bucket,
we'll be able to see in here that that delete marker
has now been removed,
and if we turn versioning off or in the UI,
we'll be able to see we can now see the index.html.
So that is versioning in a nutshell.
So on to my exam tips.
Just remember that all versions of an object
are stored in S3, and this includes all writes
and even if you delete an object, like we just saw.
Versioning is a great backup tool,
and once you have set versioning up on a bucket,
it can't be disabled--it can only be suspended.
So versioning will always stay on, but you can suspend it.
You can't go and delete versioning from a bucket.
Versioning can be integrated with lifecycle rules,
and we're going to cover off lifecycle rules
in another lecture.
And it also supports multi-factor authentication.
This is where it'll come up the most.
Essentially, like I said,
if you get a scenario-based question,
they're talking about, you need to protect your objects
in a bucket from accidental deletion.
You want to turn versioning on,
that's 1 great way of doing it,
but you can also enable multi-factor authentication
so that you need 2 forms of authorization
in order to delete an object.
So that is it for this lecture, everyone.
If you have any questions, please let me know.
If not, feel free to move on to the next lecture.
Thank you.


S3 Storage Classes
==================

Okay. Hello, Cloud Gurus.
And welcome to this lecture in this lecture,
we're going to look at storage classes.
So we're going to start with all the different storage
clauses so far, we've been using S3 Standard.
So we'll just have a look at what that is.
And then we'll look at S3 Standard-Infrequently Access,
S3 One Zone-Infrequently Access.
Then we'll look at Glacier,
Glacier Deep Archive,
and then S3 Intelligent Tiering.
And then we'll look at how all the storage costs differ
between these different types of S3.
And then we'll go onto my exam tips.
So let's talk about S3 Standard.
It's designed for high availability and durability
and data is stored redundantly across multiple devices
in multiple facilities.
And essentially objects are always stored in
at least 3 availability zones,
and that's at least three availability zones.
So in us-east-1, for example,
it might be stored across 6 different availability zones.
And because it's stored across
at least 3 availability zones,
you get 99.99% availability of your data.
And then you get 11 nines durability
and durability just simply means they won't lose your files.
You get 11 nines durability,
and this is all underwritten by their SLAs.
And so S3 Standard is designed for frequent access.
So this is data that you're frequently accessing that you
want to store in the cloud
and it's suitable for most workloads,
and it comes as the default storage class.
So when you create an S3 bucket by default,
it's going to be set up on S3 Standard
and use cases include
things like websites, content distribution, mobile,
and gaming applications, and big data analytics.
Now let's move on to S3 Standard-Infrequently Accessed.
And this is designed as you might have guessed from the name
for infrequently accessed data.
So this still allows you to get rapid access to your data.
It's used for data that's accessed less frequently,
but still requires a rapid access when needed
and we're talking milliseconds access,
but you do pay to access your data.
So there is a low per gigabyte storage price
and then a per gigabyte retrieval fee when you try
and access your data.
And in terms of the use cases,
this is great for long-term storage,
could be great for backups
and a data store for your disaster recovery files.
And it also comes with 99.9% availability,
so only 3 nines and then 11 nines durability.
So that's S3 Infrequently Accessed as a storage class.
It's just called S3 Standard IA.
Moving on, we then have S3 One Zone Infrequently Accessed,
and this is like S3 Standard IA,
but data's stored redundantly just within a single
availability zone.
So it's only in a single availability zone,
still multiple facilities, but it is in one AZ.
And this will cost you 20% less than your regular
S3 Standard Infrequently Access.
And this is great for long lived infrequently access
noncritical data.
So you are taking a little bit more of a risk because
as I've explained before,
it's cross multiple facilities
or be multiple data centers,
but if there's some kind of flood or major storm,
it could knock out that availability zone
and then you would not be able to access your data.
So just remember that you'd only use
S3 One Zone Infrequently Accessed
if your data is non-critical
and it comes with 99.5%
availability and 11 nines durability.
So moving on to Glacier,
Glacier comes with 2 options
and Glacier's basically just
cheap storage and it's optimized for data
that's very infrequently accessed.
And I mean,
you'd probably only be wanting to access this data a couple
of times a year
and you pay for each time you access your data
and it's used for archiving data.
So every time you go into an exam and you see the word
archiving, I want you to think of Glacier.
Archiving is a Glacier service.
So we have 2 different options with Glacier.
We've got option one, which is just called Glacier.
This is long-term data archiving with the retrieval times,
that range from between one minute to 12 hours,
and this could be historical data
that's only accessed a few times a year.
We then have option two,
which is Glacier Deep Archive.
And this is archiving data that's rarely accessed.
And your retrieval time is by default 12 hours.
So this could be financial records that may be accessed once
or twice per year.
And you can basically afford to wait to retrieve this data
so you can wait at least 12 hours.
So really going into the exam,
you'll be given different scenario questions,
and it could be that you want to save the most amount of
money possible then that will be Glacier Deep Archive,
or it could be that you need to be able to access this data
within a 12 hour period
then you will want normal Glacier.
So that is Glacier in a nutshell,
it comes with 4 nines availability
and 11 nines durability.
So moving on,
let's go to a scenario where you don't know whether you're
going to be accessing your data frequently or infrequently,
or sometimes you need to access your data frequently.
And other times you'll be accessing it infrequently.
What storage tier should you choose?
Well, then we have this thing called S3 Intelligent Tiering,
and this basically moves your data automatically to the most
cost-effective tier based on
how frequently you access each object.
And it does this using machine learning.
So S3 Intelligent Tiering is a great way of saving money,
it's all automated.
And basically the machine learning algorithms
just look at how often these objects are accessed,
and then it will move it to the best tier
that will suit you.
And S3 Intelligent Tiering comes
with 4 nines availability,
and 11 nines durability,
and it allows you to optimize your costs.
So it has a monthly fee of 0.0025 dollars per 1000 objects.
So let's have a look at our performance
across the S3 Storage Classes.
So with S3 Standard all the way
through to S3 Glacier Deep Archive,
we're always going to have 11 nines durability.
The availability will change depending
on the storage class,
but you can see it's always going to be 99.9%,
depending on the storage class,
some will have greater availability such as S3 Standard.
Some will have less availability
such as S3 Intelligent Tiering.
We've got SLAs built in as well.
So there's are contractual SLAs with Amazon.
So moving on to availability zones.
So each tier has a minimum of 3 or more availability
zones, except for S3 One Zone Infrequently Accessed.
We've got our maximum capacity charge and our minimum
storage duration charges are summarized there.
Don't worry,
you won't really need to know those going into your exams.
Do note with retrieval fees,
that there are no retrieval fees for S3 Standard
and S3 Intelligent Tiering,
but all other storage classes
within S3 does have a retrieval fee.
Everything is object based storage
and every storage tier supports lifecycle transitions.
And we're going to have a look at that in the next lecture.
Let's have a look at the breakdown of our storage costs.
So you can see S3 Standard is up here.
So it starts with the first 50 terabytes per month.
Then we move on to the next of 450 terabytes per month
and becomes cheaper.
And then over 500 terabytes a month,
it becomes even cheaper as well.
And you'll see that as a common theme,
going through all the storage tiers,
the more you store data within S3,
the cheaper and cheaper it becomes.
So going into your exam,
just remember that S3 Standard is always going to be your
highest cost tier.
Then we have S3 Intelligent Tiering.
This is where you're going to have cost optimized
for unknown access patterns.
So if you get a scenario-based question where you just don't
know how frequently accessed your data is going to be,
I want you to immediately think of S3 Intelligent Tiering.
And then with the other 4 storage classes,
we have a retrieval fee.
So that's for S3 Infrequently Access,
S3 One Zone Infrequently Access,
and then Glacier and Glacier Deep Archive as well.
So in terms of my exam tips just really remember the 6
different storage tiers
and their different use cases.
So S3 Standard is suitable for most workloads.
S3 Standard Infrequently Access
is where you need long-term infrequently accessed
critical data that you are going to be able
to access immediately.
So you don't want to store it in Glacier.
You want to have access
within, you know, milliseconds essentially,
we then have S3 One Zone Infrequently Access.
So this is for
long-term infrequently access data that's non-critical cause
it's only in one availability zone.
We then have Glacier,
which is used for archiving,
and essentially you just do the 12 hour rule.
So if you need to access your data within,
let's say an hour,
then you probably want S3 Glacier.
S3 Glacier Deep Archive is the cheapest one,
and this is rarely accessed data.
And the retrieval times by default are going to be 12 hours.
And then finally we have S3 Intelligent Tiering.
This is always used for unknown
or unpredictable access patterns.
So to summarize, going into your exam,
just know the 6 different storage classes of S3
and their use cases.
You'll never be asked about availability or durability
or SLAs, et cetera, et cetera.
You literally just need to know the different storage
classes and their use cases.
So that is it for this lecture everyone.
If you have any questions,
please let me know if not,
feel free to move on to the next lecture.
Thank you.


Life Cycle Management with S3
===============================

Okay. Hello, Cloud Gurus.
And welcome to this lecture.
In this lecture, we're going to look at
lifecycle management with S3.
So we're going to explore what lifecycle management is.
Then we'll look at lifecycle management
and how we can use it with versioning.
Then we'll go in and do a demo.
And then we'll move on to my exam tips.
So what is lifecycle management?
Well, this basically automates moving your objects
between the different storage tiers,
thereby maximizing your cost effectiveness.
So it's a way of automating, moving your objects
to save you money.
So if you're not using an object
or it hasn't been accessed recently,
you can move it from S3 Standard to S3-Infrequent Access.
So, say, if it hasn't been accessed in the last 30 days,
it would automatically move to S3 IA.
And then if you haven't used that object in,
let's say, 90 days after it's been moved to S3 IA,
it would then be moved on to Glacier and archived off.
And of course, Glacier is a lot cheaper than S3 Standard.
So that's all lifecycle management is.
It's a way of automating the removal of your objects
from 1 storage tier to another to save you some money.
You can also use lifecycle management with versioning.
So you can actually use lifecycle management
to move different versions of the objects
to different storage tiers as well,
thereby saving you even more money.
So let's go into the AWS console
and have a look at how we can do this.
Okay, so here I am in the AWS console.
We're going to go over to S3, which is always under Storage.
And then what I'm going to do is
I'm going to go into 1 of the buckets I created earlier--
the ryan3141kroonenburg--
and this was the 1 that had our 2 image files on it.
And in this, I'm going to go ahead
and turn on lifecycle management and versioning.
So if I go into Properties, I can turn bucket versioning on.
So let's go ahead and enable.
Also, once you enable versioning on a bucket, like I said
in a previous lecture, you can only suspend it.
You can't remove it.
So just bear that in mind.
So versioning is now turned on.
Once we've done that,
what we're going to do is we're going to
go over to Management
and you'll see our lifecycle rules are in here
and we can go ahead and create a lifecycle rule.
So we'll call this rule MyLifeCycleRule,
something like that.
And then what we're going to do is
you can choose a rule scope.
So you can limit the scope of this rule
using 1 or more filters
or you can say that this rule applies
to all objects in the bucket.
So you could do it to subfolders in your bucket,
for example.
I'm going to do it to all objects in the bucket.
Just click in here and say that I acknowledge
that it will apply to all objects in our bucket,
and in here, this is our lifecycle rule action.
So you choose the action you want the rule to perform.
Now, because we have versioning turned on,
we can do it with current versions or previous versions.
What I'm going to do is I'm going to do it
using current versions. Why not?
And then we can also expire current versions of objects.
So I'm going to say Transition Current Versions
of Objects Between Storage Classes.
So the first 1 is Storage Class Transitions.
So this is where it's going to from S3.
And it makes sense to do it to Standard-Infrequent Access.
And in here, we're going to put in 30 days.
So we can then go ahead and hit add transition.
And then basically what we're saying is,
"Okay, after 30 days, move everything
"from S3 to Standard-Infrequent-Access."
And then we can go over to Glacier, and we can say,
"Okay, after 90 days, let's go ahead
"and archive things off to Glacier."
And you can add as many transitions as you want.
So what we'll do is we'll go in
and we'll hit I Acknowledge
and then we'll go ahead and create this rule.
So this has now created the rule.
And so now automatically, our objects will move
after 30 days into Standard-Infrequent Access
and then archive off to Glacier.
If you actually click in here,
you'll be able to see this in a timeline summary.
So it shows you what happens.
So on day 0 object's uploaded.
On day 30, objects are transitioned
to Standard-Infrequent Access,
and on day 90, objects are transitioned over to Glacier.
So that's all lifecycle rules are.
They do come up quite a bit in the exam
and let's move on to my exam tips.
So just remember going into your exam
that lifecycle management rules automate
moving of objects between our different storage tiers.
It can be used in conjunction with versioning.
So you can just do this for previous versions of objects
or you can do it for every object within a bucket.
And like I said, it can be applied to current versions
as well as previous versions.
So that is it for this lecture, everyone.
If you have any questions, please let me know.
If not, feel free to move on to the next lecture. Thank you.


S3 Object Lock and Glacier Vault Lock
======================================

Okay. Hello, Cloud Gurus.
And welcome to this lecture.
In this lecture, we're going to look at S3 Object Lock
and Glacier Vault Lock.
So we're going to start off with S3 Object Lock
and we'll look at what it is.
It comes with 2 different modes.
It comes with governance mode and compliance mode.
So we'll explore what they are.
We'll then look at retention periods and legal holds,
and then we'll move on to what Glacier Vault Lock is,
and on to my exam tips.
Now, this definitely comes up in the exam, so
you will need to know what S3 Object Lock is.
And essentially, you use it to store objects
using a WORM model.
And a WORM model just stands for write once, read many.
So you can only write once to your objects,
but you can read them many, many times.
And it can help prevent objects from being deleted
or modified for a fixed amount of time or indefinitely.
And you can use S3 Object Lock
to meet your regulatory requirements.
So it could be that you have regulatory requirements
from the federal government, for example,
and these regulatory requirements could require WORM
storage, or you could also use it to add an extra layer of
protection against object changes and deletions.
So we have our 2 different modes,
and we'll start with governance mode.
And in governance mode,
users can't overwrite or delete an object version
or alter its lock settings
unless they have special permissions.
So with governance mode, you basically protect objects
from being deleted for most users,
but you still grant some users permission
to alter the object or retention settings of that object
or even allow those users to delete the object if necessary.
And so this is the key thing.
Essentially, if you get a scenario question
where you do need some of your users
to be able to alter an object, or to delete an object,
then you want governance mode.
If, however, you want to make sure
that nobody can delete an object,
then you want compliance mode.
And this is where a protected object version
can't be overwritten or deleted by any user,
including the root user in your AWS account.
And so when an object is locked in compliance mode,
its retention mode can't be changed
and its retention period can't be shortened.
So you could set its retention mode and period for a year,
and you can't then go in and change that object,
even if you're the root user.
So compliance mode ensures an object's version
can't be overwritten or deleted
for the duration of the retention period.
So if you get a scenario-based question where it's talking
about protecting your data and so that no users
can go in and alter that data or delete that data,
then you want S3 Object Lock with compliance mode.
In terms of our retention periods--
so a retention period protects an object version
for a fixed amount of time.
When you place a retention period on an object version,
Amazon basically stores an S3 timestamp
in the object's version's metadata.
Metadata is just data about data.
And this basically indicates
when the retention period expires.
So you might want to place a retention period
for 10 days, 100 days, or a year.
And after the retention period expires,
the object version can be overwritten or deleted
unless you also place a legal hold on the object version.
And of course you might be wondering, "What's a legal hold?"
Well, legal holds basically are S3 Object Locks
that also enable you to place
a legal hold on the object version.
And so it's like a retention period,
except a legal hold prevents an object version
from being overwritten or deleted.
However, a legal hold doesn't have
an associated retention period
and remains in effect until it's removed.
So you have to remove a legal hold
in order to go in and delete that object.
So legal holds can be freely placed and removed by any user
who has the S3 put object legal holds permission.
So remember that going into your exam as well.
So that's Object Locks covered off.
Next thing we need to look at is Glacier Vault Lock.
So Glacier Vault Lock basically allows you to easily deploy
and enforce compliance controls for individual
S3 Glacier vaults with a vault lock policy.
So you can specify controls, such as WORM,
in a vault lock policy
and lock the policy from future edits.
And once locked, the policy can no longer be changed.
So essentially, Glacier Vault Lock
is a way of applying a WORM model to Glacier.
That's all Glacier Vault Lock is.
Just remember the WORM model,
which is write once and read many.
And if you hear that being applied
in a scenario-based question, they're talking about Glacier,
and you're going to want to store it in Glacier Vault Lock.
So onto my exam tips.
So if you see the term WORM in a scenario based question
and it's talking about S3,
then you're going to want to use S3 Object Lock
to store your objects using a write once, read many model.
Object Lock can be done on individual objects,
or it can be applied across the bucket as a whole.
And Object Locks come in 2 modes which we covered off.
So we have governance mode and compliance mode.
So remember that compliance mode is simply
where a protected object version
can't be overwritten or deleted by any user whatsoever.
Whereas governance mode basically prevents most users
but you can have some users who are able
to either alter an object or delete it.
Finally, moving on to S3 Glacier Vault Lock.
If you see a scenario-based question
where it's talking about WORM models with Glacier,
I want you to think immediately of S3 Glacier Vault Lock.
This allows you to easily deploy
and enforce compliance controls
for individual S3 Glacier vaults with a vault lock policy.
And you can specify controls, such as WORM,
in a vault lock policy
and lock the policy from future edits.
And once locked, the policy can no longer be changed.
So again, to summarize,
if you see a scenario-based question, and
they're talking about S3 and you want to use a WORM model,
I want you to think of S3 Object Lock.
That comes in 2 different modes,
compliance mode and governance mode.
If they're talking about a WORM model with Glacier,
I want you to immediately think of S3 Glacier Vault Lock.
So that is it for this lecture, everyone.
If you have any questions, please let me know.
If not, feel free to move on to the next lecture.
Thank you.


Encrypting S3 Objects
======================

Okay, hello, Cloud Gurus,
and welcome to this lecture.
In this lecture, we're going to look at how we can use
encryption to secure our S3 buckets.
So we'll look at the different types
of encryption available.
We'll look at how we can enforce server-side encryption
using bucket policies.
Then we'll go in
and actually use the console to encrypt our objects
and then we'll move on to my exam tips.
So it's really important
that you understand the different types of encryption
going into your exam.
It is tested all the time.
So first of all, we have encryption in transit.
So this is when you're sending objects
to and from your bucket.
And we do this using SSL certificates, TLS,
and typically you'll see this whenever you access,
you know, S3 using HTTPS.
The HTTPS just means that it's encrypted.
So the S at the end means it's encrypted using SSL.
It goes over port 443.
And that means that if there was anyone
in between your computer and, let's say, your S3 bucket
who was trying to eavesdrop,
well, they wouldn't be able to unlock the contents
of your information that you're sending
because it's encrypted.
Next, we have encryption at rest.
So as opposed to in transit,
this is when the object is actually inside S3
or sitting on your computer, for example.
That's all it means.
It's at rest, you're not sending it.
So we have encryption at rest,
which is known as server-side encryption.
And there's 3 different types.
So we have SSE, which is server-side encryption S3.
So S3 is where S3 manages the keys.
So AWS manage all the encryption and decryption for you.
You don't need to worry about keys or anything like that.
And it uses AES 256-bit encryption.
And this is the most common type of encryption,
it's the easiest to use.
Next, we have SSE-KMS.
And we're going to cover off KMS
in the security section of the course.
The KMS just stands for Key Management Service.
And this is essentially where you work with Amazon
on your own key management
using the Key Management Service.
So you can use KMS keys to encrypt your objects as well.
And then we have server-side encryption C, or SSE-C,
which is where you provide the keys.
So that's the 3 different types.
So we've got S3,
which is where it's just done by the S3 service.
KMS, this is where you use your keys
and it's provided by the KMS service.
And then there's customer-provided keys
and this is where you provide the keys.
And then finally,
we also have encryption at rest client side encryption.
So this will be where you encrypted your objects
on your desktop, for example,
and then you upload it to S3.
So you control the encryption and the decryption
and all your objects sitting inside S3 are encrypted.
Now your company might have a policy
where you have to enforce encryption.
And there's 2 ways of enforcing server-side encryption.
You can do it using the console,
and this is where you select the encryption setting
on your S3 bucket.
And this is--
the easiest way to do this
is just a checkbox in the console.
And that's what we're going to look at how to do
in the demo.
And then we also have it through bucket policy.
So you can also enforce encryption using a bucket policy.
And this method sometimes comes up in the exam.
So I'm going to go into the details of how it happens,
how it all works.
Don't worry if it gets a bit technical,
you don't need to remember it.
What you need to remember
is that you can enforce encryption using a bucket policy.
So if we were to upload a file to S3,
we're essentially sending this.
It's a put request.
So we're going to put a put request into S3.
And every time a file is uploaded to S3,
a put request is initiated by our browser.
And our browser is essentially
just sending this information here.
So in order to enforce server-side encryption,
we've got 2 different options.
You can see up here we've got our x-amz-
server-side-encryption.
So if the file is to be encrypted at upload time,
then the x-amz-server-side-encryption parameter
will be included in the request header.
And you've actually got 2 options.
You've got x-amz-server-side encryption: AES256,
which is basically just saying, "Use S3-managed keys."
And then we have x-amz-server-side-encryption KMS.
And this is where you're using KMS-managed keys.
And so this put request is put into the header.
So when this parameter is included
in the header of the put request,
it tells S3 to encrypt the objects
at the time of the upload
using the specified encryption method.
So this is--
basically, your browser is creating this put request
with those parameters.
Now, what you can do
is you can basically use a bucket policy,
and you can see the parameter down here in the orange.
So we've got our x-amz-server-side-encryption.
This is using AES256.
You can see it in there.
And so this request header tells S3 to encrypt the file
using server-side encryption S3
at the time of the upload.
Now, what you can do is you can create a bucket policy
that denies any S3 put request
that doesn't include the x-amz-
server-side-encryption parameter in the request header.
So that's how you can do it using bucket policies.
So don't worry if this is a bit technical.
The actual parameters,
they might test you on
in the Developer Associate Certification.
In the Solutions Architect Associate,
you probably won't see the specific parameters
in the question.
At a high level, all you really need to know
and to remember going into your exam
is that you can create bucket policies
that deny any S3 put request
that doesn't include encrypted objects.
So it doesn't include the encryption parameter
in the request header.
So you can enforce encryption using bucket policies.
So let's go over to the AWS console
and look at how we can create buckets
that are automatically encrypted.
Okay, so here I am in the AWS Management Console.
As always, we can find Storage, S3 under Storage.
So let's go ahead and click in there.
And what we're going to do
is we're going to create a new bucket
and we're going to call it "My Encrypted Bucket,"
something like that.
So I'll call it myacloudguruencryptedbucket
and then just some random numbers.
And I'm going to scroll down.
We will just block all public access settings
because there's no point turning on server-side encryption
and then having people being able to access our objects.
So I'm going to block it.
We scroll all the way down.
We can see here we've got default encryption.
And this is automatically encrypt new objects
stored in this bucket.
So let's go ahead and turn that on.
And then in here you can see
we've got our different options.
So we've got server-side encryption using S3
or we can use KMS.
I'm going to use S3 because that's the easiest.
So let's go ahead and hit Create Bucket.
That is now creating our bucket
and you can see our bucket is in here.
So let's go ahead and go in there.
And then we're just going to upload some objects.
I'm going to go in, add some files.
And I'm just going to go in--
let's go to my Downloads directory.
And I'm just going to upload a picture of the Netherlands.
This was actually taken a few days ago.
There's people ice skating in front of some windmills,
and it looks like a Dutch grandmasters painting,
but it is a real photo.
So I'm going to go ahead and upload that,
and go ahead and hit Upload.
And once it's uploaded, we'll be able to view our object.
So it has succeeded now.
If we click on the name of the object,
you'll be able to see in here that it is encrypted.
And we can see that just by scrolling down.
You can see here, Server-Side Encryption,
Default Encryption Enabled.
So it is encrypted on the server-side.
And that's it.
That's how easy it is to encrypt objects.
Now, like I said earlier,
you can also do this using bucket policies.
So you can go in onto Permissions
and create a bucket policy in here
that will only allow you to upload encrypted objects.
I'm not going to do that.
It's outside the scope of this course.
You will need to know how to do that
for the Developer Associate,
but for the Solutions Architect Associate
what's important is just to remember
that you can enforce encryption using bucket policies.
So let's go on to my exam tips.
So on to my exam tips,
just remember that you can do encryption
in a number of different ways.
We've got encryption in transit, which uses SSL.
So whenever you type in HTTPS,
you know that that's going to be encrypted in transit.
Then we have encrypted at rest
using server-side encryption.
So we've got server-side encryption S3.
We've got server-side encryption KMS,
and we've also got it where you manage your own keys,
which is server-side encryption customer managed.
And then we have client-side encryption.
And this is where you encrypt the files yourself
before you upload them to S3.
And then finally, just remember going into your exam
that you can enforce encryption using a bucket policy.
And you can do this encryption using a bucket policy,
which basically denies all put requests
that don't include the x-amz-
server-side-encryption parameter in the request header.
So that is it for this lecture, everyone.
If you have any questions, please let me know.
If not, feel free to move on to the next lecture.
Thank you.


Optimizing S3 Performance
==========================

Okay, hello, Cloud Gurus,
and welcome to this lecture.
In this lecture, we're going to look at
how we can optimize S3 performance.
So the first thing we're going to look at is S3 prefixes,
what they are. We'll then look at how we can use prefixes
to optimize S3 performance.
We'll then look at limitations with KMS--
so limitations when we're using Amazon's encryption service.
We'll then look at S3 performance in terms of our uploads,
then in terms of our downloads,
and then we'll go on to my exam tips.
So let's start with S3 prefixes.
So when we create a bucket,
we have our bucket name and then after our bucket name,
we can have folders, so we can have folder1
and then let's say subfolder1
and then we have our object name,
so it could be myfile.jpeg.
So the S3 prefix is just the folders inside our buckets.
Okay, that's all it is.
So I want you to think of folders
or directory inside your S3 buckets.
That is what an S3 prefix is.
So in this example, we've got another S3 prefix,
so we've got folder2. It also has subfolder1.
And so the prefix here would just be folder2
and then subfolder1.
And this is a completely separate prefix
because we've got folder1 and then folder2,
if you follow.
We then could have folder3
and have nothing inside it,
no other sub-directories,
but that would then be its own prefix,
and then we could have folder4 with subfolder4
and that would be the prefix.
So the prefix is just the folders.
It doesn't include the object name and the file type--
it's literally just the folders within the bucket.
So how can we use S3 prefixes to get better performance?
Well, S3 has extremely low latency
and you can get the first byte out of S3
within about 100 to 200 milliseconds
and you can also achieve a high number of requests.
So let's say 3,500 requests for put, copy, post, delete
and 5,500 get requests
or head requests per second per prefix.
So in other words, the more prefixes
that you have inside your S3 buckets,
the higher performance you're going to be able to get,
and the key number is to look at the 5,500 get requests.
So say I'm trying to access an object in my bucket,
I'm doing a get request,
I get 5,500 requests per second per prefix.
So if we wanted to get better performance out of S3
in terms of reads, what we would do is we'd spread our reads
across different prefixes or across different directories.
So, for example, if you're using 2 prefixes,
you'd be able to achieve 11,000 requests per second.
And in our last example,
we looked at 4 different types of prefixes.
So subfolder1--or, sorry, folder1, folder2,
folder3, folder4.
If we were to use 4 different prefixes,
then we're going to get 5,500 requests times 4,
which would give us 22,000 requests per second.
So the key thing to take away from this is the more folders
and subfolders you have in your bucket,
the better performance you can get from S3
for your applications.
Moving on to some limitations with S3.
So when you're using Key Management Service,
which is Amazon's encryption service,
if you're using SSE-KMS
to encrypt and decrypt your objects,
you have to keep in mind
that there's actually built-in limits within KMS.
So when you upload a file, you're going to call this thing
called a generate data key in the KMS API
and when you download a file,
you call the decrypt in the KMS API.
And the key thing, like I said, to remember
is that KMS comes with built-in limits.
Now the built-in limits are region specific.
However, it's going to be roughly 5,500,
10,000, or 30,000 requests per second
and uploading and downloading is going to count
towards your KMS quota.
And currently, you can't even request
a quota increase for KMS.
If you're getting an encryption question inside your exam,
you might want to consider just using the native S3
encryption that's built-in rather than using KMS.
If you're asked to troubleshoot a KMS question,
it could just be that you're hitting this KMS limit
and that could be what's causing your downloads
or your requests to be much, much slower.
So moving on to uploads,
and we're going to look at multipart uploads
and this is recommended for files
that are over 100 meg and it's actually required
for any files over 5 gigabytes in size.
And what multipart uploads does is basically
it allows you to parallelize your uploads--
I hate saying that word--parallelize your uploads.
And this basically allows you to increase your efficiency.
So you're basically taking a big file,
you're cutting it into chunks,
and then you're uploading those chunks all at the same time,
you're doing parallel uploads,
and this increases your efficiency.
So again, if you see something
about a scenario-based question where it's talking
about how you can get a better performance doing uploads,
I just want you to think of multipart uploads.
Then we have a very similar scenario for downloads.
So we've got S3 byte-range fetches
and this basically parallelizes your downloads
by specifying a byte range.
So you can say, "Hey, I want the first 128 bytes
"and then I want the next 128 to 256 bytes, etc."
And if there's a failure in the download,
it's only for that specific byte range.
So we've got our S3 bucket, we've got our users,
we've got a big file that's sitting inside our S3 buckets.
And of course you wouldn't do 128 bytes.
You might say, "Okay, I want to download 1 gig at a time
"or 1-gig chunks."
And let's say the file is a 5-gig file,
you could split that into 5 different 1-gig chunks
and download them all at the same time.
So that's all it is, it's parallel downloads,
and it's just called S3 byte-range fetches.
When you do uploads, it's called S3 multipart uploads.
So that's all S3 byte-range fetches are,
they can be used to speed up your downloads
and they can be used
to download partial amounts of the file.
And this could just be the header information, for example.
So on to my exam tips, just remember what a prefix is.
It's simply the folders and subfolders
within your S3 bucket.
The more prefixes that you use
with your applications using S3,
the better performance you're going to get.
So you can always achieve a high number of requests.
You can do 3,500 puts, copies, posts, and deletes
and 5,500 get and head requests per second per prefix.
So the more prefixes you have, the faster performance.
So you spread your data and your reads
across different prefixes.
I gave the example 5,500, so if you're using 2 prefixes,
you could then achieve 11,000 requests per second.
If you're doing it with 4 prefixes, you could achieve
22,000 requests per second.
Moving on to KMS, just remember if you are using SSE-KMS
to encrypt your objects in S3,
you must keep in mind that there are built-in limits.
So uploading and downloading data will count
towards the KMS quota.
It is region specific, but it's either going to be 5,500,
10,000, or 30,000 requests per second.
And currently you can't request a quota increase for KMS.
And just remember when you're uploading files
to use multipart uploads to increase your performance
when uploading your files to S3.
If there's any file that's over 100 meg,
then you should be using it.
And if you're doing it for any file over 5 gigs,
you're forced to use it.
And then when you're downloading files,
remember to use S3 byte-range fetches
to increase your performance
and it's basically doing the opposite.
You're splitting up the files into different byte ranges
and you're downloading them all at the same time.
So that is it for this lecture, everyone.
If you have any questions, please let me know. If not,
feel free to move on to the next lecture.
Thank you.


Backing up Data with S3 Replication
===================================

Okay, hello, Cloud Gurus,
and welcome to this lecture.
In this lecture, we're going to look at
how we can back up our data using S3 Replication.
Now S3 Replication used to be called
cross-region replication.
That was because you do it from 1 region to another,
but now you can do it just from 1 bucket
to another bucket in the same region as well.
So they've renamed it
and it's just called S3 Replication
rather than cross-region replication.
So in this lecture, we're going to learn
what, you know, S3 Replication is.
We are then going to go
and have a look at how we can do this in the AWS console
and then we'll go on to my exam tips.
So S3 Replication is exactly what it sounds like.
It's a way of replicating objects
from 1 bucket to another.
Now versioning must be enabled on both the source
and destination buckets in order for this to work.
And objects in an existing bucket
are not replicated automatically.
Once replication is turned on,
then all subsequent updated objects
will be replicated automatically.
So if you're going to do this,
it's best to create 2 new buckets
(a source bucket and then a destination bucket),
turn replication on, and then upload your objects.
And also remember going into your exam
that delete markers are not replicated by default.
However, you can turn that on if you want.
So deleting individual versions or delete markers
will not be replicated by default.
So let's go into the AWS console
and have a look at this in action.
Okay, so here I am in the AWS console,
let's go over to S3.
And we're going to go ahead and create 2 buckets.
So let's create our first bucket.
I'm going to call it mysource--
mysourcebucketacloudguru
and then just some numbers.
And then I'm going to do this in the US East,
Northern Virginia region.
And then I'm going to leave everything as default.
And like I said, you do need versioning turned on,
but I'm going to leave that disabled for now
and we'll see why in a second.
So let me go ahead and create that bucket
and then I'm going to go and create my destination bucket.
So we're going to replicate from our source bucket
to our destination bucket.
So let's go ahead and call it mydestinationbucket
acloudguru.
And then again, some random numbers.
And then I'm going to replicate this
all the way over to Japan.
So let's replicate this all the way over to Tokyo.
It's going across the Pacific.
So let's go ahead and create this bucket.
And that has now created our 2 buckets.
So we've got our source bucket and our destination bucket,
and you can see them in here.
So the first thing we're going to do
is we're going to go over to our source bucket.
And what I'm going to do
is I'm just going to upload an object
just to prove to you that
by turning replication on,
it's not going to do it automatically.
So again I'm going to choose that picture
of the Netherlands.
So let's go ahead and hit Upload.
That has now uploaded my object to Northern Virginia.
And then the next thing I'm going to do
is go in and turn on replication.
Now, if we go to Properties,
we can see that versioning has been disabled for now.
So it hasn't been enabled.
What we're going to do is
we're going to go over to Management,
and its Replication Rules are under our Lifecycle Rules.
So let's go ahead and create a replication rule.
And straightaway, it's saying,
"Hey, you need versioning enabled on the source bucket
"in order to do replication."
So let's go ahead and hit Enable.
And that has now turned versioning on.
In here, my replication rule,
I'll just call it replicationtojapan.
And in here we've got, "Choose whether
"this rule will be enabled or disabled when created."
Well, we want it obviously to be enabled.
Our source bucket,
we want this to apply to all objects in our bucket.
You can do it to subfolders if you wanted,
but I'm going to do it to all objects in our bucket.
And in here, we pick our destination bucket.
So you can click Browse S3
and you can go ahead and select the bucket that you want.
So, and you can see the different regions in here.
So I'm going to select my 1 in Japan.
And I'm going to go ahead and choose that bucket.
Now straightaway, it'll also give me another warning saying,
"Hey, you need versioning turned on
"on your destination bucket as well."
So let's go ahead and do that.
So we now have versioning enabled.
In here we will need to create a IAM role for replication.
So let's go ahead and just hit Create New Role.
And that will create the role automatically for us.
And then we've got optional encryption
and we can change our storage classes.
I'm going to leave everything as default.
This is quite important.
So you could basically have your source bucket as normal S3
and then your destination bucket
as infrequently accessed and save some money.
So you can change the storage class
at your destination if you want.
I'm just going to leave everything as default.
And then, like I mentioned before,
delete markers are not replicated automatically.
You can enable it, but you have to click in there.
I'm not going to do that.
I'm just going to leave everything as default
and go ahead and hit Save.
That has now created cross-region replication.
So we'll be able to see our replication rules in here.
And so if I actually come in and click on my source bucket,
go over to Management,
I can see my replication rules are in there
and you can actually go in
and view the configuration if you want.
So let's go over to S3
and let's go in and have a look at our destination bucket.
As you can see, right now there are no objects in it.
So that picture of the Netherlands that we uploaded,
that hasn't been replicated automatically
because that was already in the bucket
at the time that we turned replication on.
If we then go over to our source bucket
and we were to upload that file again,
let's go ahead and hit Upload, add our files.
And we're going to go in and add that picture again.
And then I'm going to go ahead and hit Upload.
That is now uploading.
You can see it's in progress and it's succeeded.
So we have now uploaded that over to our--
our source bucket.
If we hit Exit, we'll be able to see it in here.
And then what we want to do
is we want to go over to Amazon S3
and we'll go over to our destination bucket.
Now sometimes it can take a couple of minutes
and, you know, if you think about it,
it's going across the Pacific.
So it does take--it can take a little bit of time.
What I'm going to do is I'm just going to sit here
and pause the video.
In fact, I don't need to pause it.
There you go.
It has now replicated across,
from Northern Virginia to Japan.
So you can see our object is in there
and it's happened automatically.
If I went back over to my source bucket
and I went in to delete this object,
because I didn't check that box about delete markers,
the delete maker will not be replicated automatically.
That's what we would assume would happen.
So let's type in here delete
and go ahead and hit Delete Object.
That object has now been deleted.
If we go back over to Exit,
we can see that there are no objects in our bucket.
However, we know that,
because we've got versioning turned on,
if we just click on this little button,
we'll be able to see the object
and then we'll be able to see
the next version that we uploaded
and then the delete marker here.
If we go back over to S3 and go to our destination bucket,
you'll be able to see that the object is still in here
and the delete marker has not been replicated.
And you can go ahead and turn
delete maker replication on if you want,
but just going into your exam
remember that by default delete markers are not replicated
from 1 bucket to another.
So let's go on to my exam tips.
So on to my exam tips,
just remember what S3 Replication is.
It used to be called cross-region replication.
So if they haven't updated the questions
and you see the mention of cross-region replication
then that is just simply, the language has changed.
Just think of cross-region replication
equals S3 Replication.
So you can replicate objects from 1 bucket to another.
They can either be in the same region
or in different regions.
Objects in an existing bucket
are not replicated automatically.
And delete markers are not replicated
automatically by default.
However, you can enable that if you so want.
So that is it for this lecture.
If you have any questions, please let me know.
If not, feel free to move on to the next lecture.
Thank you.


S3 Exam Tips
=============

Okay, well, congratulations, everyone.
You have made it through the S3 section of the course.
So in this lecture,
we're just going to summarize everything that we learned
in this section of the course.
So going into your exam,
just remember the S3 is object-based storage.
It allows you to upload and store files in the cloud.
It's not suitable for operating systems or database storage.
So you can't just go in and install Windows or Linux
or run a MySQL database on S3.
It's suitable for files
that are up to 5 terabytes in size.
Your files can be anywhere from 0 bytes
to 5 terabytes, and you get unlimited storage.
So the total volume of data and the number
of objects you can store within S3 is unlimited.
Remember that files are stored in buckets,
and that S3 is a universal namespace.
So all a universal namespace means is you're not
allowed to have the same bucket name as other people.
And when you create a bucket,
it's always going to have the bucket name in the URL.
In this example,
I've got acloudguru then dot S3, then dot the regions.
It could be us-east-1 and then .amazon.aws.com
and then forward slash the key name.
And the key name is just the file name.
So it could be Ralphie.jpeg.
And remember that when you upload files to S3, your browser
always get an HTTP 200 status code.
Moving on to our S3 object tips.
Just remember that the key is the object name.
So it's the file name--so Ralphie.jpeg.
The value is the data itself,
which is made up of a sequence of bytes.
We then have our version ID.
We looked at that when we were doing versioning,
and this allows you to have multiple versions
of the same object, and then we always have metadata in S3.
And this is just data about your data,
so data about the data that you're storing.
So it could be the content type,
for example, if it's an image,
or when it was last modified, etc.
So moving on to securing your buckets with S3,
remember that S3 buckets are private by default.
When you create an S3 bucket,
it's private and all the objects within it are private,
and you have to go in
and allow public access on both the bucket
and its objects in order to make the bucket public.
Remember that you have object ACLs,
and this is basically access control lists that you apply
on individual objects using object ACLs.
So this is a way of giving permissions
to make individual objects public,
or you could put an object ACL saying
you're not allowed to delete this object.
We then have bucket policies.
Bucket policies are bucket-wide policies.
So you can make entire buckets public using bucket policies.
And I've already covered this off, but like I said
when you upload an object to S3 and it's successful,
you always get an HTTP 200 status code.
Remember that you can host a static website with S3.
You do this using bucket policies. It's best practice. So
basically, you create a bucket policy that makes every
object within your bucket public.
And this is basically used just for static content.
It's not used for dynamic content.
So if you need a database connection using S3,
you don't want to run your websites in S3.
You would then use something like EC2.
So it's static content only.
And remember that S3 automatically scales with demands.
You don't have to worry
about things like load balancers, etc.
S3 is a great place to store your static websites with.
So moving on to versioning.
So we looked at how we can use versioning
with our S3 buckets, and all versions
of an object are going to be stored in S3.
This includes all writes.
So we have our web page where we had version 1,
version 2, version 3.
And even if you delete an object,
all that's doing is placing a delete marker
over that object version.
And then you can basically remove this
by deleting the delete marker.
So versioning can be a great backup tool.
It can't be disabled,
once you turn versioning on,
it can only be suspended.
You can't go in and turn it off.
And just remember you
can use versioning with lifecycle rules,
so you can integrate it with lifecycle rules.
So you can basically move your older versions
to different storage tiers,
and it supports multifactor authentication as well.
Moving on to our different storage tiers.
This will come up an awful lot in your exam.
Just know there's 6 different storage tiers,
and know their different use cases.
So S3 Standard is suitable for most workloads--
so websites, content distribution,
mobile and gaming applications.
S3 Standard-Infrequent Access is good
for long-term, infrequently accessed but critical data.
So this could be your backups, your data store
for your disaster recovery, etc.
S3 One Zone-Infrequent Access is great
for long-term, infrequently accessed but non-critical data
because it is only going to be in 1 Availability Zone,
and they might even give you scenario questions
where it says company policy dictates
that it must be stored
in at least 2 or more Availability Zones.
Then straightaway, you can rule out
S3 One Zone-Infrequent Access storage as a storage class
in that scenario.
We then have Glacier and Glacier Deep Archive.
Just remember the retrieval times.
So essentially if you need it before 12 hours,
then you want S3 Glacier.
If you're okay to wait an average
of 12 hours or more, then you want Deep Archive.
If you want to save the most money,
then of course you always want Deep Archive.
And then we have S3 Intelligent-Tiering.
This basically just uses machine learning
to move your objects between the different tiers
to save you the most amount of money.
And it's used for unknown or unpredictable access patterns.
So moving on to lifecycle management,
so 3 tips for lifecycle management.
This basically automates the moving
of your objects between the different storage tiers.
It can be used in conjunction with versioning,
and it can be applied to current versions
and previous versions of your object.
We then looked at S3 Object Lock to store objects
using a write once, read many model.
So as soon as you see the term WORM
and it's talking about S3, I want you to think
of S3 Object Lock, and this can be on individual objects
or applied across the bucket as a whole.
And it always comes in 2 modes.
It comes in governance mode and compliance mode.
And just a gentle reminder of what each 1 of those is.
So with governance mode, users can't overwrite
or delete an object version after its lock settings
unless they have special permission.
So you can do it with a certain number of users.
But if you need to ban all users
from being able to access or to be able to write
and delete those objects, then you want compliance mode.
This basically stops anyone from doing it,
including the root account
or the root user within your AWS account.
If you see a WORM model,
but it's not talking about S3, it's talking about Glacier,
then you want S3 Glacier Vault Lock.
This allows you to easily deploy
and enforce compliance controls
for individual S3 Glacier vaults with a vault lock policy.
And you can specify a control, such as a WORM model,
in a vault lock policy
and lock the policy from future edits.
And once locked, the policy can no longer be changed.
Moving on to encryption with S3.
So we had 2 different types of ways we can encrypt data.
First of all, is sending the data to S3.
So this is encryption in transit.
So we can encrypt our data using SSL or using HTTPS.
So whenever you visit S3 using a browser,
you're going to visit using HTTPS.
And that means all the objects that you send to S3
will be encrypted in transit.
We then have encryption at rest
using server-side encryption.
So that's all SSE stands for is server-side encryption.
And it comes in 3 different flavors.
We've got server-side side encryption with S3,
and this is using the AES 256-bit encryption algorithm.
So this is where Amazon handles all
of the encryption on our behalf for us using the S3 service.
We can then also use a external service from S3, which
is Key Management Service, to encrypt our data as well.
And then we've also got it where the customer
handles the keys themselves.
So this is SSE-C.
And then we've got client-side encryption.
And this is basically where you just
encrypt the files yourself before you upload them to S3.
And you can enforce encryption with a bucket policy.
So a bucket policy can deny all put requests
that don't include
the x-amz-server-side-encryption parameter
in the request header.
Moving on to optimizing performance with S3.
So we looked at prefixes.
So a prefix is simply the folder
and then subfolder within a S3 bucket.
So in this example, we've got folder1
and then subfolder1, that is 1 prefix.
Remember that you can achieve a high number of requests.
So 3,500 put, copy, post, deletes
and then 5,500 get and head requests
per second, per prefix.
And of course the more prefixes you have,
the better performance that you get.
So if you have 2 prefixes, so 2 different prefixes,
you could then achieve an 11,000 requests per second
for any GET requests because it's spread over 2 prefixes.
If you did it across 4 prefixes,
you're going to be getting 22,000 requests per second.
So that's all prefixes are.
Just think of prefixes as folders and subfolders within S3.
The more of them that you have,
the better performance that you get.
Also remember that if you're using SSE (server-side
encryption) KMS to encrypt your objects in S3,
you must keep in mind that there are built-in limits.
The built-in limits are region specific,
but it's either going to be 5,500,
10,000, or 30,000 requests per second.
Uploading and downloading is always going to count
towards your KMS quota.
And currently you can't request an increase for KMS.
Remember, if we want to optimize our performance,
we can use multipart uploads to increase performance
when uploading to S3.
This should be used for any files over 100 megs,
but it must be used for any file over 5 gigs.
And similarly, if we are downloading data,
we can use S3 byte-range fetches to increase performance
to download our files from S3.
And this is basically just splitting the files
into a smaller versions,
and then parallelizing the downloads.
And then finally, moving on to S3 Replication.
Just remember what S3 Replication is.
This is where you can replicate objects
from 1 bucket to another.
It used to be that you would do this across regions.
So if the exam questions haven't been updated,
they could call it cross-region replication,
but you can also do it to buckets in the same region,
as well as different regions.
Just remember when you turn this on
that objects in an existing bucket are
not going to be replicated automatically.
And by default, delete markers are not
replicated automatically from 1 bucket to another,
but you can turn that on as an option.
So that is it,
for this section. You've done really, really well.
We're going to move on to EC2,
which is 1 of the bread-and-butter services of AWS.
It's a huge topic in the exam, and it's basically
just where you provision virtual machines in the cloud.
So if you've got the time,
please join me in the next section.
Thank you.



Elastic Cloud Compute (EC2)
============================

EC2 Overview
============

Okay, Hello, Cloud Gurus.
And welcome to this lecture.
In this lecture we're going to look at EC2.
So EC2 is 1 of the most fundamental services of AWS.
And for that reason, it comes up an awful lot
in your Solutions Architect Associate exam.
So if you want to have any chance of passing the exam
you need to learn EC2 inside out.
So in this lesson, what we're going to do
is we're going to start with the basics.
We're going to learn what EC2 is.
We're going to look at the different pricing options.
Then we'll go onto our exam tips.
So EC2 just stands for Elastic Compute Cloud,
or "EC2" is the way it's pronounced.
And this is basically secure, resizable compute capacity
in the cloud.
And basically all it is, is it's a virtual machine,
but this virtual machine is hosted in AWS
instead of your own data center.
So it's just a virtual server that sits in the cloud,
hosted on AWS.
And it's designed to make web-scale cloud computing easier
for developers, and we'll look at
what that means in the next slide.
But essentially what it does is it gives you
the capacity that you want when you need it.
And you're in complete control of your own instances.
It's not managed by AWS.
AWS can't go in and log into your EC2 server.
It's completely controlled by you.
And EC2 was launched in 2006,
and it was a complete game changer.
It literally changed the entire industry
pretty much overnight.
And the reason for that is a few reasons,
but 1 of them is you only pay for what you use.
So it changed the economics of cloud computing.
Before you'd have to go out and physically buy
or rent servers.
And they basically would have a very long-term contracts.
Great thing about EC2 is you don't need
to waste any capacity.
You can select the capacity that you need right now,
and you can grow and shrink your servers as you go.
And with on-premise infrastructure, like I was saying
you basically had to estimate your capacity,
and then you had these long-term investments
over 3 to 5 years.
So I used to work at Rackspace,
and we would get orders in from customers
and we'd have to try and figure out what kind of capacity
that they were going to need.
And the contract lengths would run
from anywhere to 1 to 5 years.
And at the time when I worked there, this was early 2010.
Most of it was physical infrastructure.
So they would have physical servers dedicated
to our customers.
And we had to basically estimate how much is this going
to grow over the next 5 years.
And take all that into account.
So you have to do a lot of capacity planning
with on-premise infrastructure.
And you always basically buy hardware
that you don't need now, but you know
you probably will need it in 2 years
so you have this expectation that your application
is going to grow into your hardware.
And because of that, you have a lot of wasted capacity
you're paying for stuff that you don't actually need.
And 1 of the big problems was it takes,
when I worked at Rackspace
and other managed service providers,
the average time to provision a server
would be 10 to 20 business days,
depending on the environment.
So a customer would place an order with me,
and within 10 to 20 business days later,
we'd have that order racked ready, networked,
basically ready to go for the customer.
Now in AWS release EC2, it's done using an API call.
You could literally have your EC2 instances
up in minutes, and you didn't have to wait for months.
All you needed was to basically provide credit card details.
Your infrastructure guys, didn't have to worry about
waiting for these things to be provisioned.
It would be done through software,
but basically via an API call,
and it would provision our EC2 instances in minutes.
And you could essentially within 5 or 10 minutes,
have your web servers, database servers,
application servers up and running
and being able to test and develop on them.
And you didn't have to wait these 10 to 20 business days
for this capacity to become available.
So it really did change the industry overnight.
So in terms of EC2 comes in these
4 different pricing options,
and you are going to be quizzed
about this a lot in your exam.
So let's cover off the 4 fundamental ones.
We've got On-Demand.
This is where you pay by the hour or second
depending on the type of instance you run.
We then have Reserved Capacity.
This is where you, essentially, you get into a contract
with AWS for between 1 to 3 years.
The longer the contract and the more you pay upfront
the greater discount you get.
So that's all you have to remember going into your exam.
If you want to save the most amount of money,
if you want to save money compared to On-Demand,
what you do is you reserve capacity you say,
Hey, I need 3 web servers this size,
I'm going to do it for 3 years,
I'm going to pay it all up front,
and you'll get the most discounts on that.
So that's the way of making it most cost efficient.
We then have Spot, and this is where you purchase
unused capacity at a discount of up to 90% of prices,
but prices fluctuate with supply and demand.
So with spot prices, basically the price
of your EC2 instance moves up and down
sort of like the way a share price moves up and down.
And we'll cover that off in a bit more detail later on.
And then we have Dedicated, and this is more expensive.
This is where you have a physical EC2 server
that's dedicated for your use.
It is the most expensive option
but there's a few reasons why you do that.
Now going into your exam, you are going to get
a whole bunch of different scenario questions,
and it will ask you what the best pricing options are
to suit that scenario, to suit your objectives.
So you do need to know the 4 different pricing options
and their use cases going into the exam.
So let's deep dive a little bit further
into the 4 different pricing options.
And we'll start with On-Demand Instances.
So where would you use an On-Demand instance?
Well, this is where you need flexibility.
So if you need the low cost and flexibility of EC2
without any upfront payment or long-term commitment.
Typical use case so this is where you're building
a site from scratch, and you just want to see
what the application looks like,
And you're coding it for the very first time.
It's also good and useful for short term.
So applications with short term or spiky
or unpredictable workloads that can't be interrupted.
Perhaps you're launching an application for the first time,
but you just don't know what the market is going to be like
it could go viral or people could not be very interested
and you don't want to waste your money
doing long-term commitments until you understand
how the market is going to react to your application.
And that's basically the third point as well.
You're just testing the waters.
So your applications are being developed
or tested on Amazon EC2 for the first time.
So typically, always think of On-Demand Instances
as where you need flexibility. It's short term,
and you're doing it for test and dev purposes.
That's typically where you would use On-Demand Instances.
Moving on to Reserved Instances,
and essentially it's the opposite of that.
So it's where you've got predictable usage.
This is where you understand the usage patterns
of your applications, and it's a steady state
predictability behind it.
It's also where you've got specific capacity requirements.
So you know that your web servers, your database servers
are going to need 4 vCPUs and 16 Gigs of RAM
or something like that because you've got the data
over the last 2 years to back that up.
It's also very useful way you can afford to pay upfront
maybe you've gone out and gotten VC funding,
so you're able to burn some cash for a couple of years.
You can make some upfront payments
to reduce the total computing costs even further.
And typically, the more you pay upfront, the longer
the contract term, the better discount you can get.
With a standard reserved instance,
you can get up to 72% off the On-Demand price.
So you can save more than 2/3 if you pay 3 years upfront
and you pay it all upfront.
We then have the Convertible Reserved Instances.
So these are similar to Standard Reserved Instances
except you have the option to change
to a different reserved instance type
with equal or greater value.
And so, as we go through this course, you'll start to learn
that there are different types of EC2 instances.
So of EC2 instances that will focus primarily on RAM.
You have compute-based EC2 instances.
You might have GPU-based EC2 instances.
And the cool thing about Convertible Reserved Instances,
it allows you to change between the different classes
of EC2 instances.
But you do save less. It's only 54% compared to On-Demand.
So if you want to save the most amount of money
then you want standard reserved instances.
And then finally we have Scheduled Reserved Instances.
And this could be, let's say you're running an intranet
inside your organization. You've got 2000 seats.
And every time everyone comes into work in the morning
about nine o'clock they all log into the intranet
for the first time.
So you might need a huge amount of capacity at 9:00 a.m
just to handle that.
But then you can scale back capacity as everyone has
first logged in and start sending their emails et cetera.
So you only need a capacity reservation
for let's say 2 hours.
That's what Scheduled Reserved Instances are.
They basically allow you to launch
within the time window that you find,
so you can match your capacity reservation
to a predictable recurring schedule that only requires
a fraction of the day, week, or month.
So that's all Scheduled Reserved Instances are.
Now the thing to understand about reserved instances
is they operate at a regional level.
So when you reserve an instance in say us-east-1,
you are just doing that inside us-east-1.
You can't take that reservation
and apply it to say the London region
or to Paris, et cetera.
So just to summarize your savings plans
with Reserved Instances, you can save up to 72%
versus On-Demand Instances regardless of the type or region.
Essentially what it is is the more you pay upfront
and the longer the contract term
then the greater discount that you get.
So if you commit to 1 or 3-year contract
and you pay for it all upfront, you will hit that 72%.
And just also remember that it is super flexible.
So you don't have to just use reserved instances with EC2.
You can actually use it with other technologies
like Lambda and Fargate, and we'll get into what they are
later on, but there are other compute technologies as well.
We're going to move on to Spot Instances.
Now we have a whole lecture on Spot Instances.
But I mentioned that the spot price moves
around sort of like a share price.
And essentially you basically say at which price
that you want to have instance capacity at
and when it hits that price you'll have your Spot Instances
as soon as it moves away from that price
you'll lose your Spot Instances.
So a good use case for Spot Instances
is where you have applications
that have flexible start and end times.
Now you wouldn't want this use Spot Instances
for a web server, for example,
because if the spot price changes,
your instances will get terminated
and your website will go down.
So you wouldn't use it for websites.
It's really good for applications that are cost sensitive
because you can get really, really low compute prices
using spot prices.
So if you're running applications in the middle of the night
on a Sunday morning when there's a huge amount
of capacity available, you'll get a much better price
than if you were doing it at 9:00 a.m
on Monday morning, for example.
And typically this is where you would use Spot Instances
where you've got an urgent need for large amounts
of additional computing capacity.
So you could do Spot Instances
for things like image rendering or genomic sequencing.
So this is where you might be trying
to come up with a vaccine for a new virus
or something like that.
Or algorithmic trading engine.
So you're trying to figure out whether Tesla is going to go
up or down using algorithmic trading
and then automate the sale and purchases of those shares.
So Spot Instances is where you are very flexible
on your compute capacity.
And you're trying to save the most amount of money.
You would not use it if you needed a guaranteed uptime
for example, like with web servers.
Now we will cover off Spot Instances in a lot more detail
in a lecture later on in the course.
Right now I just want to introduce what they are
and where you would use them.
So moving on to Dedicated Hosts
and we'll have a look at different use cases
for Dedicated Hosts, and we'll start with compliance.
So this is where you've got regulatory requirements
that may not support multi-tenant virtualization.
And by multi-tenant virtualization
all we mean is the underlying hardware
is shared with other AWS customers.
Now, the way they do it is
essentially they're using software.
So my EC2 instance, can't see
or talk to another customer's EC2 instance.
But, they are using the same processor, the same RAM,
they're on the same virtual hard disk potentially.
So in some regulatory environments,
whether it be banking perhaps,
or perhaps it's a government regulations, they will say,
"Hey, no you need to have this on dedicated hardware.
We don't want to take any risks that other tenants
on that server could access the data."
So that's where you get Dedicated Hosts,
if you've got compliance requirements.
You can also do it for licensing.
So this is where you've got licensing that aren't portable.
So it doesn't support multi-tenancy or cloud deployments.
So it could be some legacy Microsoft licensing
or Oracle licensing, and you need Dedicated Hosts
in order to obey your licensing conditions.
Good thing about Dedicated Hosts is they can be purchased
on-demand so you can pay for a dedicated host hourly.
It is going to be expensive though.
And of course you can also reserve them
so you can purchase a dedicated host for up to 70%
off the on-demand price, and like with anything
the longer your contract and the more you pay upfront
the greater the discount that you get.
Now, I would recommend that you go over to calculator.aws,
and this will allow you to use AWS pricing calculator.
And it allows you to figure out
what your infrastructure would look like on AWS
so you can go in there and add web servers,
database servers, et cetera.
That can come up as an exam question scenario-based question
where it's talking about you need to estimate
the costs of a move to AWS, what should you use?
Well, you can use the AWS calculator.
So check it out. It's at calculator.aws.
So that is it for this lecture.
Let's go onto my exam tips.
So EC2 is basically like a virtual machine.
It's hosted in AWS instead of your own data center.
You can select the capacity that you need right now.
You can grow and shrink it when you need it.
You only pay for what you use
and you only have to wait minutes
for an EC2 instance to provision.
You don't have to wait for months
in order to get access to it.
And remember the 4 different pricing models.
So we had On-Demand. This is where you pay by the hour
or the second depending on the type of instance
that you want to run.
It's great for flexibility.
We've got Reserved.
This is where you've got reserved capacity
for 1 or 3 years, you get up to 72% of the discount
on the hourly charge.
This is great if you've got no unfixed requirements
such as you need these dedicated web servers
or dedicated database servers.
We then have Spot.
This is where you're purchasing unused capacity
of a discount up to 90%.
But the price moves around all the time
with supply and demand.
So this is where you've got applications
with flexible start and end times.
And then finally we have Dedicated Instances
and this will be where you've got compliance
or licensing regulations to meet.
So it's still a physical EC2 instance.
You're not sharing it with anybody else.
And like I said, it's where you've got compliance
and licensing base requirements.
And again, we'll cover that off a little bit more detail
later on in the course.
So that is it for this lecture everyone.
In the next lecture we're going to go ahead
and get our hands dirty by provisioning an EC2 instance.
So if you've got the time
please join me in the next lecture.
Thank you.


Demo : Launching an EC2 instance 
================================

Okay, hello Cloud Gurus,
and welcome to this lecture.
In this lecture we're going to look
at how we can launch an EC2 instance.
So this is a demonstration,
and I'm going to now log into the AWS Console.
Okay, so here I am in the AWS Management Console,
I'm in the Northern Virginia region.
I'm going to use this as my default region
for this section of the course,
and you'll find EC2 under Compute.
And it's very first service under Compute.
And what we want to go ahead and do
is launch our first instance,
so go ahead and hit Launch Instance.
That will then bring up our wizard,
so the first thing we need to do
is choose an Amazon Machine Image,
and this is basically where we're going to choose
what type of AWS EC2 instance that we want.
And I always like to just use the default one
which is the Amazon Linux 2 AMI,
so let's go ahead and select in there.
You can also see that we've got
our different types of CPU architecture.
We're going to use the default one,
which is x86 64-bit.
So in here, we now choose our different types.
So we've got different EC2 families in here,
and then we've got different sizes of the families.
So T2 is just used for general applications
and t2micro is our smallest one.
And you can see in here we've got
a little green sort of UI thing that's saying,
"Hey, this is free tier eligible.
You don't need to pay for it."
So let's go ahead and hit Next.
So we're going to do a t2micro,
in here, we've got a whole bunch of different things.
So we've got the number of instances that we want.
Well, we only want one.
Our purchasing options,
whether or not we want spot instances,
we don't want that right now.
Our network. This is which VPC
we're going to launch this instance into.
So a VPC, all it is
is a virtual data center in the cloud,
so it's a virtual place where we basically
launch our EC2 instances.
We've got a whole section on VPCs coming up,
so we're going to leave that as is right now.
In here we've got our subnets.
So subnets are just different availability zones,
so one sub-net is always equal to one availability zone.
So this sub-net will be us-east-1d,
this sub-net will be us-east-1c,
and we'll cover off again
what subnets are in the VPC section of the course.
Let's just leave it as no preference.
We're going to do this in any availability zone.
We're going to actually basically
leave everything as default.
You can have a look at all the different options in here.
I'm not going to go through and explain them all,
you'll learn them as you go.
But essentially, all we want to do is
just leave everything as default
and go ahead and hit Next to add our storage.
So this is our virtual machine.
Our storage is a virtual disk in the AWS cloud,
and we'll cover this off in the next section of the course
where we deep dive into the different storage types.
Right now we're just going to again,
use the default one which is an 8 gig EBS volumes,
so this is a virtual hard disk in the cloud.
We can change all the different volume types in here.
I'm not going to explain what these are just yet,
until we get to the next section of the course.
Right now we're just going to focus on EC2.
EBS is in the next section,
and we're just going to leave everything again as default.
Next we can add our tags,
so in here you can have tags such as department,
so which department does this web server belong to?
Well, this could be to the IT Department,
or even there's a better one, Applications Teams.
So the Applications Team.
And we can also add another tag
so we could have our user ID,
so what's your staff ID number
or something like that.
And you can add as many tags as you want,
and this basically allows you to create data
about the resources that you are provisioning in AWS.
Next we move on to our Security Group.
So Security Group is just basically
a virtual firewall in the cloud,
and this is where you allow traffic in and out of your EC2
instance. So we are going to create a new
security group here. I'm going to call it WebDMZ,
and I'm going to use the same description as the name.
And essentially with this,
we're going to allow our port 80,
which is where we do our HTTP traffic.
We're going to open it up to the world,
so we'll be able to browse this EC2 instance,
so that it's a web service.
Anything inside this security group
will inherit these rules,
so if you have a thousand EC2 instances
inside our WebDMZ security group,
well they're going to be internet accessible.
So we're going to be able to access them via HTTP.
You can also see you've got HTTPS in here,
and we're going to choose HTTP straightaway.
You don't need to know your port ranges,
AWS will automatically fill it in,
but HTTP is on port 80.
HTTPS is on port 443,
so you could do both in there.
And SSH is just a way
in which we communicate with our EC2 instances,
our Linux EC2 instances.
So it's just a communication protocol
that we interact with our EC2 instances,
our Linux EC2 instances.
If it was Windows,
we'd be using RDP which is on 3389.
Again, we'll just leave it all as default.
In here, we've got what's called our CIDR address ranges,
and this essentially just says which IP address ranges
can access these different services,
these different protocols.
So if you do O.O.O.O/O
that means anyone on the internet
is able to SSH into this box.
They're able to access it via HTTP and HTTPS.
And here you can see we've got,
this is IPv6-compatible.
So it's just basically ::/O,
and that means anyone on any IPv6 address
can browse this using HTTP or HTTPS.
So let's go ahead and hit Review and Launch.
It will give us a little warning just saying,
"WebDMZ is open up to the world."
Don't worry about that,
we want to make it open to a world
cause this is our web servers,
we want to be able to access our web servers.
We can restrict so that people using port SSH
can't access it unless they're on specific IP addresses,
but for the purposes of this demo,
we're just going to go ahead and keep it simple.
In here, this is where we create a security key,
and a security key is basically a key that we use
to be able to administrate our Linux servers.
So it's our SSH key,
and it's just like a key that allows you
to get in to your EC2 instances.
Now, we don't have any keys at the moment,
so what we've got to do is we've got to go ahead
and create a new key pair.
I'm just going to call it My
and then Northern Virginia KP, so key pair.
And I'm going to go ahead and hit Download.
It will download that to your downloads directory,
so I'm just going to close that off.
And we are now ready to go ahead and launch that,
and we'll look at how we can use that a little bit later on.
Okay, so our instance has now launched.
Let's go ahead and view our instance.
So you can see our instance is now pending,
so it's in here. We haven't given it a name.
We can actually click in here
and just call it web server or something like that.
WebServer01,
and that's just adding a tag using the name tag,
and you can see that the instance state is pending.
If you get a bit bored, you can hit refresh
and there you go, it is now running.
It is up and running.
So let's have a look at how we can go ahead
and access this instance.
Well, in order to access it,
we can just hit Connect.
And in here we can use EC2 Instance Connect.
We can also use an SSH client.
This really depends on whether
or not you're using Windows or Mac,
and you would need to use your key pair in there.
To be honest for the rest of this course,
let's just use EC2 Instance Connect
because it is so super simple.
All you need to do is go in and hit Connect,
and then that's it.
You'll be able to connect in to your EC2 instance.
You don't need to worry about your keys at all.
Amazon handles all of this on the backend.
And you can see in here,
we have now connected to our EC2 instance,
and what we can do is we can go ahead and run.
We can elevate it privileges to root,
and so sudo su,
this makes us the root user
which basically means we've got God mode.
So you've got our EC2 user here,
and now we are at root levels,
so we're at administrator level.
And now we can go ahead
and do anything we want to this instance,
so we can run a yum update -yes,
and this will go ahead
and update our EC2 instance
with all the latest security patches.
So that is how we connect in.
Now you can use different SSH clients
to connect into your EC2 instance,
but for the purposes of this course,
I would just go ahead
and use EC2 Instance Connect
just cause it is so super straightforward and simple.
Now, that we've done that we can go back to our instance
and what we can go ahead, is go ahead,
and terminate this instance.
So let's go ahead over to Instance State,
and go ahead and hit Terminate Instance.
This will then delete our EC2 instance,
and we've only would have paid for the amount of time
that it was running in seconds,
cause this is a Linux EC2 instance.
So this is now been successfully terminated in here.
You hit refresh, so we have a look at our instant state in
here. You can see that it says Shutting Down.
Eventually it will change to Terminated,
and that means we've deleted our instance.
And then we don't have to worry,
we're no longer incurring any kind of fee on this.
So that is it for this lecture, everyone.
If you have any questions, please let me know.
If not, feel free to move on to the next lecture.
Thank you.


AWS Command Line
=================

Okay, hello Cloud Gurus
and welcome to this lecture.
In this lecture we're going to look
at the AWS command line.
So what we're going to do is just have a quick review
of what the console looks like.
Then we'll talk about what the command line, what it is
and have a look at one of the most basic commands.
Then we'll cover off what we're going to cover off
in this demo, the different steps that we'll go through,
we'll do the demo and then we'll move on to my exam tips.
So this is the AWS Management Console.
So this is the GUI way,
or the graphical user interface way of interacting with AWS.
It's basically where you've got your mouse,
you can point and click, and you can provision resources.
However, there's another way you can interact with AWS
and this is by using the command line.
And this allows you to interact
with AWS simply by typing commands.
So whenever you're using the command line,
you always start with AWS, and then you type the service.
So if we're using S3,
we type S3 and then LS just means list.
So this very simple command will say, "Hey,
tell me all of the S3 buckets that exist
in this AWS account."
So that's what we're going to do.
We're going to go have a look at how this works.
In terms of the lesson objectives,
the first thing we're going to do is launch an EC2 instance.
This is where we're going to use the command line.
Then we're going to create an IAM user
and we're going to give this user permissions to access
and create S3 resources.
And then we'll go in and configure the AWS command line.
So we're going to configure it using the user's credentials.
And then we're going to use the CLI to create an S3 bucket
and we'll even upload a file to that S3 bucket.
So that's what we're going to do in this lesson.
So if you've got the time,
please join me in the AWS console.
Okay, so here I am in the AWS console,
I'm going to go over to EC2 and we're going to go ahead
and provision a new EC2 instance.
So let's go ahead and hit launch instance.
We're going to use the Amazon Linux 2 AMI.
We're going to use a T-2 micro. Go ahead and hit next.
I'm going to leave it all as default.
So we're not going to add anything else in here.
Same with our storage.
So we're just going to leave that as default. In terms
of our tags, let's add a tag. We'll just do name,
so what's the name of this instance?
And I'm just going to call it CLI demo.
And then I'm going to go ahead and hit Next.
We're going to add this in to an existing security group.
So we're going to add it to our web DMZ
which we created in the last lecture.
And then I'm going to go ahead
and hit Review and Launch.
And in here I'm going to use my existing key pair.
So this is the key pair that I used
to SSH in to my last EC2 instance.
And I'm going to go ahead and hit Launch Instance.
So that is now launching my instance.
We can go in here and hit view
and here under instance state this'll tell us
when the instance has become live.
Over here you can see we've got our public IPv4 address.
If you actually click on the instance and you scroll down,
you can just click in here onto this icon
and it'll copy that into your clipboards.
That's a great way of being able to SSH in very easily.
So you can now see that is running.
So what I'm going to do is,
I'm going to SSH in to this EC2 instance.
So I'm going to do that using Terminal on a Mac environment.
Now you typically would want to just use this hit connect
and use EC2 instance connect.
The only reason I'm not doing that that way
is 'cause I want to be able to show you the text.
I can make it a lot bigger in Terminal.
I'm also a lot more comfortable working
with Terminal because I've been doing it for years.
EC2 instance connect is still relatively new.
And I also just want to show you how you need
to change some permissions
on your private key and how that works.
So let's go over to our Terminal window.
So for Mac users, in order to get into Terminal,
you just go over to applications, you go to utilities
and you open up Terminal.
Okay, so I'm logged into Terminal.
If I type in LS, I can see all my different directories.
So I just need to go over to my downloads directory.
So just type in CD (change directory) and then Downloads.
And that will then go into my Downloads directory.
Again, I can type in LS
and I can see my private key in there, MyNVKP.pem.
Now in order to be able to access my EC2 instance,
first thing I need to do is change the permissions
on this file.
And again, you only need to do this
if you're accessing it via a Mac and you're using Terminal.
If you're using windows,
honestly I just use EC2 instance connect.
It's just so much easier, but I'm just showing you this
for those Mac users out there anyway.
So you just type in the name of the key.
So you type in chmod 400, and then the name of the key.
That will change the permissions.
Then I'm going to go ahead and clear the screen.
So in order to connect into my EC2 instance,
I just type in SSH and then my username,
which is ec2-user@ and then I paste in the public IP address
which we got when we created our EC2 instance
and then -i, and then the name of our key pair,
so it was MyNVKP.pem. I go ahead and hit Enter.
It will then say you've never logged into this host before,
are you sure you trust it?
Just go ahead and type in, yes, and there you go.
You've now logged in using Terminal on Mac.
So let's go ahead and clear the screen.
And I'm going to go ahead
and elevate my privileges to root.
So I now have a root use access.
And the first thing we might want to do is just go ahead
and run yum update -y.
So we're just going to update this
to the latest security patches
just to make sure our EC2 instance, you know,
has all the latest security patches installed on it.
So that has now finished installing our security patches.
So we are using the command line already
but we're just using the command line inside our Linux AMI
with the Linux 2 AMI, the command line comes pre-installed
on our EC2 instance.
So if I go ahead and clear the screen, if you remember,
in order to access the AWS command line, we type in AWS,
then the name of the service and then LS.
So we're going to say, "I want to list all my S3 buckets,"
and I'm going to go ahead and hit enter.
Now straight away.
It gives us this error message,
it's like, well, I have AWS installed
but I don't have your credentials.
I don't know what I'm going to use to log into AWS.
So you need to run AWS configure.
We type in aws configure into the command line.
It's going to ask us for our Access Key ID.
Now currently, we don't have one,
so let's go ahead and create one.
So you need to go back to the AWS console.
Okay, so I'm back in the AWS Management Console
and I'm going to scroll down to security
and I'm going to go over
to identity access management or IAM.
And what we need to do is go in and create a new user
and a new group that will have access to S3.
So let's start with our group first.
So let's go ahead and create our new group.
And we'll just call this, we'll just call it S3 access,
something like that.
And we're going to go ahead and hit Next.
And in here, we're going to search for a policy.
So if you just type in S3,
we're going to do the S3 full access policy.
And so we're going to go ahead and hit Next.
And we're going to go ahead and create that group.
So anyone in this group
will have full administrative access to S3.
Let's go over now and create our user.
So we're going to go in and click here.
We're going to add our user.
And let's just say that this is John Smith,
something like that.
That's our username.
We're going to give this user only programmatic access,
so not console access.
So this user won't be able to log in to the AWS console
but they will be able to use an access key ID,
a secret access key, to use the command line
and interact using the command line.
So let's go ahead and hit next.
And we're going to add this user to our group.
We're going to add them to the S3 group.
I'm going to go ahead and hit Next.
I'm not going to add any tags,
and I'm going to go ahead and create my user.
Now that has now created my user
and straightaway I've got my access key ID.
I'm going to copy this into my clipboard
go back over to my Terminal window.
I'm just going to paste it in here.
So there's my access key ID.
Next, let's tab back out and get our secret access case.
So I'm going to go ahead and copy that into my clipboard.
Paste it in here, and in here our default region name,
I'm just going to hit Enter or default to us-east-1
and our default output format, again, just hit Enter.
And there we go.
Now, if I now type, aws s3 ls,
you can see we've got two S3 buckets
and these are the ones that I created in earlier lectures.
So I now have the ability to go ahead and interact
with S3 using the command line.
Let's clear the screen to make it a bit easier.
So we type in aws s3 mb.
So we're now going to go ahead and try and create a bucket.
I'm going to call it, it always starts with S3://
and then you type the name of your bucket.
And I'm just going to honestly do a whole bunch
of random letters and numbers.
And I'm going to go ahead and hit create
and that has now created the bucket.
And if I type aws s3 and then ls,
you can see that that bucket has now been created.
We go back over to the AWS Management Console
and we click on our services
and we go to S3 and we click in here under storage,
you'll be able to see that that bucket has now appeared.
So there is that bucket there.
Just to make my life a bit easier,
I'm going to copy the bucket name into my clipboard,
and again, I'm just going to clear the screen.
And so what we're going to do now
is we're going to create a file.
So I'm just going to say, echo "Hello"
and we're going to do that out to hello.txt.
And if we just type in ls,
we can now see that we've created this file.
And if you want to view the contents of the file,
you can just type in cat hello.txt,
you can see inside the text file, it has the word, hello.
So we've just created a file.
Next thing we're going to do is type in aw s3 cp,
and then the name of the file.
So we're going to copy hello.txt over to our bucket.
And then I'm just going to paste my bucket name in here.
And then I'm going to go ahead and hit enter
and you can see it says it has now uploaded hello.txt
to that bucket.
So let's go back over to our console.
And if I go into the bucket, I should now be able
to see straightaway it's there, hello.txt.
That's what the command line is in a nutshell,
that's how it works.
It allows you to interact
with the AWS platform using commands
rather than using the console.
So let's go onto my exam tips.
So onto my exam tips
and always remember the principle of least privilege.
So always give your users the minimum amount
of access required to do their job.
That's why we only gave them S3 admin access.
We didn't give them access to any other services
because we just wanted to demonstrate S3.
Next thing is to use groups.
So create IAM groups
and assign your users to those groups.
And group permissions
are assigned using IAM policy documents.
So we added the S3 IAM policy document,
the S3 administrator policy document
and of course our users
will always automatically inherit the permissions
of the group in which they're assigned.
Moving on to my three CLI exam tips.
So number one,
just remember that what a secret access key is
basically the password for using your AWS command line.
So you're only ever going to see this once
and if you lose it, you can delete the access key
and secret access key and regenerate them.
But you need to run AWS configure again,
you need to reconfigure the command line.
Don't share your key pairs.
So each developer should have their own access key ID
and secret access key, just like passwords,
they should never be shared.
And just remember that CLI is supported
on Linux, Windows, and Mac OS.
So you can actually go in
and install the CLI on your Mac or on Linux or Windows PC.
And you can also use it on EC2 instances.
So we just used it on an EC2 instance,
but you can actually install the command line
at home on your laptop or desktop
and then go in and provision resources
within AWS just by using the command line.
So that is it for this lecture everyone.
If you have any questions, please let me know, if not,
feel free to move on to the next lecture. Thank you.


Using Roles
============

Okay, hello, Cloud Gurus
and welcome to this lecture.
In this lecture, we're going and to look at using roles.
So first of all, we'll look at what is an IAM role.
We'll then look at the fact that roles are temporary.
We'll also look at what else roles can do.
And then I'll show you how to set up a role in the console,
and we will go through and create an EC2 unit
that will assume that role to create an S3 Bucket,
and then we will go on to my exam tips.
So what is an IAM role?
Well, basically it's an identity that you can create in IAM
that has specific permissions.
So a role is similar to a user,
as it is an AWS identity with permission policies
that determine what that identity can and cannot do.
So it's all just using the JSON and policy documents.
That's how you go ahead and create a role.
However, instead of being uniquely associated
with one person, a role is intended to be assumable
by anyone who needs it.
So it's not the same thing as a user.
It's basically designed to be assumed
by anyone who's going to need access to it.
And the important thing to remember
is that roles are temporary.
So it doesn't have the standard long-term credentials,
the same way passwords or access keys do.
Instead, when you assume a role,
it provides you with temporary security credentials
for your roles session.
So what else can roles do?
Well roles can be assumed by people, AWS architecture
or other system level accounts.
And roles can allow cross-account access.
And this basically allows one AWS account
the ability to interact with resources
in other AWS accounts.
So in this console demo,
what we're going to do is we're going to go in
and create an IAM role,
and we're going to ensure that it has S3 access,
we're then going to go to go ahead
and provision an EC2 Instance,
I'm going to log in to that EC2 Instance,
and we're going to make sure that we attach the role
that we just created to that EC2 Instance.
Once I'm logged in, we're going to go ahead
and create an S3 Bucket using the command line,
and then we're going to try and access S3
from our EC2 Instance.
So let's go ahead and log into the AWS Console.
Okay, so here I am in the AWS Console.
If I just go over to Services and I scroll down,
I'll find IAM under Security, Identity, and Compliance.
And it's in here,
we're going to go ahead and create a role.
Now I've been using this account quite a bit,
it's already have quite a few roles
that have been created previously,
you might not have that,
depends if you're using an AWS account
that you've used before.
So let's go ahead and hit Create Role.
And in here we can see the different types
of trusted entities.
So we can have an AWS service or another AWS account,
or web identity, or it could be using SAML federation
which is basically a corporate directory, et cetera.
What we're going to do is use it for EC2.
So it says here, common use cases EC2
allows EC2 Instances to call AWS services on your behalf,
and that's exactly what we're going to do.
So let's click on EC2,
and let's go ahead and hit Next to add our permissions.
So in here, we'll be able to see all our different policies.
If the policy is managed by AWS
then it's going to have the AWS icon in here.
And what you can do is there's so many of them
that you can scroll down endlessly,
but you can also just use the search bar.
So I'm just going to type in here S3,
and we can see we've got an Amazon S3 full access policy.
And you can actually click in there
and be able to have a look at the actual policy summary.
And it basically is just saying effect, allow,
action, S3, resource, everything.
So allow you to do everything in S3.
So let's click on that and go next to add our tags.
I'm not going to add any tags.
And now we just need to give our role a name.
So we'll say S3, and then admin,
and then access something like that,
and I'm going to go ahead and create the role.
That has now created the role,
and note that it's global,
so that role exists all over the globe basically.
So we don't have to worry about creating roles
in individual regions or anything like that.
Let's go over to our services and go to Compute.
So I'm going to go to EC2,
currently in the London region
because that's where I'm recording right now.
So I'm going to go ahead and launch an instance
by clicking in here,
and I'm going to use an Amazon Linux 2 AMI,
and I'm just going to use a t2micro.
And in here, I'm going to leave everything as default,
but you can see in here, we've got our IAM role,
and I'm going to add our S3 admin access to it there.
Next, I'm going to go in and add my storage,
I'm going to hit next,
not going to add any tags, configure my security groups.
I have a web DMZ security group
that will give me access to port 22 and port 80
from anywhere in the world.
And we go ahead and launch.
And I'm just going to use my existing key pair.
So that is now launching our EC2 Instance.
Role will be attached to it.
If you forget to attach your role,
you can go over to actions
and then you can go over to security.
And you see in here, you can do modify IAM role,
and so you can attach roles to your instances
after they have been created.
So I'm just going to pause the video
and wait for this instance to come up online.
Okay, so that EC2 Instance is now up online.
If I just make this a bit bigger,
I'll be able to go and get my public IP address.
I'm going to copy that to my clipboard.
I'm going to use the terminal window to log into this,
but don't forget, you can always just hit Connect
and then go ahead and use EC2 Instance Connect.
So to connect in, and I'm just going to type ssh ec2
and then -user, and then @
and then my public IP address,
and then -i,
and then I think it was called MyLondonKP.pem.
I'm going to go ahead and hit Enter.
Type yes, and there we go.
So I'm just going to clear the screen now.
So I've logged in to my EC2 Instance.
And if I just type in AWS S3 and then ls,
I'll be able to see all my S3 Buckets,
so I can see them in there.
So what I can do is I can do AWS S3 and then mb,
and then we're going to call it S3,
and then we just need to give our bucket a name.
So I'm going to call it acloudguru.
Problem I have is that I've been doing this for so long
that everyone's steals all the acloudguru bucket names,
so I'm just going to enter a whole bunch of random numbers
in here,
and hopefully, that will be available.
And if I type in AWS S3 and then LS,
I'll now be able to see my bucket.
So here it is here,
it's this acloudguru3549 et cetera, et cetera.
So one more thing we can do is we can just do echo
and then we can say, hello world, something like that.
And then we can output it to a text file,
so we just call it ACloudGuru.txt.
So that has now created a ACloudGuru.txt file
on our EC2 Instance.
And if we cat that file,
so if we just read the contents of it,
you can see that it says hello world.
So all we want to do now
is we want to copy this to our S3 Bucket.
So it's AWS S3 and then cp
and then we're going to copy our ACloudGuru.
So ACloudGuru.txt to S3,
and then it's our bucket name,
which is going to be acloudguru.
And so now if I just go ahead and hit Enter,
that has now copied it over to our ACloudGuru bucket.
So let's go ahead and have a look at that in S3.
So I'm back in the console.
Let's go over to Services,
and we can see S3 under Storage.
And if we just go into S3,
we'll be able to see our buckets in here.
We can now see our new bucket, ACloudGuru.
And if we go in there, we can see the txt file.
So that was super easy.
We were able to use a role to allow our EC2 Instance
to communicate and interact with S3,
and we went ahead and created a bucket
and then we created a file,
and then copied that file to the S3 Bucket.
So onto my exam tips.
What should you remember when using roles?
They're the preferred option from a security perspective.
And that's so you can avoid hard-coding your credentials.
You don't have to hard code your access key ID
and secret access keys.
You can basically use roles with EC2 for example,
to grant you temporary credentials.
And roles, roles are made up of policy documents,
so they control a role's permissions.
And you can actually go in and update a policy
attached to a role,
and it will take effect immediately.
So there's no waiting involved.
And you can also attach and detach a role
running to an EC2 Instance
without having to stop or terminate those instances.
So going into the exam,
you'll probably see a scenario question
where it's talking about whether you use access key IDs
or secret access keys for your application,
or if you should just grant your EC2 Instance
a role that gives you the ability to interact
with another service of AWS.
You always choose roles over hard-coding your credentials.
So that is it for this lecture everyone,
if you have any questions, please let me know,
if not, feel free to move on to the next lecture.
Thank you.


Security Groups and Bootstrap Scripts
======================================

Okay. Hello, Cloud Gurus
and welcome to this lecture.
In this lecture, we're going to look at security groups
and bootstrap scripts.
So the first thing we're going to look at
is how humans sense things or how we perceive things.
And then we're going to look at how computers communicate
and then we'll look at security groups
and then what a bootstrap script is.
And then we'll tie it all in together into a demo
and then we will go onto my exam tips.
So let's start with basic human sensors.
So we see, hear, and feel using different mechanisms.
So light, we see light to using our eyes.
With sound, we hear sound using our ears
and heat, we can feel heat using our skin.
So we have these different senses
and different ways of interpreting our outside environment.
And so that really leads on to how computers communicate.
So Linux, for example, when you want to administer Linux
you do this over SSH, and this is over a port number.
So it's over port 22.
Port 22 is the SSH port.
Windows uses RDP.
And this has done on port 3389.
So this is how you interface with Windows machines remotely.
So when you remote desktop in,
you're doing that over port 3389.
When you're accessing websites using HTTP,
that's on port 80.
And when you access HTTPS, that's on port 443.
And so when you communicate with a computer,
you do so over different mechanisms,
depending on what it is you're trying to communicate.
So if you're trying to communicate over SSH,
you want to do that over port 22.
If you're trying to just do some basic web browsing
that's not encrypted using SSL, then that's on port 80.
If you want to use the HTTPS site, that's on port 443.
So that's all you need to understand
about how computers communicate
for the purpose of this exam.
And of course, if you really want to get
into advanced networking,
we have a great advanced networking course
where we deep dive into advanced networking.
So that leads us onto security groups.
So security groups are basically just virtual firewalls
for your EC2 instance.
And by default, everything is blocked.
So when you create an EC2 instance,
you won't be able to SSH into it.
You won't be able to go and use it as a web server
because port 22 is going to be blocked
and port 80 is going to be blocked.
So in order to unblock it, you need to open up those ports
in a security group.
And to do that, you just basically open up
this IP address range, which is 0.0.0.0/0.
So in order to be able to communicate
to your EC2 instances via SSH or RDP or HTTP,
then you're going to need to open up the correct ports.
And we're going to do that in a console demo in a second.
Now in a production environment,
you'd only open up 0.0.0.0/0 to port 80 and to port 443,
so HTTP and HTTPS. You wouldn't do it for RDP or for SSH
because it means anyone could start brute forcing their way
into your EC2 instance.
But just for the purposes of this demo in this course
we're just going to open it up.
because most of the time your EC2 instances won't exist
longer than a couple of minutes,
so it's fairly safe.
Now bootstrap script is a script that runs
when the instance first runs.
So when you first boot up an EC2 instance,
a bootstrap script will go in
and start running at root level.
So it has root level permissions,
and it can go in and do anything.
So here's a good example of a bootstrap script.
So on the very first line, we've got the number symbol
and we've got an exclamation mark,
and this is called a shebang.
And then we've got /bin/bash.
And that's the path to our interpreter.
And our interpreter just basically takes our commands
and interprets basically what to do, how to run them.
So it's almost as if you're on the command line.
So anything you type underneath that will run automatically.
So here we've got yum install httpd -yes.
So we're installing Apache.
And then we're starting the Apache servers.
So yum service httpd start.
So by adding these tasks at boot time,
it adds to the amount of time it takes to boot the instance.
However, it allows you
to automate the installation of your applications.
So you can go in and create a web server straight away.
You don't need to log in and physically do it.
You can run a script that goes in,
installs the Apache servers and then starts that server.
So that's what bootstrap scripts are.
And they'll save you an awful lot of time in the long run.
So this is in the resources section of the course.
This is the bootstrap script we're going to run.
So here we've got our shebang and then the path
to our interpreter, and we're running a yum update.
We're installing Apache. We're starting the Apache servers.
We're going over to our var.www.html directory,
which is basically the root directory for our web server.
And then essentially we're just creating
a little index.html file that says, "Hello Cloud Gurus."
So all you need to do is copy that into your clipboard,
go over to the AWS management console
and we're going to go and use EC2, under Compute.
And what we're going to do is we're going to go ahead
and launch an instance and we're going to go in
and we're going to use an Amazon Linux 2 AMI.
And then what we're going to do
is we're going to use a t2micro.
Go ahead and hit Next.
Now, to do your bootstrap scripts,
you've got to go down here and it's under here as user data.
So this is what we're going to pass to our instance.
So all you need to do is paste that in there.
So you can see it in there.
Paste as text and that's our bootstrap script.
And we leave everything else as default.
And we're going to go ahead and add our storage.
We're just going to use our standard storage.
And in here, we've got our tags.
We're going to leave that.
And in here, we've got our security group.
So let's create a new security group,
and we, basically everything
in the security group is going to be a web server.
So we'll call it webDMZ.
So it's our web demilitarized zone, so DMZ.
In here, I'm going to add a rule.
And I'm going to go ahead and add HTTP.
So you don't need to know all your port numbers
and you'll never be quizzed on it
in the Solutions Architect Associate Exam.
Amazon pre-populates the port numbers here.
So if I select HTTP,
you can see that that is open to port 80.
If I add in HTTPS,
So we'll go in here
and HTTPS you can see automatically
it's going to populate that with port 443.
So that's a great way of saving some time.
So let's go ahead and hit Next,
and we'll go ahead and hit Launch.
And now we're going to launch our instance.
And go ahead and hit Launch.
And so that is now launching our instance.
So our instance is going to sit behind a security group
that's open to port 80.
So we'll be able to go ahead and view that.
If you actually click on security and scroll down.
You'll be able to see the security group here,
and you'll be able to see the rules that are associated
with it here as well.
So we're going to just pause the video
and wait for this instance state to come up.
Okay, so my instance state is now up and running.
And what I'm going to do is just going to go back
to the bottom of the page.
I'll just make this a bit bigger so I can see it
and scroll all the way at the top and go to Details.
And I'm just going to grab my public IP address
and copy that into my clipboard.
I've just pasted it in here,
and you can see it says, "Hello Cloud Gurus."
So essentially what we've done is we've automated
the deployment of a web server.
So we've gone in, we've started
and installed the Apache servers.
We've updated all the security patches.
We've started the Apache servers.
And then we've just created
a little web page that says, "Hello Cloud Gurus."
We haven't needed to log into the command line at all.
We've been able to do that automatically
using a bootstrap script.
And the reason we can go in and view our website
is because we've opened up port 80 on our security group.
Now, if you were to make any changes on the security group,
those changes will take effect immediately.
So if we were to delete port 80,
we would no longer be able to access
that EC2 instance over HTTP.
So do remember that also going into your exam.
So onto my exam tips. Like I said, that just remember
that changes to security groups take effect immediately.
You can have any number of EC2 instances
within a security group.
You can have multiple security groups attached
to EC2 instances as well.
And then all inbound traffic is always blocked
by default and all outbound traffic is allowed.
So that's all you need to remember
about security groups going into your exam.
And then with bootstrap scripts,
just simply remember what a bootstrap script is.
It's a script that runs when the instance first starts
and it passes all user data to the EC2 instance
and can be used to install applications like web servers,
which is what we just did
or databases as well as do updates and more.
Now a bootstrap script is called user data.
We also have a thing called metadata
and we're going to cover that off in the next lecture.
So if you've got the time,
please join me in the next lecture.
Thank you.


EC2 Metadata and User Data
============================

Okay. Hello, Cloud Gurus
and welcome to this lecture.
In this lecture we're going to look at EC2 metadata
as well as user data.
Now we covered off what user data was in the last lecture.
In this lecture, we're going to explore what metadata is
and then how we can combine metadata with user data
to make dynamic scripts.
So let's start with what EC2 metadata is.
We'll look at how we can retrieve it.
We'll look at how we can use user data to save our metadata.
We'll go through and I'll show that to you
in the AWS console.
And then we'll go onto my exam tips.
So what is EC2 metadata?
What is metadata?
We covered it off in S3
but metadata is just data about data.
So EC2 metadata is simply data about your EC2 instance.
And this can include things such
as your private IP addresses, public IP addresses,
the hostname of your EC2 instance,
what security groups it's in, et cetera, et cetera.
So it's just data about your EC2 instance.
So to retrieve our metadata,
all we need to do is type in a command,
and we type in the curl command.
And then what we're doing is we're going to
http://
and then it's 169.254.169.254,
/latest, and then /meta-data.
And then we can basically get a whole bunch
of different options so we can get things like
our hostname, our local IPv4 address.
We can get our public IPv4 address, et cetera, et cetera.
So that's all we do.
We use the curl command query about metadata
about our EC2 instance
and I'll show you how to do that in just one second.
What we could also do is use user data to save our metadata.
So in here we've got a bootstrap script
and we're running the curl command.
And so we're running curl http://169.254.169.254
/latest/meta-data
and then /local-ipv4.
And that's then going to save our IPv4 address
into a text file.
So let's call it myIP.txt.
So in this lecture,
what we're going to do is run this bootstrap script.
It's in the resources section of the course.
And essentially what we're doing
is we're just creating a web server.
We're starting the Apache servers,
and then we're making a website.
And it's basically running this echo command.
So it's saying my IP is
and then it's adding it to our index.html
and then we're running the curl command
and we're getting our public IPv4
and we're appending that to the index.html.
And then we're just closing out our HTML.
So it's a really, really simple bootstrap script
but running this bootstrap script will allow us
to get our public IP address by querying our metadata.
And this is a way that we can go in
and pass using user data to query our metadata
and then basically output it somewhere.
So let's go over to the AWS console.
Okay, so here I am in the AWS console
I'm going to go over to EC2 and I'm just going to go in
and terminate the instance that we set up
in the last lecture.
So we'd go to instance state, terminate instance
and terminate it.
And then what I'm going to do is I'm going to go over
and create a new instance, so hit Launch Instances up here.
We're going to use the Amazon Linux 2 AMI
and we're just going to use the t2.micro.
And then what I'm going to do
is I'm going to go down to my user data
and I'm going to paste it in here.
So you can see my bootstrap script is in here.
And then I'm going to go ahead and add my storage,
and I'm going to leave everything else as default.
I'm going to use my security group that we created
in the last lecture.
So we're using our web-DMZ security group,
and then I'm going to go ahead and hit Launch.
And I'm going to use my existing key pair.
So that has now launched.
Now, the first thing that we want to do
is check whether or not our bootstrap script has worked
has it gone through, queried our metadata
to give us our public IP address?
So what we're going to do is just going to wait
for this instance to come up online,
and then we're going to go ahead
and grab the public IPv4 address.
And then I'm just going to type that into my browser
and hit Enter.
So I'm just going to wait for this to come up online.
Okay, so my EC2 instance is now up online
I've got a public IP address.
I'm going to copy that into my clipboard
and we can see that that script works.
So our bootstrap script or our user data
has gone through and queried our metadata.
It's got our public IPv4 address
and has then written it to a HTML file.
So the next thing we're going to do is just have a look
at how we can query our metadata manually.
So I'm just going to log into the SSH terminal window.
So here I am in my terminal window.
I'm just going to type in ssh ec2-user
and then @ and then the public IP address,
and then -I and then my key name.
And then I'm going to go ahead and connect.
You might just want to do this using EC2 connect.
It's a lot easier.
I'm going to elevate my privileges to root,
and I'm going to go ahead and clear the screen.
So the first thing we're going to do
is we can just go over to our var/www/html directory.
So this is where our website is.
We type in ls we can see our index in there.
We can even display the contents of our file by doing that.
And so you can see that the file has been written.
Next thing we're going to do is run the curl commands.
So we type in curl
http://
and then it's just 169.254.169.254
and then /latest
and then /meta-data.
Then I'm going to go ahead and hit Enter.
And now you can see the different options that I get.
So we've got AMI-ID, we've got AMI-launch-index,
AMI-manifest-path, et cetera, et cetera.
So if I hit the up button and hit /
and let's say, I want to go and get my public IPv4 address.
So I type in public-
and then IPv4, IPv4 and hit Enter.
And there you go.
You can see it's displayed it down here.
So it's got my public IPv4 address.
So that's all you need to know
is how to use the curl command to query your metadata
for the exam.
Another thing you can also query
is your instance's user data.
So you just change the term here from metadata to user
and then data.
And that will actually show you
the bootstrap script that was run
when this instance was provisioned.
So you can query both your user data
and your metadata as well.
So onto my exam tips, just remember
that user data is simply bootstrap scripts
and metadata is simply data about your EC2 instance.
You will get scenario questions
where they're trying to confuse you
between user data and metadata.
Just remember metadata is data about data.
That's the way I always remember it.
So that's always to do with EC2,
whereas user data is your bootstrap scripts.
And also remember that you can use bootstrap scripts
or user data to access metadata.
And that is exactly what we did in this demo.
So that is it for this lecture, everyone.
If you have any questions, please let me know.
If not, feel free to move on to the next lecture.
Thank you.


Networking with EC2
======================

Okay, hello Could Gurus,
and welcome to this lecture.
This lecture, we're going to look at
our different networking options with EC2.
So we're going to explore what they are
at a very high level.
What they're going to look at ENIs.
We're going to look at it Enhanced Networking.
We're going to look at ENA versus VFs.
We're then going to look at elastic fabric adapters
and then we'll go onto my exam tips.
So let's just start
with the different networking options with EC2.
So you can attach 3 different types
of virtual networking cards to your EC2 instances
and the 3 are as follows.
You've got ENI which stands
for elastic and network interface.
And this is for basic day-to-day networking.
We then have EN, which is enhanced networking.
And this uses a thing called single root I/O virtualization
or SRIOV
And this allows you to basically have
very high performance networking
on your virtual network cards.
And then we have EFA, which is elastic fabric adapter
and this accelerates high-performance computing
and machine learning applications.
So those are our three different types.
Let's start with ENI.
ENI is simply a virtual network card
that allows you the following options.
So you get a private IPv4 address,
you get a Mac address, you get a public IPv4 address,
you get one or more security groups
which is basically just a virtual firewall in the Cloud.
And then they also allow many IPv6 addresses.
So that's all an ENI is.
And basically where you'd use an ENI
is if you want to create a management network
or you want to use network
and security appliances in your VPC
and you can use ENIs to create dual-homed instances
with workloads and roles on distinct subnets.
This allows you to have private network addresses
that are separate,
so you could have 10.0.1.0 as your management network
and then 10.0.2.0
as let's say another internal network that's separate
from your management network
and ENIs are a way of creating a low budget,
high availability solution.
So ENIs are very, very common
when you create a instance by default,
it will have an ENI attached to it.
So what is enhanced networking?
Well, this is the high performance networking
between 10 gigabits per second
and 100 gigabits per second.
And it uses single root I/O virtualization
or SR-IOV to provide higher I/O performance
and lower CPU utilization.
And so enhanced networking just provides higher bandwidth,
higher packets per second performance,
and it gives you a consistently lower
inter-instance latency.
So with enhanced networking,
it comes into different flavors.
You can use it using elastic network adapter, or ENA,
and you could also use it using the Intel 802599
virtual function or VF interface.
Now your elastic network adapter
basically supports network speeds
of up to a hundred gigabits per second
for supported instance types.
Whereas your Intel 82599 virtual function interfaces
only support speeds of up to 10 gigabits per second,
for your instance types.
And this is typically used on older instances.
Now, if you come up with a scenario question in your exam,
it's talking about using enhanced networking,
and then it's saying,
hey should you be using it in an elastic network adapter?
Or should you be using the Intel 82599
virtual function interface?
Well, to be honest
you always want to be using the elastic network adapter
always choose that over the VF interface
because it's a lot faster
and it's a lot more modern.
Moving on to EFA.
So what is an EFA?
Well, this is just an elastic fabric adapter
and it's a network device that you can attach
to your Amazon EC2 instances
to accelerate high performance computing
and machine learning applications.
And it's going to provide you with a lower
and more consistent latency and a higher throughput
than TCP transport traditionally used
in cloud-based HP systems.
So if you see anything again,
in a scenario-based question where they're talking
about high performance computing
and what network adapters should you use,
I want you to think of elastic fabric adapters.
So elastic fabric adapter can also use this thing
called an OS-bypass.
And this basically makes it a lot faster
with a much lower latency
and OS-bypasses enable high performance compute
and machine learning applications
to bypass the operating system kernel
and communicate directly
with the elastic fabric adapter device.
Now it's not currently supported with Windows,
it's only supported with Linux,
but again if you see anything about an OS-bypass
and how you can enhance your HPC
and machine learning applications,
I just want you to think of using OS-bypass
with elastic fabric adapter.
So onto my exam tips.
There is a entire certification
on networking called advanced networking specialty with AWS.
So you really don't need to know a huge amount
of advanced networking.
You just really need to know the 3 different types
of network adapters that are available to you
and what scenarios you should use those for.
So we've got ENI and this is basic networking.
So perhaps you need a separate management network
for your production network or a separate logging network
or even a monitoring network.
And you need to do this at a low cost.
Well, in this scenario, you want to use multiple ENIs
for each network and then have enhanced networking.
This is where you need speeds
between 10 gigabits per second
and 100 gigabits per second,
and essentially use this way anywhere you need
reliable and high throughput.
And then we have our EFAs
and every time you see the term high performance computing
or machine learning,
I want you to think of EFAs straight away
or if you need to do an OS level bypass.
So if you see a scenario question
talking about HPC or machine learning
and asking what network adapter you want to use,
choose an elastic fabric adapter.
So that is it for this lecture everyone.
If you have any questions,
please let me know, if not,
feel free to move on to the next lecture.
Thank you.


Optimizing with EC2 Placement Groups
=====================================

Okay, hello Cloud Gurus and welcome to this lecture.
In this lecture we're going to look at
how we can optimize EC2 using placement groups.
So, first we're going to look at our
three different types of placement groups.
We're then going to deep dive into
clustered placement groups, spread placement groups,
partition placement groups,
and then we'll move on to my exam tips.
So, the 2 types of placement groups are, guess what?
Clustered placement groups, spread placement groups,
and partitioned placement groups.
Well, what does that actually even mean?
Well, let's look at a clustered placement group.
And this is basically a grouping of instances
within a single availability zone.
And this is recommended for applications
that need really low network latency,
high network throughput, or both.
And essentially because they're
in the same availability zone
they're very very close to each other.
So, it's a way of speeding up the rate
at which your EC2 instances communicate.
And only certain types of EC2 instances can be launched
into a clustered placement group
Moving on we then got spread placement groups.
And this is where you've got a group of instances
that are each place on distinct underlying hardware.
And this is recommended for applications
that have a small number of critical instances
that should be completely separate from each other.
So, they spread out rather than being close together.
This is used for individual instances.
So, you might say, hey I don't want my primary database
to be on the exact same hardware as my secondary database
or my backup database.
I need them to be on different hardware.
So, that's where you would use spread placement groups.
And then we have partition placement groups.
And this is where every partition placement group
has its own set of racks.
And each rack has its own network and power source.
So, no 2 partitions within placement groups
share the same racks which allows you to isolate
the impact of hardware failure within your application.
So, EC2 divides each group
into logical segments called partitions.
So, partition basically it's a rack.
It's a way of grouping your EC2 instances
into dedicated network and power sources.
So, that's all it is.
And this is used when you've got multiple instances
and you want them to be on their own dedicated network
and power sources.
So, going into your exam
just remember three different types of placement groups.
So, if you've got EC2 applications
that are running high performance compute
and you need really low network latency
or high network throughput
then you are going to want a clustered placement group.
If you've got individual critical EC2 instances
that need to be on their own dedicated hardware
then you want to use spread placement groups.
And if you've got multiple EC2 instances
so this could be things like HDFS or HBase and Cassandra
and they need to be on their own racks
and dedicated network infrastructure
then you've got petitioned placement groups.
So, clustered placement group
can't span multiple availability zones
whereas a spread placement group
and a partition placement group can.
And only certain types of instances
can be launched in a placement group.
So, these are compute optimized, GPU optimized,
memory optimized, or storage optimized.
And AWS recommends that you have homogenous instances
within a clustered placement groups.
You always have the same type of instances
within a clustered placement group.
Also remember you can't merge placement groups.
You can't have 2 different clustered placement groups
and then merge them together
and you can move an existing instance
into a placement group but
before you move the instance
the instance must be in the stop state and then you can move
or remove an instance using the command line, the SDK,
but you can't do it using the console just yet.
So, that's all the placement groups are
they're a way of logically grouping your EC2 instances
depending on what it is you want to do.
You just need to remember the 3 placement group types
going into your exam and the different are
use cases for those placement groups.
So, that is it for this lecture everyone
if you have any questions please let me know.
If not feel free to move on to the next lecture.
Thank you.


Solving Licencing Issues with Dedicated hosts
=============================================

Okay. Hello Cloud Gurus
and welcome to this lecture.
In this lecture, I just wanted to explore
a little bit deeper about solving licensing issues
with Dedicated Hosts.
So a few lectures ago, we looked at
the different types of pricing models with AWS.
So we had our On-Demand,
we had Reserved Instances,
we had Dedicated Hosts,
and we have Spot.
In here, we're just going to look at Dedicated Hosts.
In the next lecture, we'll look at Spot,
and we'll just explore that in a bit more detail,
and then we finish with this section of the course.
So, in terms of this lesson,
we're just going to look at the different pricing models.
Just have a reminder.
We're going to look at Dedicated Host,
and we'll go on to my exam tips.
So like I said, we had On-Demand,
which is you pay by the hour, or by the second,
depending on the type of instance you run.
We then have Reserved,
and this gives you the most amount of savings
possible compared to On-Demand.
So you can reserve capacity for 1 to 3 years.
The more you pay up front, the more discount you get.
You can get up to 72% discount on the hourly charge.
We have Spot, and this is where you're purchasing
unused capacity at a discount of up to 90%.
But the prices fluctuate with supply and demand.
And then finally we have Dedicated,
and this is a physical EC2 server
that's dedicated for your use.
And this is the most expensive option.
So we looked at different use cases for this
and we talked about compliance.
So this is where you've got regulatory requirements
that may not support multi-tenant virtualization
or it could be that you've got licensing requirements.
So this is great for licensing
that doesn't support multi-tenancy or cloud deployments.
So this could be Microsoft licensing
or Oracle licensing, et cetera.
You can purchase Dedicated Hosts On-Demand,
so you can pay for them by the hour,
and you can reserve Dedicated Hosts as well.
For, and you can save up to 70% off the On-Demand price
if you go ahead and reserve your Dedicated Hosts.
But really the whole point of this lecture
is just to tell you this exam tip.
So any question that you see
about special licensing requirements,
I want you to think of Dedicated Host.
So Dedicated Host is just simply a physical server
with an EC2 instance capacity
that's fully dedicated to your use.
Dedicated Host allow you to use
your existing per-socket, per-core
or per virtual machine software licensing.
This can include things like Windows Server,
Microsoft SQL Server, SUSE Linux Enterprise Server,
and some Oracle licensing as well.
So really any exam scenario question
where it's talking about
you've got special licensing requirements
or compliance requirements.
I just want you to think of Dedicated Host.
So that is it for this lecture everyone.
If you've got any questions, please let me know.
If not, feel free to move on to the next lecture.
Thank you.


Timing Workloads with Spot instances and Spot Fleets
=====================================================

Okay, hello, Cloud Gurus.
And welcome to this lecture.
In this lecture we're going to do a deep dive
on Spot Instances and how we can time
our workloads using Spot Instances and Spot Fleets.
So we're just going to do a reminder of what Spot Instances
are, we're then going to go ahead
and look at our spot prices.
We're going to look at what Spot Blocks are.
We're going to look at different use cases.
And then going to look at what happens
when our Spot Instances get terminated.
We'll then look at a Spot Fleets.
We'll look at Launch Pools, we'll look
at different strategies, and we'll move on to our exam tips.
And then you're basically done
with this section of the course.
So a Amazon EC2 Spot Instance
lets you take advantage of unused
EC2 capacity in the AWS Cloud.
And basically Spot Instances are available
for up to a 90% discount compared
to on-demand prices but they come with a catch.
So where would we use Spot Instances?
Well, basically Spot Instances are used where
you need stateless, fault tolerant, or flexible
applications. So you wouldn't use Spot Instances
with things like web servers, for example,
simply because a web server needs to be on
and running all the time.
Where you would use it is for your applications
that are big data applications,
where you've got containerized workloads, CICD,
where you've got high performance computing
and then other test and development workloads.
So that's where you would use Spot Instances.
So let's talk about Spot Instances.
So to use Spot Instances,
you must first decide on your maximum spot price.
And the instance will be provisioned so long as
the spot price is below your maximum spot price.
So let's just say you were prepared to pay
let's say a dollar an hour just to throw something
an easy figure out there.
If Amazon spot prices are 50 cents an hour
then your instances will be provisioned.
If it then raises up to say $1.50 an hour
then because your maximum price is $1
your instances will either be stopped
or terminated depending on what you choose.
So the hourly spot price basically varies depending
on the capacity and the region.
So it's different even
down to the individual availability zones it's different.
So it's not just the regions itself.
It's completely different depending on the capacity
of the availability zones, as well as the regions.
And like I said, if your spot price goes above the maximum
then you have 2 minutes to choose
whether or not you want to stop or terminate your instances.
So you can either stop them and resume the workloads
once the spot price has gone below your maximum spot price
or you can go in and terminate your EC2 instances.
It depends on your use cases.
Now there is a thing called Spot Blocks
and you can use Spot Blocks to stop your Spot Instances
from being terminated
even if the spot price goes over your maximum spot price.
So you can set your Spot Blocks
for between 1 to 6 hours currently.
So let's say you've got a workload that absolutely
cannot be terminated even if the spot price
goes above your maximum spot price,
you can use a spot block to stop those instances
from being terminated or stopped
but only for 1 to 6 hours currently.
So the great thing about AWS is you can actually
get all the historical data for Spot Instances.
And you can break this down across your different
operating systems, against your different instance types,
the different EC2 families types.
You can do this over a specific date ranges
and then it breaks it down to individual availability
zones and it compares it with the on-demand price.
So in this one, you can see that for some reason
us-east-1f has been significantly more expensive
than the other availability zones within us-east-1.
So you maybe you wouldn't want to provision
your Spot Instances in us-east-1f
maybe you would want to do it in say us-east-1d,
which seems to be the lowest cost availability zone
within the us-east region.
So it gives you great flexibility
in terms of picking your maximum spot price
just by doing some research onto your pricing history.
So we talked about where Spot Instances are useful.
So we talked about the different tasks
of big data and analytics, containerized workloads,
CI and CD, and testing, image
and media rendering or high performance computing.
And to be honest, Spot Instances mostly come up
in the exam around high performance computing.
So where Spot Instances are not useful
for things like persistent workloads.
So for example, web servers or database servers
where you're always going to have a load on them.
Or things like critical jobs
because you don't want your Spot Instances to be stopped
or terminated in a critical job.
And like I said, databases
so you definitely don't want your database running
on a Spot Instance because it can be terminated.
It can be stopped
and then you won't have access to that data.
So this is where you would not use Spot Instances.
And I cannot tell you how many times it will come up
in a scenario-based question where you
have to decide whether you're going to be using on demand,
reserved, Spot Instances, or dedicated hosts.
And you basically have to make a decision based
on that scenario.
So definitely don't use Spot Instances for anything critical
or where you need persistent workloads or databases.
So moving on to Spot Instances and more importantly
how to determinate Spot Instances.
So when we create our Spot Instances,
we're creating a spot request.
And essentially what we're doing
is we're defining the maximum price.
So it might be 10 cents an hour.
We might have 100 instances in this spot request.
We then define our launch specification.
So what AMI is it?
So it might be a Linux, Amazon Linux 2 AMI.
And then we've got our request type.
And our request type can either be one time
or it can be persistent.
And if it's persistent
you have to say when the request is valid from
and when it's valid to.
Now, with the request type
as one time, it's really, really simple.
Once we create our spot requests
our instances will be provisioned,
they'll be launched as soon as the spot price
goes above our maximum price, then that's it.
The spot request has been completed.
It's a one-time spot request
and these instances will all be stopped
or terminated depending on what you configure.
So we don't need to worry about that.
If, however, we have a persistent request
what essentially it's doing is it's looking to see
if the request is open, active,
or disabled as opposed to failed, canceled, or closed.
And you can go read the guide there's
a big in-depth explanation on this
but essentially what you need to understand for your exam
is if you've got a spot request that's open and persistent
you can't just go in and terminate your instances,
because your spot request is going to look
at the spot price and it's going to say, hey
well you said that we needed 100 instances,
there's currently none in there
and the spot price is still lower than your maximum price.
It will then just keep re provisioning those instances.
So you get stuck in this loop.
What you have to first do is go in
and cancel the spot requests.
So you have to go in and cancel the spot request
that, in itself, won't terminate your instances.
You still need to go in and terminate those instances.
So this is very popular scenario-based question
essentially what it's asking is
how do you go in and terminate Spot Instances
under a persistent spot request?
Well, it's pretty simple,
you just go in and you cancel the spot requests
and then you go in and you terminate your instances.
And I would go ahead and read that guide
if you've got the time.
Moving on to Spot Fleets,
what is a Spot Fleet?
Well, it's just a collection Spot Instances
as well as optionally on-demand instances.
And a Spot Fleet attempts to launch the number
of Spot Instances and on demand instances
to meet the target capacity that you specified
in the spot fleet request.
And the request for Spot Instances is fulfilled
if there's available capacity
and the maximum price you specified
in the request exceeds the current spot price.
So the spot price is below your maximum price.
The Spot Fleet also attempts to maintain
its target capacity fleet
if your Spot Instances are interrupted.
So Spot Fleets will try and match the target capacity
with your price restraints.
That's all you need to remember with Spot Fleets.
It's just a way
of meeting your capacity within your budget, essentially.
So you can set up different Launch Pools.
So you can define things like EC2 instance types,
operating systems and availability zones.
They can all be in different pools
and then you can have multiple pools.
And the fleet will choose the best way to implement
depending on the strategy that you define.
And we'll come to strategies in one second.
So Spot Fleets will stop launching instances
once you reach your price threshold or capacity desire.
So let's look at the different strategies.
So we've got capacity optimize
and this is where Spot Instances will come from the pool
with optimal capacity for the number of instances launching.
So it's where you are basically optimizing your capacity.
Then we've got diversified.
This is where the Spot Instances are distributed across
all of the different pools.
Got lowest price is probably one of the most popular.
And this is where your Spot Instances come from the pool
with the lowest price.
And this is the default strategy.
And then we have our instance pool to use count.
And this is where the Spot Instances are distributed
across the number of Spot Instance pools you specify.
And this parameter is only valid when used
in combination with lowest price.
So that is it for Spot Instances and Spot Fleets.
Onto my exam tips.
Just remember Spot Instances save up to 90%
of the cost of on demand instances.
It's useful for any type
of computing where you don't need persistent storage.
You can block Spot Instances
from terminating by using a Spot Block.
And then a Spot Fleet is basically just a collection
of Spot Instances and optionally on-demand instances.
And you can configure it in a variety of different ways
and using the 4 different strategies we just covered
and you do them in pools.
And you can have multiple pools in a spot fleet.
So that is it for this lecture everyone,
if you have any questions please let me know, if not,
it's time to move on to the very last lecture
in this section of the course.
So if you've got the time
please join me in the next lecture.
Thank you.


EC2 Exam Tips
===============

Okay, hello, Cloud Gurus. Congratulations.
You're at the end of the EC2 section of the course.
EC2 is 1 of the most important technologies
that you need to know going into your exam.
So let's review everything that we learned about EC2.
So what is EC2?
Well, it's like a virtual machine.
It's hosted in AWS instead of your own data center.
And you basically select the capacity
that you need right now,
and you can grow and shrink when you need to,
and you only pay for what you use.
And it takes only minutes
to provision an EC2 instance, not months.
So let's look at our
4 different EC2 instance pricing options.
And you will be given scenario-based questions
and there will be quite a few of them
and you have to select the best pricing.
So let's start with On-Demand.
This is where you pay by the second or the hour
depending on the type of instance that you run.
And it's great for flexibility.
No longterm contracts, et cetera, et cetera.
Spot.
This is where you purchase unused capacity
at a discount of up to 90%.
And the prices fluctuate with supply and demand.
And it's great for applications
with flexible start and end times.
We then have Reserved instances
and this is where you reserve your capacity
for 1 to 3 years.
And the more you pay up front
the greater savings that you have.
And you can save up to 72% discount on the hourly charge.
And this is great if you've got known or fixed requirements.
And then finally we've got Dedicated.
And this is a physical EC2 server
that's dedicated for your use.
And it's great if you have server-bound licenses to reuse
or compliance requirements.
So perhaps the government says
that you can't do it on multi-tenant hardware,
then you want dedicated instances.
So you are going to probably get at least
4 different scenario-based questions.
Each where you have to select the right pricing option
and you will have to select basically these 4.
So make sure you know what the different pricing options are
going into your exam and what their use cases are.
Moving on to the command line.
So the AWS command line interface.
So we learned that you should always
give your users Least Privilege.
So when we created our group,
we basically didn't give them administrative privileges,
we just gave them S3 administrative privilege.
So you always give your users the minimum amount
of access required to do their jobs.
And you should always use groups.
So you should create IAM groups
and assign your users to groups.
And group permissions are assigned
using IAM policy documents and your users will automatically
inherit the permissions of the group.
So as soon as you add a user into the group
that's where they're going to inherit the permissions
from that group.
Staying on the command line,
just remember these important AWS CLI tips.
So your secret access key, you'll only ever see this once.
If you lose it, you can delete the access key ID
and secret access key and regenerate them.
But you're going to need to run at the AWS configure again
and enter in your secret access key
as well as your access key ID.
Don't share your key pairs.
So each developer should have their own
access key ID and secret access key.
So just like usernames and passwords
you shouldn't be shared.
You should always have 1 key pair per human user.
And remember that the command line is supported
by Linux Windows and Mac OSX.
So you can install the CLI on your Mac, Linux, or Windows PC
and you can also use it on EC2 instances.
Moving on to roles.
So roles are the preferred option.
If you can use roles from a security perspective,
then you should.
Avoid hard coding your credentials with roles.
You don't need to save access key IDs and secret access keys
into your code in order to be able to access things
like S3 from EC2.
Instead, you can basically create a role
and that allows you to use other AWS services
without the use of access key IDs and secret access keys.
We then have policies.
So policies control a role's permission
just like policies control your user permissions
and group permissions.
So it's exactly the same thing.
And in terms of updates, you can update a policy
and attach it to a role and it will take effect immediately.
And attaching and detaching.
You can attach and detach a role to running EC2 instance
without having to stop or terminate these instances.
Moving on to security groups.
So, just remember that when you create a security group
and you make a change to the security group,
perhaps you add port 80 or port 443,
those changes will take effect immediately.
You can have any number of EC2 instances
within a security group
and you can have multiple security groups
attached to EC2 instances.
Remember that when you create a security group,
all inbound traffic is blocked by default
and all outbound traffic is allowed.
And moving on to bootstrap scripts.
So a bootstrap script is a script that runs
when the instance first runs, it runs at root level.
So it has full administrative privileges
and it passes user data to the EC2 instance
and can be used to install applications
like our web servers, database servers,
as well as do updates and whatever else you want it to do
when you first boot that EC2 instance.
Now you will get bunch of scenario questions
where they're trying to confuse you
with user data and metadata.
So if you remember,
user data is simply your bootstrap scripts
whereas metadata is data about your EC2 instances.
And you can use bootstrap scripts
or user data to access metadata.
And that's what we did in that lab.
We basically ran a bootstrap script
which got our public IPv4 address
and it made a little webpage telling us
what that IPv4 address is.
Moving on to networking with EC2.
So you're going to get different scenarios on the exam,
and you will be asked to choose
the correct networking device.
So you need to understand
the 3 different networking devices
and what their use cases are.
So ENI, for example. This is for basic networking.
Perhaps you need a separate management network
from your production network, or a separate logging network,
or maybe even something to monitor.
So you need it separate from your production network
and you need to do this at low cost.
So in this case you just are going to use
multiple elastic network interfaces for each network.
Enhanced networking.
This is for when you need speeds
between 10 gigabits per second and 100 gigabits per second.
Basically anywhere you need a reliable and high throughput.
And then EFAs.
This is where you need to have
high-performance computing and machine-learning applications
or you need to do an OS level bypass.
If you see a scenario question talking
about HPC or machine learning
and asking what network adapter you should use,
then you want to choose EFA, elastic fabric adapter.
Moving on to our different types of placement groups.
So a placement group has just a logical grouping
of EC2 instances.
You can have cluster placement groups.
This is where you need low network latency
and high network throughput.
So again, for high performance computing.
Spread placement groups.
This is where you have basically key,
critical EC2 instances,
and you don't want them to be on the same hardware,
so you'd need to spread them around.
And then we've got our partition placement groups.
And this is where you've got multiple EC2 instances.
Things like HDFS, HBase, and Cassandra.
So if you see any scenario-based questions talking
about those technologies,
you want to put them on partition placement groups.
Moving on, things to remember for the exam
about placement groups.
Our cluster placement groups can't span
multiple availability zones,
where spread placement group
and partition placement group can.
Only certain types of instances
can be launched into placement groups.
So this will be compute optimized, GPU optimized,
memory optimized, and storage optimized.
AWS always recommends homogenous instances
with the clustered placement groups.
And you can't merge placement groups
but you can move an existing instance
into a placement group
before you move the instance,
the instance must be in the stop state.
You can then move or remove an instance
using the AWS CLI or the SDK,
but you can't do it via the console just yet.
So moving on to dedicated hosts.
So any question that you see
that talks about special licensing requirements
or maybe even compliance requirements,
I want you to think of dedicated hosts.
An Amazon EC2 dedicated host is a physical server
with an EC2 instance capacity
that's fully dedicated for your use.
It's not shared hardware.
So dedicated hosts allow you
to use your existing per socket, per call,
or per VM software licenses, including Windows server,
Microsoft SQL server, and SUSE Linux Enterprise server.
Remember with Spot instances and Spot fleets,
that Spot instances can save you
up to 90% of the cost of on-demand instances.
You can block Spot instances from terminating
by using Spot block,
and it's useful for any type of computing
where you don't need persistent storage.
And then a Spot fleet is a collection
of Spot instances and optionally on-demand instances.
So that is it for this lecture, everyone.
If you have any questions, please let me know.
If not, feel free to move on to the next section.
Thank you.



Elastic Block Storage (EBS) and Elastic File System (EFS)
=========================================================

EBS Overview
============

Okay, hello Cloud Gurus,
and welcome to this section of the course.
In this section of the course,
we're going to be looking at Elastic Block Storage or EBS.
And in this lecture we're going to have a look
at an overview of EBS.
So we're going to understand our EBS volumes, what EBS is,
we'll look at where we would use EBS,
so for mission critical applications.
We're then going to explore the different volume types,
so we'll start with our solid state disks,
and then we'll move on to our hard disk drive,
and then we'll move on to our IOPS versus throughput,
what the differences are between them,
and then onto our exam tips.
So what is EBS?
Well, like I said,
EBS just stands for Elastic Block Storage.
And there are different types of EBS volumes available
which we will cover off in a few slides.
But basically, there's different use cases
for each type of EBS volumes.
And as you go into your exam,
you'll be given scenario-based questions,
and you will need to choose the best EBS volume type
for that scenario.
So, like I said, EBS just stands for Elastic Block Store,
and these are storage volumes
that you can attach your EC2 instances to.
So let's say we've got a T3 EC2 instance--
that's the compute side of it.
We obviously need to attach storage,
so we need to have a place
where the operating system is installed
or where the applications or database is installed.
So let's say we're using an Amazon Linux operating system.
So we would then install that on our EBS volume.
We could also go ahead and install our applications.
So maybe it's a WordPress app
or maybe we're just running Apache in our own custom apps.
And then we also can install our data or database
on our EBS volumes.
So these could be flat files or it could be a MySQL database
or SQL Server or any kind of a relational
or non-relational database running on EBS.
So EBS gives us the ability
to install all kinds of things on it.
It's basically just a virtual hard disk
that is attached to your virtual EC2 instance.
So you use them the same way that we use any system disk.
You can go in and create a file system,
run a database, run an operating system,
store data, or we can also install our applications.
So I want you to think of EBS
as just a elastic block storage disk
or a virtual disk in the cloud,
because that's all it is.
It's a virtual hard disk in the cloud
that's attached to our EC2 instance.
So that's all that EBS is.
And EBS is designed for our mission critical data,
so for things like our production workloads.
So essentially, it's built from the ground up
for mission critical workloads.
It's highly available and it's automatically replicated
within a single availability zone
to protect against hardware failures.
So like I said,
you can think of it as a virtual hard disk in the cloud,
but it's not on 1 physical hard disk.
Your EBS volumes will exist across multiple disks
in probably multiple different data centers
inside a single availability zone,
but they'll always be within the same availability zone
and we'll get to that a bit later on.
So that's all EBS is.
It's a virtual disk in the cloud
that's automatically replicated
within a single availability zone
to protect against hardware failures.
It's also very scalable,
so you can dynamically increase capacity
and change the volume type with no downtime
or performance impact to your live systems.
So this is really important to remember going into the exam.
Let's say you've got 100 GB in terms of storage
for your EC2 instance
and you've suddenly run out of storage space,
you can just increase that to 150 GB,
and you're not going to have any downtime
or performance impact.
You will have to go in and resize the file system,
but you can automatically add storage
and you don't have to stress about downtime.
So let's have a look at our different EBS volume types,
and we'll start with our solid state disks or SSD drives.
And essentially, right now,
there's always 2 different types.
So we'll start with general purpose, and it's gp2.
So gp2 just stands for general purpose.
It's a balance of price and performance.
You get 3 IOPS per GiB
and a maximum of up to 16000 IOPS per volume.
And gp2 volumes smaller than 1 TB
can burst up to 3000 IOPS,
and these are basically good for boot volumes
or development and test applications
that are not latency sensitive.
And we'll be using gp2 by default
throughout the rest of this course
and it will be our boot volume.
Now, there is a new generation of gp called gp3.
Now, before you worry,
going into your exam, you're never going to be asked
to choose gp2 over gp3, or gp3 over gp2, etc.
They'll basically just update the questions
and it'll either say gp2 as an option or gp3,
but you won't have to choose between them.
So just remember that general purpose
is basically there for your boot device volumes.
This is where you're going to install your operating system.
So gp3 has a predictable 3000 IOPS baseline performance
and 125 MiB per second,
and that's regardless of the volume size.
You don't need to create specific volume sizes
to achieve specific throughput.
And it's ideal for applications
that require high performance at a low cost.
So for things like MySQL, Cassandra,
virtual desktops, and Hadoop analytics.
And customers who are looking for higher performance
can scale up to 16000 IOPS and 1000 MiB per second
for an additional fee.
Now, again, going into your exam,
don't worry, you're never going to be asked
to memorize the IOPS or throughputs
for any of these EBS volume types.
All you need to know is their use cases.
So gp, whether it's gp2 or gp3,
you are just using that to install an operating system on.
It's basically the default volume that you would use
when you provision an EC2 instance.
And just remember that the top performance of gp3
is actually 4 times faster
than the max performance of gp2 volumes.
So I don't think it's likely you're going to be asked
to choose between gp3 and gp2 in the exam,
I've never seen a question like that,
but if you do have the choice,
always choose gp3 because it is faster.
If you need super fast performance with SSD,
then we move on to Provisioned IOPS.
And just like gp2 and gp3,
you've got io1 and io2.
So io1 is the legacy one.
This is high-performance option and is the most expensive.
It gives you up to 64000 IOPS per volume
and you get 50 IOPS per GiB.
You basically use this if you need more than 16000 IOPS.
So the only thing you really need to remember
going into your exam is if you've got a high-performance,
let's say it's a database or something,
and it needs more than 16,000 IOPS,
straightaway just think of io1 or io2.
It's designed for I/O-intensive applications,
so large databases, latency-sensitive workloads, etc.
And then we have Provisioned IOPS 2 or io2.
This is the latest generation
and it's got a higher durability and more IOPS than io1.
And io2 is basically the same price as io1.
You get 500 IOPS per GiB with up to 64000 IOPS.
It gives you 5 nine durabilities
instead of 3 nine durabilities,
and it's used for I/O-intensive applications
such as large databases and latency-sensitive workloads.
So basically, applications that need high levels
of durability.
So those are all our solid state drives.
Let's move on to our hard disk drives or HDDs.
And we're going to start with throughput optimized
hard disk drive.
And this is known as st1.
And basically, this is a low cost hard disk drive volume.
And by hard disk drive, I just mean it's magnetic.
It's not SSD, it's magnetic storage.
So that's the real old school hard drives.
And you have a baseline throughput
of 40 MB per second per TB.
And you have the ability to burst
up to 250 MB per second per TB.
And you have maximum throughput
of 500 MB per second per volume.
And this is basically designed
for frequently accessed and throughput-intensive workloads.
Now, I know you're wondering,
well, what's the difference between IOPS and throughput?
And we will cover that off,
but every time you hear things like big data,
data warehousing, ETL, which is extract, transform, and
load, and log processing,
I want you to think of throughput.
So I want you to think of throughput and not IOPS,
and that will help you very much in your exam
when you're looking at different scenario questions.
So Throughput Optimized Hard Disk Drives
are a cost-effective way to store mountains of data.
And it cannot be a boot volume.
So it cannot be a volume
that Linux has already pre-installed on,
or Windows, or any other operating system with EC2.
So you have to do that.
You can't do that on throughput optimized.
This is only used for things like big data, data warehouses,
ETL, and log processing.
Next up, we have our Cold Hard Disk Drives,
and this is known as sc1
and this is your lowest cost option.
So if you hear a scenario-based question
where they're talking about what EBS volume to use
to maximize cost reduction,
just think of Cold Hard Disk Drives or sc1.
And this is a baseline throughput
of 12 MB per second per terabyte.
You have the ability to burst
up to 80 MB per second per terabyte,
but your max throughput
is 250 MB per second per volume.
And this is a good choice for colder data
requiring fewer scans per day.
So you might want to use this as a file server, for example.
So it's good for applications that need the lowest cost
and performance is not a factor.
So maybe this is where you store some of your static images.
For some reason, you don't want to do it on S3,
you want to do it on an EBS volume,
and you need to have it on the lowest cost possible,
then you want your Cold Hard Disk Drive.
And like the other option,
so our Throughput Optimized Hard Disk Drive,
this cannot be a boot volume.
So we've had our hard disk drive and we've had our SSD
and we've talked about IOPS versus throughput,
so what is the difference?
Where do you use IOPS and where do you use throughput?
So what is IOPS?
Well, it measures the number of reads and write operations
per second.
And this is an important metric for quick transaction,
low-latency apps, and transactional workloads.
So I want you to think of a busy online store.
Customer goes through your checkout
and makes the transaction,
they've got everything in their cart,
they hit purchase, and you need to save this to a database.
It's a very transactional thing,
then you want as high as IOPS as possible,
so you can conduct this transaction more quickly,
and of course, you can have thousands of transactions
or even maybe millions of transactions
going on simultaneously.
So IOPS is all about the ability
to action reads and writes very, very quickly.
And if you have a transactional database,
then you want to choose Provisioned IOPS SSD, so io1 or io2.
With throughput optimized,
this measures the number of bits read or written per second,
so MB per second.
And it's an important metric for large datasets,
large I/O sizes and complex queries.
So this is where you need the ability
to deal with large datasets.
You're going to choose Throughput Optimized
Hard Disk Drives.
So essentially, what you're looking for
in your scenario-based questions
is if they're talking about big data,
data warehouses, ETLs, etc.,
then you're going to be putting it on throughput optimized,
whereas if you're talking about transactions,
you're going to either use general purpose or IOPS,
depending on your cost savings,
depending on what your appetite is for cost.
If you want to keep your cost down,
then it's general purpose,
if you want to maximize your performance,
then you're going to use io1 or io2.
And then moving back onto cold hard disk drives,
if you just want to have the lowest cost possible
but still have an EBS volume,
then you want cold hard disk drive.
So it's pretty simple.
Just going into your exam, look for the scenario question.
If it's talking about big data,
you want throughput optimized.
If it's a transactional database,
you're going to need to move to SSD.
And then you just need to decide
if you're using a general purpose 2
or if you're using Provisioned IOPS--
sometimes it's called PIOPS in your exam--
and then you just choose io1 or io2,
depending on the scenario-based questions.
So onto my exam tips. Let's just review
all the different EBS volumes that we have learnt.
You will get scenario-based questions going into your exam
and you'll be asked to choose which EBS volume type
is going to be the best choice for that particular scenario.
It's probably going to be worth 4 or 5 marks,
so I would definitely learn your EBS volume types by heart.
But don't worry, you don't need to remember specific IOPS
or throughput or anything like that,
apart from maybe if you need more than 16000 IOPS,
then you want to go up to Provisioned IOPS,
so io1 or io2.
So general purpose 2,
this is basically really suitable for our boot disks
and general applications.
You get up to 16000 IOPS per volume.
You get 3 nines durability.
We then have gp3.
So again, this is general purpose SSD.
It's the next generation.
It's suitable for high performance applications.
It gives you a predictable 3000 IOPS baseline performance
and 125 MiB per second, regardless of your volume size,
and you get 3 nines durability with it.
Now, like I keep saying,
you're never going to have to choose gp3 over gp2.
It doesn't work like that.
You just have to remember
that if you want to install an operating system,
then you probably just want to use general purpose,
and if they've updated the exam questions
then it will say gp3 on the exam,
if they haven't, then it will be gp2.
Provisioned IOPS is where you need very fast performance.
So it's suitable for online transaction processing
and latency sensitive applications.
We'll cover what OLTP is
in the databases section of the course,
but essentially, Provisioned IOPS is basically required
when you have very high performance databases.
So it could be like an online shop or something.
You want to install that on a EBS volume
that gives you the fastest IOPS.
You get 50 IOPS per GiB
and you get up to 64000 IOPS per volume.
And it's the most high performance
but also the most expensive.
And you get up to 3 nines durability.
And we have the latest generation which is io2.
Again, this is suitable for online processing
and latency-sensitive applications.
You get 500 IOPS per GiB with io2
and up to 64000 IOPS per volume.
And it gives you 5 nines durability.
And it's basically the latest generation
Provisioned IOPS volume.
So like I said, you're never going to see an exam question
where you have to choose gp2 over gp3, or io1 over io2,
or vice versa.
It doesn't work like that.
And you won't have to go in there memorizing
specific performance metrics.
The only thing I would just remember
is if you need more than 16000 IOPS per volume,
you're going to go from general purpose to I/O intensive.
So io1 or io2.
So those are our SSD volumes in a nutshell.
Let's move on to hard disk drive volumes,
and these are magnetic storage.
So we've got st1,
which is Throughput Optimized Hard Disk Drives.
This is suitable for big data, data warehouses,
ETL, which is extract, transform, and load,
and you have a max throughput
of 500 MB per second per volume.
And st1 cannot be a boot volume.
You get 3 nines durability.
And then we have our Cold Hard Disk Drive, and this is sc1.
And this gives you the max throughput
of 250 megabytes per volume.
And basically, this is used
for less frequently accessed data.
So it could be file servers or something like that.
And again, it cannot be a boot volume either,
but it is the lowest cost.
So if they are asking you to choose
the lowest cost EBS volume,
then you always want to go with sc1 or Cold Hard Disk Drive.
And it gives you up to 99.9% durability.
So that is it for this lecture everyone.
If you have any questions, please let me know.
If not, feel free to move on to the next lecture.
Thank you.


Volumes and Snapshots
======================

Okay, hello Cloud Gurus,
and welcome to this lecture.
In this lecture we're going to look
at volumes and snapshots.
So we're going to start with, what are volumes?
and then, what are snapshots?
And we'll do 3 tips for snapshots.
And we'll look at what to know about EBS volumes,
and we'll go in and do a demo
and then we'll get onto my exam tips.
So what are volumes?
Well, volumes exist on EBS.
So think of it as a virtual hard disk.
Volumes are simply virtual hard disks.
And you need a minimum of 1 volume per EC2 instance.
And this is always called the root device volume.
This is where your operating system is installed.
So this is where Windows is installed,
or where Linux is installed, etc.
So that's all a volume is. So what are snapshots?
Well, the camera might give it away.
Snapshots exist on S3, so they do not exist on EBS.
They're actually stored on S3, but essentially,
what it is is it's a snapshot,
it's a photo, of the virtual desk or of the volume.
So you're literally taking a snapshot of the volume,
how it exists when you initiate that snap.
So snapshots are point in time.
When you take a snapshot
it's a point-in-time copy of a volume.
So it's just like taking a photo.
It's a point-in-time copy of something that is occurring.
So snapshots are incremental,
this means that only the data that has been changed
since your last snapshot has been moved to S3.
So essentially this saves dramatically on space
and the time that it takes to take a snapshot.
So let's say we installed a Linux OS
and then we took a snap of it.
And then we created a text file that just said,
Hello Cloud Gurus, and it was called hello.txt.
When we take a second snap,
it's only the changes that have occurred from our first snap
that will be replicated over to S3.
So essentially it's just going to be a small text file.
Now it's actually--it doesn't quite work like that,
it's not like it's moving the file over,
but it's basically taking a photograph
of the volume how it exists.
And then essentially it just moves
over the delta, so the change in those volumes.
So that's all it is.
Snapshots are incremental, and it saves you a lot of money
on space and time because you're not moving
the entire data set over.
It's only the changes that are being moved over.
That being said, though,
the first snapshot may take some time to create,
as there's no previous point-in-time copy.
So when you take your first snapshot,
it's going to take longer
than when you take your subsequent snapshots.
So what are my 3 tips for snapshots?
So if you want consistent snapshots,
snapshots only capture the data that has been written
to your EBS volume.
And this might exclude any data
that's been locally cached in your application
or operating system.
So for a consistent snapshot, it's recommended
that you stop the instance and take a snap.
Basically, if you're doing it on a running operating system,
and your applications running, and it hasn't saved it to
disk, and you go and take a snap because it's not saved to
disk, it might still be in RAM, or it might still be cached,
or whatever, then it's not going to be on that snapshot.
So if you want a completely consistent snapshot,
the best bet is to stop an instance and then take a snap.
In terms of encrypted snapshots,
if you take a snapshot of an encrypted EBS volume,
then of course the snapshot will be encrypted automatically.
And in terms of sharing snapshots, you can share snapshots,
but only in the region in which they were created. To share
to other regions, you first need to copy them
to the destination region first,
and we'll have a look at that in the demo,
but that's crucial point going into your exam.
You do not know how many scenario-based questions where
they'll test you on how to move EC2 instances
from one region to another. Well you can do it,
you basically just take a snapshot,
then you create a copy of that snap, and you copy it
to another region, and then you can provision
that EC2 instance in another region.
So what should you know about EBS volumes?
Well, you should always know the location.
So EBS volumes will always be in the same availability zone
as your EC2 instance. It's crucial to know this.
And that kind of makes sense
because you do not want the latency.
If you imagine an EBS volume being a virtual hard disk,
you don't want your hard disk in one availability zone
and the actual computer that it's supposed to be connected
to in another availability zone, 20 or 30 miles apart,
like that kind of latency is just not going to work.
So your EBS volumes will always be
in the same availability zone as your EC2 instances.
And it's really important to remember
that going into your exam.
Also remember that you can resize on the fly,
so you can resize your EBS volumes on the fly.
You do not need to stop or restart the instance.
However, you will need to extend the file system
in the operating system so the operating system
can see the resize volume,
and you can switch and change volume types on the fly.
So you can go from gp2 to io2,
and you don't need to stop or restart the instance.
And we'll have a look at how to do that right now.
So let's log into the AWS console.
Okay, so here I am in the AWS console.
I want to go over to EC2 which is under Compute.
And we're going to go ahead and launch an instance
and just check your region, so I'm in Northern Virginia,
and we're going to basically copy our EC2 instance
over to another region.
So I'm going to use the Amazon Linux 2AMI,
and I'm just going to use the standard t2.micro,
I'm going to go ahead and hit Next.
I'm going to leave everything as default
and hit Next to add my storage.
So this is where we add our EBS volumes.
Now you can see our root device volume is here.
The volume type is called root.
You can see that by default, it's 8 GiB in size.
And this snap is basically just the EC2 instance
that we chose.
So it's the standard Amazon Linux 2 AMI.
So it's already using existing snapshot that Amazon manage.
And then here we've got our volume types.
So gp2, this is our general purpose 2,
we've got gp3 that we can use.
We've got io1, io2,
and then we've got magnetic (standard).
So what we're going to do
is we're going to launch it using gp2,
and I'm going to go ahead and add a new volume,
and in here you can now see that I can change this
to cold hard disk drive and throughput optimized as well.
So I can use the other 2 magnetic volumes
that we talked about earlier on.
So it's up to you what you want to do.
I'm going to choose cold hard disk drive
and just for fun we'll promote this to provisioned IOPS.
So go from one extreme to the other.
You can see here, it's already talking about my throughput
and my IOPS is a nonapplicable.
So it's always throughput when we're dealing
with magnetic, IOPS, when we're dealing with SSD.
Like I said before, IOPS is all about transactions;
throughput is all about things like big data,
large data sets, etc.
Let's go ahead and add our tags
and I'm just going to give this a name
and I'm going to call it MyEBSTest, something like that.
And then we're going to go in
and configure our security groups.
We're going to use an existing security group, so
I'm going to use my WebDMZ security group.
I'm going to go ahead and hit Review and Launch
and I'm going to go in and Launch that instance,
and I'm going to use my Northern Virginia Key pair.
Now we weren't actually SSHed into this instance,
we're just going to go and play with it in the console.
So I'm just going to pause the video
and wait for this instance to come up online.
Okay, so that instance is now live.
So we can click in here, and we can go down
to our storage options and we can see our different storage
or EBS volumes down here.
So we've got our volume 8 size.
This is our root device volume and it's not encrypted,
you can see here
and we'll cover off encryption later.
And then if we actually click on this, we'll be able to see
what type of volume it is. So you'll be able to click in
here and you can see by going down
to our volume type, it's gp2.
So that's our volume in there.
Now, this is actually a filter.
So if we click up here and we remove this filter,
we'll be able to see all our EBS volumes.
So we can see our 2 different EBS volumes in here.
So we've got our volume type gp2.
This one here is our cold storage one.
And so what we can do is if we just uncheck this.
And so if we click in Actions up here,
we can go ahead and modify this volume.
So this is sc1, so this is our cold hard disk drive.
And what we can do is change this,
and we can change it all the way up to provision IOPS
and we can keep the size.
You can see here, the minimum is 4 GiB
and we've got 125 GiB.
So we can keep that size and this is our IOPS in here.
We can go ahead and hit Modify
and it'll ask us if we're sure we want to modify it,
and we'll go ahead and hit yes.
And now that has gone through.
I just hit the Refresh button up here.
You'll see that the volume type has now changed to io2.
I did modify that extremely fast,
but that is now up and is ready to go.
So we can now install very, very fast databases on it.
Let's go ahead and remove this volume.
So what we can do is we can go ahead and detach the volumes.
Right now it is attached to our EC2 instance.
I'll go ahead and Detach that volume.
Once that volume is detached,
we can then go in and terminate the volume--get rid of it.
So it's just basically a way
of attaching and detaching our EBS volumes to EC2.
So I'm just going to pause the video
and wait for this to finish detaching, okay.
That has now detached.
You can see the state is now available.
We can go in and go to Actions
and we can go ahead and Delete the Volume,
and that will no longer exist on our EC2 instance.
So the next thing we're going to do is we're going to look
at snapshots and we'll look at how we can move
an EC2 Instance from one region to another.
So we've got our root device volume in here.
It's on general purpose 2.
What we want to do is just go into Actions
and we want to Create a Snapshot.
And it's that simple, and we'll call it myfirstsnap.
And because it is our first snap,
it's going to take a little bit longer
than any other subsequent snap,
and that's because snapshots are incremental
and they're stored on S3.
So I've just created my first snap.
If we go in and have a look at our snapshots in here,
they're under Elastic Block Storage. Going to click on our
Snapshots, and we can see that that is pending.
So I'm going to pause the video
and wait for this to come back up online.
Okay, so that snap is now up online.
And if I wanted to deploy this EC2 instance
into another region, what I need to do is I need to go
over to Actions and I need to go ahead and Copy this snap.
And you can see here,
I can copy the snapshot from one region to another.
So I'm in US East (Northern Virginia).
Let's copy this across to London
and I can even click this option to encrypt this snapshot,
and that will make sure that when it's copied
to my London region, it'll be encrypted,
but we'll cover that off later on in the course,
right now we're just going to copy this snapshot
from one region to another.
So I'm going to go ahead and hit Copy,
that has now started to copy.
And if I go over to my Northern Virginia region,
I go back over to London.
So eu-west-2, I'll be able to see
that my snapshot in here is pending.
So I'm going to pause the video
and wait for this to come back up online.
Okay, so that has now been copied to my region in London.
And what it can do is I can go into Actions,
and I can go ahead and create an image out of this snap.
And essentially what that will do
is create an Amazon machine image.
And that's what we use to create EC2 instances.
So that's how I can migrate an EC2 instance
from one region to another.
Basically all we do is we create a snapshot.
We copy that snapshot from one region to another,
and then we create an Amazon machine image
out of that snapshot
and then we provision our EC2 instance using that.
And that is a very popular topic on the exam.
So now what we've done this, let's go on to my exam tips.
So the 5 things you need to remember
for EBS volumes and snapshots going into your exam
is remember that volumes exist on EBS,
whereas snapshots exist on S3.
Snapshots are point-in-time photographs of volumes,
and they are incremental in nature.
The first snapshot will take some time to create
and for consistent snapshots,
you need to stop the instance and then detach the volume.
You can share snapshots between AWS accounts,
as well as between regions.
But first you need to copy that snapshot
to the target region, which is what we just did.
And then finally, just remember that you can resize
your EBS volumes on the fly,
as well as changing the volume type
and it won't cause you any downtime.
So that is it for this lecture everyone.
If you have any questions, please let me know.
If not, feel free to move on to the next lecture.
Thank you.


Protecting EBS volumes with Encryption
======================================

Okay, hello Cloud Gurus,
and welcome to this lecture.
In this lecture we're going to look
at protecting our EBS volumes with encryption.
So first thing we'll do
is we'll explore what EBS encryption is,
we'll look at what happens when you encrypt,
we'll look at some basic facts about EBS encryption,
and we'll look at how to encrypt
existing volumes that are unencrypted,
then I'll actually show you how to do this in the console.
And that is a very, very popular exam topic,
so that's definitely something to remember
going into your exam,
and then we'll just cover off my exam tips.
So EBS encryption, basically you can encrypt
your EBS volumes and you use a data key
that's using the industry-standard AES-256 algorithm.
So Amazon EBS encryption uses AWS KMS,
as well as customer master keys,
when encrypting encrypted volumes and snapshots.
And like I said, we will explore KMS in a bit more detail
in the security section of the course,
but just for the purposes of this lecture,
all you need to understand
is you can encrypt your EBS volumes,
and you can either manage the key yourself,
or you can have Amazon manage it for you.
So what happens when you encrypt an EBS volume?
Well, the data at rest is encrypted inside the volume,
and all data in flight moving between the instance
and the volume is encrypted.
And all snapshots are going to be encrypted
and all volumes created from your snapshot
are going to be encrypted as well.
So the encryption is basically end to end.
So in terms of EBS encryption, it's handled transparently,
encryption and decryption are handled transparently,
basically you don't need to do anything.
In terms of latency,
encryption has a minimal impact on latency,
so you won't get any performance degradation.
Copying an unencrypted snapshot allows encryption,
and this is a very popular exam topic,
and that's what we're going to cover off in the console.
Snapshots of encrypted volumes
are automatically encrypted as well.
So root device volumes, which is basically
the volume in which your operating system
is installed on by default,
you can now encrypt the root device volumes upon creation,
so that is definitely possible.
If you don't, however, we will go through
and we'll show you exactly how to encrypt
those root device volumes.
So 4 steps to encrypt an unencrypted volume.
Basically you create a snapshot
of the unencrypted root device volume.
You then create a copy of the snapshot
and you select the encrypt option.
And then we create an AMI from the encrypted snapshot,
and then we use that AMI to launch a new encrypted instance.
So let's go ahead and have a look at how this works
in the AWS console.
So here I am in the AWS console,
I'm going to go over to EC2, which is under Compute,
and I'm going to go in and launch an instance.
And what we're going to do is going to use an Amazon Linux 2
AMI. I can go ahead and hit Select.
And in here we're just going to do a t2.micro,
and we're going to leave everything as default
and add our storage.
Now this is our root device volume here,
and you can see that we've got the option to encrypt here.
So do we want to encrypt this?
And so we can go through and we can use either a KMS
or we can use Amazon's default key.
And so that's how we can encrypt a root device volume,
as soon as we provision an instance.
Now we're not going to do that because basically
you will get scenario-based questions
where you haven't done that in the past,
that you need to then go through
and make sure all your EC2 instances are encrypted,
so how do you do that?
Well, let's go ahead and provision this instance.
We'll just call this Name Unencrypted--
something like that. Keep it simple,
and we'll go ahead and hit Next,
we'll add it to existing WebDMZ security group,
we'll Review and Launch,
and then we'll go ahead and Launch our instance.
I'm just going to use my default key pair.
So that is now launching.
I'm just going to go in and View the instance,
and I'm just going to pause the video
and wait for this instance state to come up online.
So my instance is now up online,
if I click in here, I'll be able to see all the details
about my instance, and if we go across to Storage,
we'll be able to see our volume,
and we can click in there to view the volume
inside the Volumes section under EBS.
We can see over here under Encryption,
it says not encrypted.
So what we want to do is we want to make
this EC2 instance encrypted.
So the first thing we're going to do is we're going to go in
and we're going to create a snapshot of this volume.
So let's go ahead and Create a Snapshot.
And we're going to call this a snapshot MySnap,
something like that--just keep it really, really simple.
Now you see that it says encrypted: not encrypted,
and there's nothing I can do here,
so let's just go ahead and Create the Snap.
And because this is our first snap,
it's going to take some time to take effect.
Just go in here--you can see a snap from another lesson.
I might just go through and delete that quickly,
so we don't get confused.
And we've got my snap in here.
So I'm just going to pause the video
and wait for this to come up online.
So that snapshot has now been completed,
and we can go to Actions, and what we can do
is we can go ahead and copy this snapshot.
And we're going to copy it,
keep it in the same region,
but we're going to encrypt this snapshot,
and we're going to use the default key,
which is Amazon's EBS key.
And so we're going to go ahead and hit Copy,
that will then create a copy of the snapshot,
and the copy of the snapshot will be encrypted.
And you can see here under encryption, it says encrypted.
So again, I'm just going to pause the video
and wait for this encrypted snap to come live.
So that has now completed,
we can see that it's definitely encrypted,
it says so right there, and we can even see the key
that it was used and it's a standard AWS encryption key.
So what we're going to do, is we're going to click in here,
we're going to go over to our actions,
and now what we're going to do
is create an image from this snapshot.
And we'll just call it MyEncryptedEC2 instance--
something like that.
And we're going to leave everything else as default.
And you can see here now
that we've got the encrypted checkbox,
so we can have this root device volume encrypted.
So let's go ahead
and create this image from the EBS snapshot,
so that's what we've just gone and done now.
You'll find your images here, under Images in AMIs,
so if you click in there, you'll be able to see,
and there it is. It's already available,
so it was done very, very quickly.
Now based off this image,
we can then go ahead and launch our EC2 instance.
So we can go in, launch our T2 micro,
go ahead and hit Next, Next, keep our storage--
you can see it's already encrypted,
go ahead and hit Next, we'll add our tag,
and we'll call this in terms of name, MyEncryptedEC2.
And I'm going to go in,
add it to my existing security group, WebDMZ,
Review and Launch, and Launch,
and then I'm just going to acknowledge
that I've got my key pair.
And so that is now launching.
And if we actually go and view our instances--
there we go--we can see we've got our unencrypted instance,
and now we've got our encrypted instance.
So once this comes up online,
we can go through and delete this instance,
and we've basically just taken
an exact copy of this instance, but we've ensured
that the underlying EBS volume is encrypted.
And this will come up an awful lot in your exam
in terms of scenario-based questions.
You might get a scenario question
where a new chief information security officer is taken over
and you've got an existing fleet of EC2 web servers,
and those web servers are currently unencrypted,
and you have a new company policy
saying you need to make sure
all of the underlying root device volumes are encrypted.
How do you go about doing a migration?
Well, it's really, really simple,
all you do is you go in and you take a snap,
you then create a copy of that snap,
and when you're copying it, you encrypt that snap.
And then once you've got that encrypted snap,
you create an Amazon Machine Image from it,
and then you launch an EC2 instance
from that Amazon Machine Image.
So onto my exam tips,
and with encrypted volumes, just remember
that data at rest is encrypted inside the volume.
All data in flight moving between the instance
and the volume is encrypted.
All snapshots are encrypted,
and all volumes created from the snapshot are encrypted.
So this is with encrypted volumes only.
In terms of how to encrypt volumes,
so it's what we just covered off,
but essentially what you need to do
is you need to create a snapshot
of the unencrypted root device volume,
you then create a copy of the snap
and select the encrypt option,
you then create an AMI from the encrypted snap,
and then you use that AMI
to launch the new encrypted instances.
So that is it for this lecture, everyone.
If you have any questions, please let me know.
If not, feel free to move on to the next lecture,
thank you.


EC2 Hibernation
=================

Okay, hello Cloud Gurus,
and welcome to this lecture.
In this lecture, we're going to look at EC2 hibernation.
So, we're going to look at our EBS behaviors reviewed.
Then we're going to explore what EC2 hibernation is.
Then we'll look at EC2 hibernation in action,
and then we'll move on to my exam tips.
So, EBS behaviors reviewed.
We've learnt so far
that we can stop and terminate EC2 instances.
And if we stop the instance,
the data is kept on disk with EBS and will remain
on the disk until the EC2 instance is started.
If the instance is terminated, then by default
the root device volume will also be terminated,
but we can save that EBS volume if we wanted.
So, when we start our EC2 instance, the following happens:
The operating system boots up,
and then the user data script is run.
(And this is your bootstrap script,
which we covered off in the last section of the course.)
And then your application starts.
So, this might be things like Apache,
might be you know, MySQL, etc. etc.
So, your applications, when they're starting up,
they can take some time, especially, if you're dealing
with very complicated, big data applications, for example.
So the EC2 hibernation--basically what happens is
when you hibernate an EC2 instance,
the operating system is told to perform hibernation,
and this, basically, suspends to disk.
Now, hibernation saves the contents
from the instance memory.
So, from its RAM to your EBS volume.
And we persist the instance's EBS root volume
and any attached EBS data volumes.
So, essentially, what you're doing is you're taking
what's in the RAM and you're saving it down to disk.
So, when you start your instance out of hibernation,
the EBS root volume is restored to its previous state.
And then the RAM contents are reloaded.
And the processes that were previously running
on the instance are also resumed.
And previously attached data volumes are then reattached
and the instance retains its instance ID.
So, here's a good little diagram.
We've got our EC2 instance that's running.
It's got EBS root device volumes.
We start hibernation.
And then what happens is the RAM is then stopped,
and it's saved to disk,
and the instance is shut down and completely stopped.
And then what happens is when we start that instance,
the EBS root volume is restored to its previous state,
the RAM contents are reloaded,
and the processes that were previously were running
on the instance are resumed.
So, what does this mean in practice?
Well, with EC2 hibernation, the instance boots a lot faster,
because the operating system does not need to reboot,
because the in-memory state, or the RAM, is preserved.
And this is useful for things like long-running processes
or services that take time to initialize.
So, going into your exam, you, basically, just need to know
what EC2 hibernation is.
So, EC2 hibernation preserves the in-memory RAM
on persistent storage, so, on EBS.
And it's much faster to boot up.
You don't need to reload the operating system.
And the RAM must be less than 150 GB in size
for EC2 hibernation to work.
And it includes instance families,
such as C3, C4, C5,
M3, M4, M5,
R3, R4, and R5.
So, it's just the C, the Ms and the R instance families.
And it's available for Windows,
Amazon Linux 2 AMI, and Ubuntu.
And instances can't be hibernated for more than 60 days.
You can only hibernate your instance for 60 days or less.
And it's available for on-demand instances
and reserved instances as well.
So, that's it for this lecture everyone.
If you have any questions, please let me know.
If not, feel free to move on to the next lecture.
Thank you.


EFS Overview
============

Okay. Hello, Cloud Gurus,
and welcome to this lecture.
In this lecture, we're going to look at elastic file system
or EFS as an overview.
So we're going to start with what EFS is,
what the different use cases are,
we'll do an overview of it,
and then we'll look at different performance elements,
and how we can control our performance with EFS.
We'll then look at our different storage tiers,
we'll do a demo as how to set this up in the AWS console,
and then we'll move on to our exam tips.
So what is EFS?
Well, EFS stands for Amazon elastic file system,
and basically, it's a managed NFS,
or network file system,
that can be mounted on many different EC2 instances at once.
So EFS works with EC2 instances
that are in multiple availability zones.
So you can set up EFS and have centralized storage
and attach it to multiple EC2 instances.
So it's basically just shared storage.
And it's highly available, it's scalable,
however, it is expensive.
So this is essentially what it would look like.
We've got our AWS Cloud and then we've got our VPC
and inside it, we've got our EC2 instances,
and they are basically have a mount target towards EFS,
and it's a way for them to share data.
So you can have even up to,
let's say a thousand EC2 instances,
all connecting to the same file system.
So this is a great way of having web server farms.
It's a great way for things like content management systems,
or shared database access, etc.
So it's a shared storage across multiple EC2 instances.
So in terms of its use cases,
we can use it for things like content management.
So it's a great fit for content management systems.
This could be things like WordPress blogs, for example,
or Joomla websites,
because you can easily share content
between your EC2 instances.
It can also be used as a web server farm.
Like I said, it's great fit for web servers.
You basically just have a single folder structure
for your website.
In terms of EFS, it uses the NFSv4 protocol,
it's compatible with Linux-based AMIs,
Windows is not supported at this time.
(And we'll cover off shared storage for Windows
in a later lecture.)
But right now this is Linux only for EFS.
It uses encryption at rest using key management service.
And we do cover that off later on
in the security part of the course.
And basically, it's a file system that scales automatically,
and you don't have to worry about capacity plannings.
You don't have to say,
"Hey, I'm going to need 100 GB."
It basically scales automatically as you add files to it.
So it's great way of having an elastic file system.
And it's pay per use.
So the more you use, the more you pay,
and like I said, it is expensive.
EFS has amazing performance capabilities,
however, you can have thousands of concurrent connections.
So EFS can literally support thousands
of concurrent connections or thousands of EC2 instances.
You can get 10 Gbps throughput,
so it's really super fast in terms of throughput,
and it scales to the petabyte.
So you can scale your storage up into the petabytes.
So it has really, really good performance.
And now when you create your EFS file system,
you can actually set
what performance characteristics you want,
so you can have general purpose.
This is great for things like web servers,
content management systems, etc.
Then you can have things like max I/O,
and this can be used for big data
or media processing, etc.
Moving on to storage tiers,
EFS is very similar to S3
in that it comes with 2 different storage tiers,
and you can even use lifecycle management,
which allows you to move your data
from one tier to another
after a set number of days that you specify.
So we've got our standard storage tier.
This is for our frequently accessed files.
But then we also have our
infrequently accessed storage tier.
And this is for files that are not frequently accessed.
So you can move your files
after, let's say, 30 days from standard infrequently
accessed, and you'll save some money.
So let's go ahead to the AWS console
and have a look at how we can set up EFS.
Okay. So here I am in the AWS console,
and as you might guess, you will find EFS under Storage.
So let's go ahead and click in there.
And what we're going to do is we're going to go ahead
and just click in here,
we're going to create our first file system.
So in here we give a name to how file system.
I might just call it MyTestEFS.
EFS is regional, so you've got to select the VPC.
So I'm currently in Northern Virginia,
so I'm just going to select my default VPC.
And I want this to be a regional EFS cluster,
so it's going to store data redundantly
across multiple availability zones.
Now you can do it redundantly
across a single availability zone,
but honestly, like what happens
if that availability zone goes down?
You probably want it regional,
especially if it's a production system.
So I'm going to go ahead and hit Create.
And boom, there it is.
It's created in less than half a second.
And to be honest, EFS has come a very long way.
I remember when it first came out,
to provision a new EFS file system
would take up to 10 minutes.
Right now you basically hit Create, and boom, there it is.
So here is our EFS file system.
If we need to go in and edit it, just click on the name,
you'll be able to see things such as the size,
our monitoring, tags,
file system policies, access points, network, etc.
It's beyond the scope of the solutions architect
associate exam to cover all this off.
Essentially, you just need to know what EFS is
and where you would use it.
You can go up here and hit Edit,
and basically, you can see that you've got automatic backups
are enabled automatically, or by default.
We also have lifecycle management
that's enabled by default.
So 30 days is the default metrics.
So it'll move it from frequently accessed
to infrequently accessed,
if you haven't touched that object in 30 days.
And in here we've got our throughput mode.
So this is where we can say
bursting or we can say provisioned.
And if we click in Provisioned,
we can actually set what provision throughput that we want
for our EFS file system.
So let's go onto my exam tips.
So going into your exam, like I just said,
what you really need to know about EFS
is where you would use it.
So EFS supports the network file system version 4,
NFSv4 protocol.
You only pay for the storage that you use,
so you don't need to worry about pre-provisioning
like we just saw then.
You click it, and it's there almost instantly.
It can scale up to petabytes,
it can support thousands
of concurrent NFS connections at once.
And data is stored across multiple availability zones
within a region.
And you get read-after-write consistency.
And if you have a scenario-based question
around highly scalable shared storage using NFS,
I want you to think immediately of EFS.
So that is it for this lecture everyone,
if you have any questions, please let me know.
If not, feel free to move on to the next lecture.
Thank you.


FSx Overview
============

Okay, hello Cloud Gurus,
and welcome to this lecture.
In this lecture, we're going to look at FSx Overview.
And so where this comes up in your exam
is there's different types of FSx services.
So there's FSx for Windows, FSx for Lustre.
And then we basically compare it with EFS.
So in this lecture, we're just going to look at
what FSx for Windows is.
We're going to compare FSx for Windows vs. EFS.
We're then going to look at FSx for Lustre.
And we're going to look at FSx for Lustre performance,
and then we'll go onto my exam tips.
So let's start with, What is FSx for Windows?
Well Amazon FSx for Windows File Server
provides a fully managed native
Microsoft Windows file system.
So you can easily move your Windows-based applications
that require file storage to AWS.
And basically, Amazon FSx is built on Windows server.
So if you see any scenario-based questions where
it's talking about SharePoint migrations
and you need something like shared storage,
I want you to immediately think of FSx for Windows.
So how is FSx for Windows different from EFS?
Well FSx for Windows is a managed Windows server
that runs Windows server message block.
So it runs SMB-based file services.
So it's designed for Windows
and Windows applications essentially.
And so it supports things like active directory users,
access control lists, groups, security policies,
along with the distributed file system,
namespaces and replication.
Whereas EFS is a managed NAS filer for EC2 instances
which is based on the network file system, or NFS version 4.
And this is one of the first network file protocols
that was native to Unix and Linux.
So if you get a scenario-based question where it's talking
about migrating SharePoint or active directory
on to some form of shared storage,
I want you to immediately think of FSx for Windows.
So moving on to Amazon FSx for Lustre.
This is a fully managed file system
that's optimized for compute-intensive workloads.
So this could be things like high performance computing
or machine learning, or media data processing workflows
or electronic design automation.
So essentially it's anything to do with AI
and machine learning.
So if you get any scenario-based questions
where it's talking about processing massive datasets,
so you have hundreds of gigabytes per second of throughput,
you need millions of IOPS and sub-millisecond latencies,
I want you to think of Amazon FSx for Lustre performance.
It gives you amazing performance capabilities.
And it's, like I said, used for AI and machine learning.
So in the exam
you'll be given different scenarios and asked to choose
whether you should use EFS, FSx for Windows,
or FSx for Lustre.
So you'd use EFS where you need distributed,
highly resilient storage for Linux instances
and Linux-based applications.
Amazon FSx for Windows is where you need centralized storage
for Windows-based applications, such as SharePoint,
or SQL server, or workspaces, or IIS web server,
or basically any other native Microsoft application.
Essentially anything that's Microsoft,
and it needs centralized storage, just straightaway,
think of Amazon FSx for Windows.
And then Amazon FSx for Lustre is where you need high-speed,
high-capacity distributed storage.
And this will be for applications
that do high performance computing,
financial modeling, etc, etc.
And remember that FSx for Lustre
can store data directly on S3.
So that is it for this lecture, everyone.
If you have any questions, please let me know.
If not, feel free to move on to the next lecture.
Thank you.


Amazon Machine Images : EBS vs. Instance Store
===============================================

Okay, hello Cloud Gurus, and welcome to this lecture.
In this lecture, we're going to look at
Amazon Machine Images or AMIs.
And we're going to look at the 2 different types
that you can have.
So we're going to look at EBS Volumes vs. Instance Store.
So we're going to first explore what an AMI is,
we're going to look at the 5 things
you can base your AMI on,
then we're going to compare EBS volumes to instance store,
then we'll look at instance store volumes,
EBS volumes, and we'll do a demo,
and then we will go on to our exam tips.
So what is an AMI?
Well, it's an Amazon Machine Image
and this provides the information
required to launch an instance.
And you must specify an AMI when you launch an instance.
And we basically have seen this in the console demos
leading up to this lecture.
So when we're selecting our Amazon Linux 2 AMI,
that's the AMI that we were choosing,
it's just an Amazon Machine Image.
So the 5 things that you can base your AMI on
are region, an operating system,
the architecture, so whether it's 32-bit or 64-bit,
the launch permissions, and most importantly,
the storage for your root device volumes.
That's where your operating system is installed.
And all AMIs are categorized as either
being backed by Amazon EBS or by instance store.
So with EBS, the root device for an instance launched
from the AMI is an EBS volume created
from an Amazon EBS snapshot,
so that's what we've been looking at already.
Whereas for instance store, the root device
for an instance launched from the AMI
is an instance store volume,
which is created from a template
that's stored in Amazon S3.
So that brings us onto instance store volumes.
So instance store volumes
are sometimes called ephemeral storage.
So instance store volumes cannot be stopped
if the underlying host fails,
so the underlying physical piece of hardware,
so it could be a physical server.
If that fails, then you're going to lose your data.
You can, however, reboot the instance
without losing your data.
So ephemeral storage is basically means
it's not going to be saved if the instance is stopped.
If your underlying host does fail,
you're going to lose your data.
That's all you need to remember going into your exam.
If you delete the instance,
you're going to lose the instance store volumes.
That instance store volume is only ever going to exist
while the hardware is functioning
and you cannot go ahead and stop the volume
and obviously if you delete that instance,
then you're going to lose that instance store volume.
Now EBS volumes--they can be stopped.
You do not lose the data on the instance
if it's stopped. You can also reboot an EBS volume
and not lose your data.
And by default, the root device volume
will be deleted on termination.
However, you can tell AWS to keep
the root device volumes with EBS volumes.
So if you see anything about ephemeral storage,
think instance store.
If you need consistent and permanent storage,
think EBS volumes.
So I'm back in the AWS Console.
I'm going to click on EC2 under Compute
and we're going to go ahead and Launch an Instance.
The first one we're going to launch
is one using instance stores.
So if I actually go over to my community AMIs,
and we go down to our Root Device type,
we can see that we have 2 options: EBS and instance store.
And I'm going to go ahead and select Instance Store,
and then we have our different architectural types
down here, we've got our different operating systems.
I'm just going to use an Amazon Linux instance store device,
and I'll probably just select this top one.
So let's go ahead and hit Select.
In here, we can choose our different instance types,
so you can see that range of families
or the type of EC2 instance is restricted,
so the smallest one I can get is a c1.medium.
That's fine, I'll go ahead and hit Next.
In here we add our storage,
and our storage is instance store,
our volume type in here. We can't actually change that.
So let's go ahead and add our tags,
I'm going to go ahead and name this
Name and then instance store, InstanceStore,
and then I'm going to go ahead and hit Next.
I'm going to select an existing security group, and
I'm going to use my WebDMZ security group,
and I going to go ahead and hit Launch,
and I'm going to use my existing key pair.
That is now launching my EC2 instance.
Next, let's go back over and view our instance,
so we still have our EBS test instance
from the last lecture.
We've got our InstanceStore in here.
Let's just go in and launch one more instance.
We're going to do just a normal EBS one in here and hit
Select. In our instance types, I'm going to use a t2.micro,
I'm going to go ahead and hit Next.
I'm going to leave everything as default in here,
but when we go and add our storage,
I'm going to uncheck this, Delete on termination.
So that means if I delete this EC2 instance,
my EBS volume underneath it is going to be saved.
It's not going to be terminated.
And I'm just going to add a name,
so I'm going to call this EBSKeepTermination,
keep on termination.
So there we go, that's the name of my EC2 instance.
I'm going to add this into my WebDMZ security group,
go ahead and hit Next, and I'm going to go ahead
and launch this instance and click in here,
and hit Launch Instances.
So that is now launching my instances.
So all my instances are up and running right now.
We're going to have a look at the 3 different types.
So we've got EBSKeepTermination, MyEBSTest,
which will automatically
delete our EBS volume on termination,
and then we have our InstanceStore instance.
Now, if we go over to our EBSTest,
we're going to go into our instance state
and you can see in here, we can terminate our instance.
So let's go in and Terminate that instance.
And that will then terminate the EC2 instance,
and because we have said that it should terminate
the EBS volume as well, that volume will now go ahead
and be deleted as soon as that instance is terminated.
So if I just hit Refresh a couple of times,
I just hit refresh, that has now been deleted.
If I go back over to my volumes in here,
if we just hit Refresh up here,
you can see that the volume has now disappeared.
So when we deleted that EC2 instance,
it also deleted the EC2 volume.
Lets go back over to our dashboards
and have a look at our running instances,
we can now see that that instance has been deleted.
So let's have a look at our instance store.
If I click on this and I go to our instance state
and I try and stop this instance,
it's going to try and stop our instance store,
which is on ephemeral storage.
So what do you think is going to happen?
Well, if you remember in the start of the lecture,
we said you cannot stop instance store.
It says here, failed to stop the instance,
it does not have an EBS root device type
and it cannot be stopped.
So if for some reason, the underlying hardware
was interrupted, and this EC2 instance ceased to exist,
that would be it. You'd lose all of your data.
So you cannot stop instance store instances.
So the next thing we're going to do
is we're going to click in here to our EBS
and Keep on Termination.
We're going to go in and go to our instance state
and we're going to terminate this instance
go ahead and hit Terminate.
And then also what I will do
is I'll go into my instance store
and I'm going to go ahead and terminate that instance as
well. Go ahead and hit Terminate,
and we can see that we've successfully
terminated both of them.
Now, if I go back over to my Volume Types,
it can take a little bit for the termination to take effect,
but essentially, this state will change
from in-use to the available state.
And you can see that in there.
So this EBS volume has still been kept,
even though my EC2 instance has been terminated.
So the key thing going into your exam
is to remember the difference between
EBS volumes and instance store volumes,
and that brings us onto the exam tips.
So onto my exam tips.
My EBS volumes vs. my instance store volumes:
instance store volumes
are sometimes called ephemeral storage,
and that is because the data is ephemeral.
So instance store volumes cannot be stopped.
If the underlying host fails,
you are going to lose your data.
Whereas EBS-backed instances can be stopped,
and you're not going to lose the data on this instance
if it's stopped.
You can reboot both EBS volumes and instance store volumes
and you will not lose your data.
However, by default, both root volumes
will be deleted on termination,
but with EBS volumes,
you can actually tell AWS to keep the root device volume,
which is exactly what we did just then.
And just remember, going into the exam
that an AMI is just a blueprint for an EC2 instance,
so it has things like the CPU architecture,
whether it's got instance store or EBS volumes,
or what operating system it is, etc. etc.
So that is it for this lecture, everyone.
If you have any questions, please let me know.
If not, feel free to move on to the next lecture.
Thank you.


AWS Backup
===========

Okay, hello, Cloud Gurus,
and welcome to this lecture.
This is a really quick lecture,
and we're going to learn about AWS Backup.
So we're going to look at what AWS Backup is.
We'll look at how we can use
AWS Backup with our Organizations.
We'll look at the benefits of AWS Backup,
and then we'll go onto my exam tips.
So what is AWS Backup?
Well, it's probably what you imagine.
It's a way of backing up the different AWS services.
So Backup allows you to consolidate your backups
across multiple AWS services,
so things like EC2, EBS, EFS, Amazon FSx for Lustre,
FSx for Windows File Server, as well as Storage Gateway.
And so it can include other AWS services,
such as database technologies, like RDS and DynamoDB.
You can also use AWS Backup with organizations.
We will cover
off what Organizations are later on in the course,
but basically it's a way of having multiple AWS accounts
and consolidating them into 1 organization.
So Backups can actually be used
with AWS Organizations to back
up multiple AWS accounts in your organization.
And it gives you centralized control
across all AWS services in multiple AWS accounts
across your entire AWS organization.
So you can back up all those different services that I said
across multiple accounts, provided those accounts are set
up using AWS Organizations.
And like I said, we will cover
off what that is later on in the course.
So in terms of the benefits of AWS Backup,
just remember that you get central management.
So you basically use a single, central backup console,
which allows you to centralize your backups
across multiple AWS services and multiple AWS accounts.
You also get automation,
so you can create automated backup schedules
and retention policies.
You can also create lifecycle policies
which basically allow you to expire unnecessary backups
after a period of time.
And then you can get improved compliance.
So your backup policies can be enforced
and while backups can be encrypted both
at rest and in transit as well.
And this allows you alignment
towards your regulatory compliance,
and it's also great for auditing.
So auditing is made easy due to consolidated view
of backups across the many different AWS services.
So you've got a single pane of glass.
You can go in there and say,
"Okay, I have backed up my RDS instances,
"my EFS instances, my EBS instances, etc., etc."
So then onto my exam tips.
So just remember that AWS Backup gives you consolidation.
So use it to back up AWS services, such as EBS, EFS
Amazon FSx for Lustre, Amazon FSx for Windows File Server
as well as Storage Gateway.
So if you see any exam questions where it's talking
about, "Hey, you need to back up all these services.
"How do you do it and maintain a centralized point of view?"
Then you want to use AWS Backup.
You can also use it in conjunction
with AWS Organizations to backup your different AWS services
across multiple AWS accounts.
And just remember the benefits
of AWS Backup going into your exam.
Basically, Backups gives you centralized control,
letting you automate your backups
and define lifecycle policies for your data.
You get better compliance
as you can enforce your backup policies
and ensure your backups are encrypted.
And then you can also audit them once complete.
So literally you just need to know what AWS Backup is going
into your exam.
So that is it for this lecture, everyone.
If you have any questions, please let me know.
If not, feel free to move on to the next lecture.
Thank you.


EBS Exam Tips
==============

Okay, hello, Cloud Gurus,
and welcome to this lecture.
Well, congratulations,
you're at the end of the EBS section of the course.
In this lecture,
we're just going to review everything
that we've learnt in this section of the course.
So let's start with EBS volumes themselves.
I'm going to start with SSD volumes.
They're highly available and scalable storage volumes
that you can attach to an EC2 instance.
So you had a look at gp2, this is general purpose SSD.
This is suitable for boot disks and general applications.
Gives you up to 16000 IOPS per volume,
and it gives you up to 3 nines durability.
Then we looked at the newer one, which is gp3.
This is suitable for high performance applications
and it gives you a predictable
3000 IOPS baseline performance
and 125 MiB per second, regardless of your volume size.
And again, this is 99.9% durability,
so 3 nines durability.
If you need a faster SSD volumes,
then we have io1,
this is provisioned IOPS SSD.
This is suitable for online transaction processing
and latency-sensitive applications.
You get up to 50 IOPS per GiB
and up to 64000 IOPS per volume.
It's high performance,
but it's also the most expensive
EBS volume that you can get.
And you get 3 nines durability.
We then have iO2,
which is the the new provisioned IOPS SSD.
Again, this is suitable for online transaction processing
and latency-sensitive applications.
You get 500 IOPS per GiB
and about 64000 IOPS or up to 64000 IOPS per volume.
And with io2, you actually get 5 nines durability.
So it has the best durability out of any EBS volumes.
And it's the latest generation provisioned IOPS volume.
Moving on to our magnetic storage.
So we had throughput optimized hard disk drive.
This is suitable for big data, data warehouses,
extract, transform, and load.
And the maximum throughput is 500 MB per second,
and it cannot be a boot volume
but you do get up to 3 nines durability.
We then have sc1.
This has a max throughput of 250 MB per second,
per volume.
It gives you less frequently accessed data.
It cannot be a boot volume,
but it is the lowest cost,
and it gives you 3 nines durability.
So going on to some tips for volumes and snapshots.
So just remember that volumes exist on EBS,
whereas snapshots exist on S3.
And snapshots are basically
like what you would do with a camera.
They're a point-in-time photograph of a volume
and they're incremental in nature.
And because of this,
the first snap that you will take
is going to take some time to create
and for consistent snapshots
you should really stop your instance and detach the volume.
That way everything is then saved to disk.
You can share your snaps between AWS accounts,
as well as between regions,
but first you need to copy that snapshot
to the target region.
And you can also resize EBS volumes on the fly,
as well as changing the volume type.
So you can go from gp2 to gp3, for example.
Moving on to AMIs.
And we're going to look at EBS versus instance store.
So instant store volumes
are sometimes called ephemeral storage.
And you can reboot both EBS instances
and instance store volumes
and you will not lose your data.
However, instance store volumes cannot be stopped.
And we saw that in our lecture.
We tried to go in and stop it
but we got that error message.
So if the underlying host fails,
you are going to lose your data with instance store volumes.
By default, both root volumes
will be deleted on termination.
However, with the EBS volumes,
you can actually tell AWS to keep the root device volume.
EBS backed instances can be stopped.
So you're not going to lose any data on your instance
if it is stopped.
And just as a bonus tip,
an AMI is just a blueprint for an EC2 instance.
So it will have things like the architecture type,
whether it's Windows or Linux,
and then the instance, like size, for example,
etc., etc.
So that's all an AMI is.
Moving on to encrypted volumes.
So data at rest is encrypted inside the volume.
All data in flight,
moving between the instance and the volumes
is going to be encrypted.
All your snapshots of encrypted volumes are encrypted
and all volumes created
from your snapshots are encrypted as well.
This is the really important thing to remember--
is how to encrypt existing root device volumes
that are unencrypted.
So you want to go ahead and create a snapshot
of the unencrypted root device volume.
You then create a copy of that snapshot
and you select the encrypt option.
You then create an AMI from the encrypted snapshot,
and then you use that AMI
to launch a new encrypted instance.
And that's what we did in our console demo.
Moving on to E2C hibernation.
This is where it preserves the in-memory RAM
on persistent storage.
So it basically saves the RAM to disk on EBS.
and it gives you much--
it's basically much faster to boot up,
because you don't need to reload the operating system.
Now, your instance RAM must be less than 150 GB
and it's only available across the C-class families,
the M-class families, and the R-Class families.
And it's available for Windows,
Amazon Linux 2 AMI, and Ubuntu.
And instances cannot be hibernated for more than 60 days.
And it's available for on-demand instances
and reserved instances.
Moving on to EFS.
So this supports the network file system
version 4 or NFSv4 protocol.
It can support thousands of concurrent NFS connections.
You only pay for the storage that you use.
You don't have to worry about pre-provisioning
and starting with, let's say a hundred gigs.
You literally just create the EFS volume,
like we saw in our demo, and it exists straight away.
And then as you add more and more files to it,
it will automatically adjust--it's elastic.
So the data is stored across multiple AZs within a region.
It can scale up to petabytes,
and it has read-after-write consistency.
Now, if you have a scenario-based question
around highly scalable shared storage using NFS,
I want you to always think of EFS.
In the exam, you are going to be given different scenarios
and asked to choose whether you should be using EFS,
FSx for Windows, or FSx for Lustre.
EFS is when you need highly distributed
and highly resistant storage for Linux instances
and Linux-based applications.
Amazon FSx for Windows is when you need centralized storage
for Windows-based applications.
So this could be things like SharePoint,
SQL server, Workspaces, IIS,
or any other native Microsoft application.
And then Amazon FSx for Lustre
is when you need high-speed,
high-capacity distributed storage.
And this will be for applications
that do high-performance computing, financial modeling.
And remember that FSx for Lustre
can store data directly on S3.
So just summing up
and comparing all our different storage options
because this is a big chunk of the exam--
definitely going to be 10 or 12 marks,
where you have to choose different storage options,
based on your different scenarios.
So S3 is great for serverless object storage.
So things like files, videos, etc.,
where you don't want to have to manage E2C instances.
Glacier--just think of every time you hear the word archive.
That's going to be a Glacier question.
So it's used for archiving objects.
EFS is network file system or NFS for Linux instances.
So it's a centralized storage solution
across multiple availability zones.
FSx for Lustre is file storage
for high performance computing Linux file systems.
EBS volumes is persistent storage for EC2 instances.
And your instance store is ephemeral storage
for EC2 instances.
FSx for Windows is file storage for Windows instances.
So centralized storage across multiple AZs.
And like I said, if you just get a SharePoint question,
I want you to think of FSx for Windows.
And then, finally, knowing the different use cases
for the storage will gain you valuable points in the exam.
And like I said,
it's probably going to be with about 12 marks
of all the different storage options.
You'll be given all these different scenario questions,
and then you have to pick which the right storage option is.
And then finally, we looked at AWS backups.
So going into your exam, just remember what AWS backup is.
It's used for consolidations
to back up your different AWS services.
So this could be things like EC2, EBS, EFS,
Amazon FSx for Lustre, Amazon FSx for Windows file server,
and AWS Storage Gateway.
And essentially,
it's going to give you a single pane of glass
to back up all these different services.
You can also use it in conjunction with AWS Organizations,
and this allows you to back up different AWS services
across multiple AWS accounts.
And just remember the benefits of AWS backup
going into your exam.
So it gives you centralized control,
letting you automate your backups
and define your life cycle policies for your data.
And you get better compliance,
as you can enforce your backup policies,
make sure they're encrypted,
and then audit them once they are complete.
So hopefully you've learned an awful lot
in this section of the course
You've done really, really well.
And if you have any questions, please let me know.
If not, feel free to move on to the next section.
Thank you.



Databases
===========

Relational Database Service (RDS) Overview
===========================================

Okay, Hello, Cloud Gurus.
And welcome to this section of the course
in this section we're going to look at
Relational Database Service
which is often just called RDS.
And in this lesson, what we're going to do
is we're going to learn what relational databases are.
We're going to look at the advantages of using
Amazon's RDS service.
We're going to compare online transaction processing
with online analytics processing
and what services you'd use for OLTP
versus what services you use for OLAP.
We're then going to explore what Multi-AZ is with RDS.
We're going to look at what happens
when you have an unplanned failure
or if you need to do maintenance on your RDS instances.
I'll then show you how to set up an RDS instance
in the console.
And then we'll go on to our exam tips.
So let's start with relational databases.
What is a relational database?
Well, they've been around for many years
and been around basically since the seventies.
And they're at the heart of many, many applications.
Whether it's a game that you're playing
on your mobile phone,
or whether it's a shop that you're interacting with,
a lot of the data will be stored in a relational database.
And they're stored in things called tables.
So data is organized into tables within a database
and just think of a table as a traditional spreadsheet.
Think of it like Excel
where you've got workbook 1, workbook 2,
and they're all contained within an Excel file.
So most relational databases will have multiple tables
inside them and that's what makes up the database.
And just like an Excel document, you've got rows
and these are basically contain the data items.
So this could be your individual customers,
and then we've got columns
and these will be the fields in the database.
So here's a good example.
We've got a customer table here
and you can see here, we've got a row.
So customer number 2, the name's Warwick Matthews.
They live at 5 Leonard Street in Newcastle.
That's their postcode, NE6 3KW.
And they live in the UK.
Now, the actual field is we've got their first name,
we've got their surname, we've got their address
their zip code, their country, et cetera.
And that is what makes up our columns.
So it's very traditional.
This has been around, like I said
for the last 40 years or so.
And you have all used it
especially if you've ever used a spreadsheet.
Now, I'm not saying a spreadsheet is a database.
A spreadsheet is not a database
but I just like to use this example
so that it's very easy to visualize
because when you do connect to, let's say
a MySQL database, you can go into the database
and you'll see a whole bunch of different tables.
And it could be that it's storing information
about your customers
or about a product, et cetera, et cetera.
So with AWS, we have 6 different RDS
or a relational database engines.
So we've got Microsoft SQL Server.
We've got PostgreSQL, we've got Oracle,
we've got MariaDB, we've got MySQL,
and we've got Amazon's own proprietary database
which is called Aurora.
So those are the 6 relational databases
that are available with RDS.
Now, in terms of the RDS advantages
you can basically provision an RDS instance
and have it up and running in minutes.
And what an RDS instance effectively is
is it's an EC2 instance
but you don't have access to the operating system.
You only have access to the database
and it will be running 1 of those 6 databases.
So it could be that it's running MySQL
or PostgreSQL or SQL Server, et cetera.
And you can have multiple availability zones.
So you can have a primary database in 1 availability zone
and a secondary database in a second availability zone.
And you have automated fail over built in.
So if you lose 1 of those availability zones
RDS will automatically fail
over to another availability zone.
It also allows you to do automated backups.
So you can have your database backed up every day
or every week, depending on what you want to set.
And before RDS came along,
I used to work at 1 of the big
managed service providers in Tech
and whenever we provision database servers
it could take 8 days or longer to set this up.
So the great thing about RDS is you can have it
up and running in about 5 or ten minutes.
Sometimes in a lot less time as well.
So when would we use an RDS database?
Well, RDS is generally used
for online transaction processing, so OLTP.
And we covered it off a little bit in the
EBS section of the course.
But what is online transaction processing
and how does it compare to online analytical processing?
Well, OLTP is basically processes data
from transactions in real time.
So when you go on to, let's say Amazon
and you place an order
this is going to store your orders in a database.
So it's very transactional and that's the hint in the name.
So when you go and do a banking transaction
perhaps you're moving money from 1 account to another
or you're making a payment
or you're going in and booking a holiday or booking a hotel
it's transactional, it's all stored in a single transaction.
It basically comes and goes very, very quickly.
You basically are just writing a transaction to a database.
OLTP is all about data processing
and completing a large number
of small transactions in real time.
Whereas OLAP, this is we you process complex queries
to analyze historical data.
So you could be analyzing
let's say your net profit figures for the past 3 years
and doing a bit of sales forecasting,
and OLAP is all about data analysis,
using large amounts of data,
as well as complex queries that take
a long time to complete.
So it could be that we've got
a transactional database like this.
So we've got our order ID.
We've got the status who ordered it.
The address that it's going to
and what country that it's in.
You can see that this particular row
is in the processing stage.
Now, if we were to run a OLAP transaction
let's say we wanted to do net profit analysis.
So we basically have been asked to produce a report
comparing net profit
for car sales in 3 different regions.
Well you've got a large amount of data
that you've got to go through.
You've got to basically add up
the sum of all the cars sold in each region.
You need to figure out what the unit costs
for those cars were in each region.
Then you need to calculate the sales price of each car.
And then you basically have to subtract the sales price
to the unit cost to come up with your profit.
Basically, you're doing an analysis.
You're not doing a single transaction
such as selling the car.
So to get to my point
RDS is not suitable for analyzing large amounts of data.
So when we're using big data, we don't want to use RDS.
We want to use a data warehouse like Redshift,
which is optimized for online analytics processing.
So if you see anything about OLTP versus OLAP in your exam,
I want you to think of online transaction processing
is always with RDS,
OLAP is going to be with a product that's called Redshift.
And we will cover that off
in the big data section of the course,
but just remember going into your exam
if you get any scenario questions
where it's talking about, OLTP that's RDS,
OLAP is going be something like Redshift.
So moving on to Multi-AZ RDS.
And Multi-AZ RDS is very similar to Multi-AZ with EC2.
So this is where you're going to have a separate copy
of your database in a separate availability zone.
So RDS actually goes in and creates an exact copy
of your production database in another availability zone.
And it's all done automatically.
So if you have a look at this network diagram
we've got our load balancer off to the left.
We've got our EC2 instances on behind that load balancer.
So we've got 3 EC2 instances.
So these might be our web servers
and they're all connecting into our primary
which is in us-east-1a and that's our RDS instance
and that automatically replicates our production database
to another availability zone.
And that will be our standby,
and that is in us-east-1b.
So RDS Multi-AZ AWS basically handle the replication for you
when you write to your production database
this will write automatically to synchronize
to the standby database.
In terms of what type of RDS instances
can be configured as Multi-AZ
we've got SQL Server, MySQL,
MariaDB, Oracle, and PostgreSQL.
It's just important to differentiate this.
So you can have all of these RDS instances
as just single standalone databases
where there is no Multi-AZ enabled
with Aurora it is always Multi-AZ.
That's just the way Aurora is built.
And we're going to have a deep dive on Aurora later on
in this section of the course.
But basically all 6 RDS instances
can be configured as Multi-AZ
but you can only configure these 5 as single instances.
You can't have Aurora in a single availability zone.
So what happens if you have unplanned
maintenance or failures?
So we've got our load balancer off to the left.
We've got our 3 EC2 instances behind our load balancer.
And the way they're connecting in to our databases
they're using a thing called a connection string
and a connection string is just the address.
It's basically a web address
of where the database is a username and a password.
Now, Amazon handle all the DNS for that web address.
So if we suddenly have a failure
so if we have us-east-1a fail,
Amazon will detect that.
Basically what they'll do is they'll keep the same
web address then pointed at us-east-1b.
So it's basically just a DNS failure
or DNS fail-over, I should say,
from your primary to your secondary,
you don't need to worry about any of it.
You don't need to go in
and update your connection strings or anything like that,
it's all automated.
So basically what I'm trying to say is
if you lose your primary database
you're going to automatically fail over
to another availability zone.
And that's done through DNS
and we'll cover DNS in a bit more detail
later on in the course in the Route 53 Section.
So RDS will automatically fail over
to the standby during a failure
so that database operations can resume quickly,
and that's without any kind of administrative intervention.
So if we have a fail over on our primary
we're automatically going to fail over to our secondary
and that will be promoted to our primary database.
So we'll fail over automatically
from us-east-1a to us-east-1b.
So Multi-AZ is for disaster recovery
it is not for improving performance.
So you cannot connect to the standby database
when the primary database is active.
You can't run queries on your secondary database
with Multi-AZ and have your web servers send
your traffic to your primary availability zone.
It doesn't work like that.
Your backup or your secondary database
is simply there in case of a failure,
it's not there to improve performance.
You can improve performance using read replicates
but we're going to cover that off
in a another lecture in this section of the course.
So let's go in and have a look at how we can provision
an RDS instance in the AWS console.
So here I am in the AWS console.
So we'll find RDS under Databases,
so you can see it here, it's under Database.
So go ahead and click on RDS.
And what we're going to do is we're going to go in
and provision an RDS.
It's very similar process
to how we provision EC2 instances.
So we'll get this RDS landing screen.
And what we want to do is go ahead
and create our first database.
So what we're going to do
is we're going to do a standard create.
We're going to use MySQL,
we'll just keep it simple.
We won't use Aurora just yet.
And we'll scroll down and you can see here
we've got our different versions of MySQL.
So you can pick the MySQL version that you want.
And in here you've got different templates.
So you've got production, and this will basically put in
a Multi-AZ built in,
we've got Dev/Test, and then we've got free tier.
If we click on Free Tier
I'll show you how to make a Multi-AZ anyway.
So in here, we've got our database name.
I always just keep it quite simple for demos.
So I was just copy and paste
acloudguru, acloudguru, acloudguru.
So we've got our database name, got our username
and the password that we're going to use
to log into our database.
Down here, we've got our instance class.
So again, it's very similar to EC2
where we've got the different types.
So this is 1 VCPU, 1GB of RAM,
and it's not EBS optimized
and you can go in and change this.
I'm just going to do it as the default t2.micro.
and I'm going to scroll down and here we've got our storage
so we can choose if we want general purpose SSD
or provisioned IOPS or magnetic.
I'm just going to use general purpose SSD.
We've got a storage auto scaling.
This basically provides dynamic scaling support
for our database.
So it basically is a way of increasing storage
as in when we need it.
And we can specify our maximum threshold in here.
I'm just going to create and leave it all as default.
In here, we can create it as a Multi-AZ deployment,
but because we have chosen up here
that we want to use free tier
it won't let us choose that option.
If you click in here and click on Production
and scroll down, you will then be able to see
that it by default has created a standby instance.
So it's going make it Multi-AZ
and you then have the option to say,
hey, no, I don't want it.
I am going to make this as a Multi-AZ instance
just so you can see how it works.
And here we're going to just put it into our default VPC.
We'll cover off VPCs in a little bit later on.
Here, public access,
so whether or not this database is publicly accessible.
So whether or not you could connect to it,
say from your laptop,
it's generally a good idea not to do that.
So by default, it will be no.
And we'll look at what this means
in the VPC section of the course as well.
So scroll all the way down.
We've got our database authentication.
So we can just use password authentication
or we can combine it with identity access management.
I'm just going to use password
and then here is going to give us
our estimated monthly costs.
So you see here, this is going to be quite expensive,
it's going to be 846. I'm just going to
go back up in case I forget to turn this off.
The reason it's so expensive is because we've gone over
to a db.m6g.large.
And the reason it went to that
is because we went from free tier to production.
So if I go back to free tier
I can then go back to a t2.micro.
It's not going to be Multi-AZ,
but if I scroll all the way down
you can see that the estimated monthly cost
is that it's going to be free.
So let's just leave it at free tier
and let's go ahead and hit create database.
So that is now going in and creating my RDS instance
that can take a little bit of time.
Depends on the time of day
which availability zone you're doing it in
and the load on AWS.
You can see in here, we've got our
acloudguru database and the status is creating.
So what we want to do now
is I'm just going to pause the video.
I'm going to wait for this database to come up online.
Okay, so that instance is now available.
You can see it in here.
I can click in this instance.
And if I click on the DB identifier
I'll get all the information about this instance.
So connectivity and security
and here I can see my endpoint.
And this is a DNS address
that I would use to connect to this EC2 instance.
And because it's MySQL
I'd be doing it over Port 3306.
Essentially all you need is this connection address
a username and password,
which was acloudguru, acloudguru,
and we'd be able to connect in and start creating tables
and putting data into our database.
You can see here that it's in us-east-1b
in this particular instance.
And if we go over to our databases and click in here
and what we can do is go in and go modify
and we can go in and change the properties of this database.
So we could go in and make this Multi-AZ.
And to do that we just scroll down
and you'll be able to see it in here
availability and durability
and I could create a standby instance if I wanted to.
I'm not going to do that
we're just showing you how to provision an RDS instance
is very, very simple, similar to EC2.
Then once you're finished, you can just go in hit Actions
and you can go ahead and delete your RDS instance.
It will ask if you want to create a final snapshot
but I'm not going to.
And then you will have to type delete me in here
to make sure it deletes it.
And so there we go, just deleted it.
So let's go onto my exam tips.
On to my exam tips
we'll just remember the different RDS database types.
So we've got SQL Server, Oracle, MySQL,
PostgreSQL, MariaDB, and Amazon Aurora.
Remember that RDS is for online transaction processing.
So this is where you are doing single use transactions
such as booking a holiday
or buying a book from a bookstore
or whatever it is, that's a single use transaction.
So it's great for processing small transactions
like customer orders, banking transactions,
as well as payments and booking systems
is not really suitable
for online analytic processing workloads.
For that you want to use Redshift
for things like data warehousing and OLAP,
task like analyzing large amounts of data,
reporting and sales forecasting.
And we're going to cover Redshift off in more detail
in the big data section of the course.
So that is it for this lecture, everyone,
if you have any questions, please let me know.
If not, feel free to move on to the next lecture, thank you.


Increasing Read Performance with Read Replicas
===============================================

Okay, hello Cloud Gurus
and welcome to this lecture.
In this lecture we're going to look at
how we can increase our read performance
using Read Replicas.
So the first thing we'll look at is what is a Read Replica?
We'll look at some key facts around Read Replica.
Then I'll show you how to make a Read Replica
in the AWS console.
And then we'll go on to my exam tips.
So what can you use to improve performance with RDS?
Well, you can use these things called Read Replicas.
So what is a Read Replica?
Well, basically it's a read only copy
of your primary database.
So it's a way of designing your applications
so that when you go and do a query,
you aren't querying your main database,
you're not putting any more load on your main database.
What you're querying is an exact copy of your main database.
So that's all a Read Replica is.
And this is great for read heavy workloads
and it takes the load off your primary database.
Now going into your exam,
you're always going to get scenario questions
where they try and confuse you
between multiple availability zones and Read Replicas.
So just remember Multi-AZ is for disaster recovery only,
Read Replica is for boosting performance.
So Read Replica can also be
across different availability zones.
So it can be in a separate availability zone
but it can also be in the same availability zone
as your primary.
And Read Replicas can also be cross-region,
so you can put them in a completely different region,
in a completely different part of the US
or part of the world,
and it will replicate across to that region automatically.
But again, it's not used for disaster recovery,
it is only used for performance.
So each Read Replica has its own DNS endpoint,
and that's what we looked at in the last lecture
when we were looking at our endpoint addresses.
It's basically the unique web address to your database.
So with Multi-AZ you have a unique DNS endpoint,
and if you lose your primary
and it fails over to the secondary,
Amazon basically just update that DNS endpoint
to point to another IP address
so you don't need to worry about failure.
With Read Replicas you actually get
your own unique DNS endpoint.
So you have 2:
so you have one for your primary
and then one for your Read Replica.
And Read Replicas can also be promoted
to become their own databases.
So you can have a Read Replica and you can go in
and promote it and make it its own independent database.
And this basically will break the replication.
So it will no longer have replication between it,
but then you could connect up
a business intelligence server to that database
and you can run all kinds of queries.
So typically you might do this
when you're doing online analytics processing
and you're about to do a massive query
towards your database and you don't want,
you basically want to take it and make it its own database.
So that's where you would promote
a Read Replica to become own database.
But when you do that, it breaks the replication.
So some key facts.
Just remember that Read Replica
is all about scaling read performance,
it's not used for disaster recovery.
It requires automatic backups.
So you must have automatic backups enabled
in order to deploy a Read Replica.
And multiple Read Replicas are supported.
So you can have multiple Read Replicas
using MySQL, MariaDB, PostgreSQL,
Oracle, and SQL Server.
And you can actually have up to 5 Read Replicas
to each database instance.
So let's go over to the AWS console
and we'll look at how we can provision this.
Okay, so here I am in the AWS console.
I'm going to go to Databases and click on RDS.
And we're going to go ahead and provision a new RDS instance
and we're just going to keep it all as standard.
So let's go ahead and hit a Create Database.
And in here, we're going to go in
and we're going to select MySQL.
And then we'll scroll down
and we're going to use this in free tier.
And we'll scroll down,
and again, I'm just going to do the exact same thing
that we did last time.
So I'm just going to call it A Cloud Guru
and then I'm going to use that
for my username and password as well.
Of course you would never do this in production
but it just makes my life easier.
And scroll down.
So we've got a DB instance class.
We're going to use the t2.micro.
And scroll down, we'll leave everything else as default.
In here, you can see that Multi-AZ
has been turned off
because we're using a free tier account.
And scroll all the way down,
just leave everything as default.
And we're going to go ahead and create the database.
So I'm just going to pause the video
and wait for this database to come up online.
Okay, so this database has now been created successfully.
If we click in here and we go to our actions
we can go ahead and hit Create Read Replica.
And in here it's going to give us the different options.
So we've got a DB t2.micro,
so one vCPU, 1GB of RAM.
So I'm going to leave that as it is.
Multi-AZ deployment.
So you can actually have Multiple Availability Zones
for your Read Replicas.
I'm not going to do that,
I'm just going to keep it simple.
Down here you can see the region that we can put it in,
so you can choose different regions for your Read Replicas
and you can also choose a specific availability zones.
So you may want it
in a different availability zone to your primary,
or you may want it in the same availability zones,
it's entirely up to you.
And then in here you can specify whether or not
you want your Read Replica to be publicly accessible or not,
I'm not going to do that.
In here we've got our availability zones
so we can put out Read Replica
in the same availability zone as our primary
or we can put it in a different availability zone,
it's entirely up to you.
And then we basically have
all our different options in here.
So we've got our database authentication,
so whether we're using password authentication or IAM.
Whether or not we want encryption
on our Read Replica, et cetera, et cetera.
I'm just going to go up here.
We need to give our Read Replica a name.
I'm going to call it acloudguru-rr, Read Replica.
And then we're going to go ahead
and hit Create Read Replica.
So that is now going in and creating our Read Replica.
So I'm just going to pause the video
while I wait for the Read Replica to come up online.
Okay, so I'm just going to go in and refresh this dashboard
and you can now see that it's available.
So here is my Read Replica.
Now I can click on this in here
and I can actually see my Read Replica
is in a different availability zone to my primary.
My Read Replica is in the same region
which is what I specified when I created it.
So what we can also do is we can go in and we can go Actions
and we can actually go and promote this Read Replica.
So if I hit Promote,
this will promote my Read Replica
from being a Read Replica to its own primary database.
So that is one thing that we can do.
So it is possible to promote your Read Replica
to a primary just by clicking on that.
The other thing you can do
is you can go back to your databases,
I'm just going to go in and hit Cancel here.
Go to my Read Replica and I can modify it.
And I can actually go in and add multiple availability zones
to this Read Replica so I can make it Multi-AZ.
I do that just by scrolling down and clicking on here
and then I could go ahead and hit Continue,
and then that would add Multi-AZ to my Read Replica.
So let's go over and have a look at my exam tips.
So in the exam you're going to come
across different scenario questions
and essentially you just need to know
where you'd use Multi-AZ
and where you would use Read Replica.
So Multi-AZ is an exact copy of your production database
in another availability zone.
It's used for disaster recovery,
so that's key to remembering.
And in the event of a failure,
RDS will automatically fail over to the standby instance.
Whereas Read Replica is a read-only copy
of your primary database
and it's in the same availability zone
or cross different availability zones
or even in a different region in the world.
And it's used to increase or scale read performance.
And it's great for read heavy workloads
and takes the load off your primary database
for read-only workloads.
So this could be for things
like business intelligence reporting, et cetera.
So essentially going into your exam,
if you have any kind of scaling issues,
you're not getting good read performance,
how can you increase this?
Will you want to add a Read Replica to your environment?
If however, you're just looking for disaster recovery
then you want to make sure you have Multi-AZ enabled
on your production database.
So that is it for this lecture everyone.
If you have any questions, please let me know.
If not, feel free to move on to the next lecture.
Thank you.


What is Amazon Aurora
======================

Okay. Hello, Cloud Gurus.
And welcome to this lecture.
In this lecture, we're going to look at
Amazon Aurora.
So we're going to first
explore what Aurora is.
Then we'll look at its performance.
We'll look at some basics around Aurora.
Look at how we can scale it.
We'll look at Aurora replicas
and the different types.
We'll look at how we can back up Aurora.
And then we'll look at Aurora Serverless
and move on to our exam tips.
So what is Aurora?
Well, Aurora is basically
Amazon's proprietary database.
It's something that they have
invented themselves.
So it's a MySQL and PostgreSQL
compatible relational database engine.
That combines the speed and availability
of high-end commercial databases
with the simplicity and cost effectiveness
of open source databases.
So with Aurora,
you get 5 times better performance
than you would with MySQL,
And you get 3 times better performance
than you would with PostgreSQL
at a much lower price point
while delivering similar
performance and availability.
So this is like I said,
it's a database that Amazon
have invented themselves.
It's their own proprietary database technology,
but it's completely compatible
with MySQL and PostgreSQL.
So things to know about Aurora,
these are the basics,
It starts with 10 gigabytes in size,
and it scales in 10GB increments,
all the way up to 128 terabytes.
And it comes with storage auto-scaling,
so automatically scale as you add
more and more data to it.
The compute resources can scale
up to 96 vCPUs and 768 GB of memory,
and 2 copies of your are data contained
in each availability zone,
with a minimum of 3 availability zones.
So this means,
you always have 6 copies of your data,
so you can see why it's really,
really redundant.
In terms of scaling Aurora.
Aurora's designed
to transparently handle the loss
of up to 2 copies of data
without affecting database
write availability,
and 3 copies of data.
And it won't affect our read availability.
So it is incredibly scalable.
Aurora storage is also self-healing.
So data blocks and disks
are continuously scanned for errors,
and they're repaired automatically.
And there's 3 different types
of Aurora replicas available.
So you've got Aurora replicas.
This allows you to have 15
read replicas with Aurora.
But then you can also have MySQL
and PostgreSQL replicas.
So you can have a MySQL replica,
and this will be 5 read replicas
with Aurora MySQL,
and then PostgreSQL again,
5 read replicas with Aurora PostgreSQL.
So if we were to compare
the types of Aurora replicas,
so we've got Aurora replicas
and MySQL replicas.
So like I said, the Aurora replicas,
you have up to 15 replicas at one time.
MySQL is 5.
The replication type,
It's going to be asynchronous with Amazon Aurora,
but it's down to the millisecond.
Whereas with MySQL, it's still asynchronous
but it's down to the seconds.
And then in terms of performance impact
on the primary.
So with Aurora replicas,
there's low performance impact,
with MySQL replicas
there's going to be high.
With Aurora replicas,
you can do it in-region.
With MySQL,
You can do it cross-region.
Automated failover from Aurora replicas
are enabled, but not with MySQL replicas.
And then you can see that the support
for user defined replication delay
and for different data or schemas
is not supported with Aurora read replicas,
but it is with MySQL replicas,
moving onto backups with Aurora.
So automated backups are always enabled
on Amazon Aurora DB instances,
and backups do not impact
database performance at all.
And you can always take snapshots
with Aurora and this doesn't impact
on performance either.
And you can actually share your Aurora snapshots
with other AWS accounts.
Moving on to Aurora Serverless.
And so, this is basically an on-demand,
auto-scaling configuration
for the MySQL compatible
and PostgreSQL compatible editions
of Amazon Aurora.
And essentially,
it's a serverless database cluster
that automatically starts up
and shuts down and scales capacity
up or down based on your application's need.
So essentially you're only paying for it,
when you are using it.
So it can save you a lot of money
but then you get all the advantages
of Aurora as well,
because it scales automatically
as in when you are going to,
you know have a significant impact
on the database.
So that's Amazon Aurora Serverless.
And to be honest, coming into the exam,
you will get some exam questions
where it's talking about,
you need the performance of Aurora,
but you're going to have spiky workloads.
What should you look at?
Then you want to look at Amazon Aurora Serverless.
So in terms of Aurora Serverless use cases,
so Aurora Serverless provides a relatively,
simple cost-effective option
for infrequent, intermittent,
or unpredictable workloads.
So onto my exam tips,
Aurora, basically just remember
going into your exam,
that you have 2 copies of your data
contained in each availability zone
with a minimum of 3 availability zones.
So you always had at least
6 copies of your data.
You can share Aurora snapshots
with other AWS accounts.
There's 3 types of replicas available.
So you've got Aurora replicas,
MySQL replicas, PostgreSQL replicas,
and automated failover
is only available with Aurora replicas.
And Aurora has automated backups
turned on by default.
You can also take snapshots with Aurora.
Like I said, you can share these snapshots
with other AWS accounts
and then use Aurora Serverless
if you want simple cost-effective options
for infrequent, intermittent
or unpredictable workloads.
So that is it for this lecture everyone.
If you have any questions,
please let me know,
If not, feel free to move on to the next lecture.
Thank you.


DynamoDB Overview
====================

Okay, hello club gurus,
and welcome to this lecture.
In this lecture, we're going to look at DynamoDB.
So we're gonna start with what DynamoDB is.
We're gonna look at it at a very high level,
then talk about the read consistency models,
and then look at DynamoDB Accelerator,
which is otherwise known as DAX.
We'll look it out on-demand capacity,
we'll then look at DynamoDB security,
and then we'll move on to my exam tips.
So what is DynamoDB?
Well, like I said,
Aurora is a proprietary database technology for Amazon.
DynamoDB is exactly the same,
except it's not a relational database technology,
it's a non-relational database technology.
So Amazon DynamoDB is a fast
and flexible NoSQL database service
for all applications that need consistent
and single digit millisecond latency at any scale.
It's fully managed database and it supports both document
and key value data models and it's flexible data model
and reliable performance make it a great fit
for things like mobile, for web, gaming,
ad-tech, IoT and many other applications.
So some facts about DynamoDB,
it's all stored on SSD storage.
It's spread across three
geographically distinct data centers,
and you get eventual consistent reads by default,
and you can also opt in for strongly consistent reads.
So what does that actually mean?
What's the difference between eventually consistent reads
and strongly consistent reads?
Well, eventually consistent reads means
that consistency across all copies of data
is usually reached within one second or within a second.
Repeating a rate after a short time
should return the updated data.
And this gives you the best read performance.
However, a strongly consistent read returns
a result that reflects all writes
that received a successful response prior to that read.
So you don't have to wait a second
with strongly consistent reads.
So let's look at DynamoDB Accelerator or DAX.
It's fully managed,
it's highly available
and it's an in-memory caching service for DynamoDB.
And it actually gives you up to 10 times
the performance improvement
than just using DynamoDB on its own.
And it reduces request times for milliseconds
to microseconds even under load.
And there's no need for developers
to manage your caching logic.
It's all done by DynamoDB DAX,
it's all built-in
and it's compatible with DynamoDB API calls.
Let's have a look at how it works.
So if you've got a traditional cache,
so you've got your application
and essentially what your application
will do is it will query the cache
and see if that information is there.
If it's not there,
then it will go and query your database.
So in this case,
it'd be DynamoDB.
DynamoDB will then return that information back
to your application,
and then it will store it in the cache
for whatever the time to live is.
So that's the way a traditional cache works.
Now, caching with DAX is a little bit different.
Basically, your application doesn't
have to juggle between the two,
your application speaks directly
to DynamoDB DAX Accelerator,
and then it looks as to whether
or not that information is inside and cached inside DAX.
If that information isn't cached inside DAX,
what DAX will do is then go interrogate DynamoDB,
bring the data out and then it will cache it in DAX itself.
So your application only has to worry
about connecting to DAX.
It doesn't have to worry about connecting to a cache,
then connecting to DynamoDB,
and then passing that information back to a cache.
So it's much more streamlined process.
In terms of on-demand capacity,
so with DynamoDB it's pay-per-request pricing.
This is a good balance of cost and performance.
You don't have to worry about a minimum capacity.
There is none,
and you pay more per request than with provisioned capacity.
And it's great for things
like new product launches, et cetera.
Moving on to security,
so we've got our encryption at rest using KMS.
We also can connect to DynamoDB using a site-to-site VPN.
We can also use Direct Connect,
which we're gonna cover off later on
in the VPC section of the course.
DynamoDB works with IAM policies and roles.
So it allows you to put fine-grain access
as to who can actually access your data
and make changes, et cetera.
And it integrates with CloudWatch and CloudTrail.
Again, we're going to look at
these services later on in the course,
but essentially it's a way of monitoring both performance
and watch changes people are making to DynamoDB.
So it integrates with all of them as well as VPC endpoints.
And again, we're gonna cover that off
in the VPC section of the course,
but essentially a VPC endpoint just allows you
to directly communicate with something
like DynamoDB without leaving the Amazon backbone networks.
So you don't have to traverse the internet
in order to speak to DynamoDB.
Your applications can do it all over the backbone network
of AWS using VPC endpoints.
So onto my exam tips,
just going into the exam,
remember these facts about DynamoDB.
First of all, it's stored on SSD storage.
It spread across three geographically distinct data centers.
By default, you're gonna get eventually consistent reads,
but you can also have strongly consistent reads.
Now, if you remember we covered it off,
but what's the difference
between eventually consistent reads
and strongly consistent reads?
Well, eventually consistent means you get consistency
across all copies of your data within one second.
So repeating a read after a short period of time,
so one second should return the updated data.
This gives you the best read performance,
whereas strongly consistent reads is where it returns
a result that reflects all writes that received
a successful response prior to the read.
So that's it as an introduction to DynamoDB,
we are gonna deep dive quite a bit on DynamoDB
in the rest of this section of the course.
So if you've got the time,
please join me in the next lecture, thank you.


When do we use DynamoDB Transactions ?
=======================================

Okay, hello, Cloud Gurus,
and welcome to this lecture.
This is going to be a quick lecture,
and we're going to look at DynamoDB Transactions.
So when do we use DynamoDB Transactions?
So the first thing we're going to look, is what ACID is.
It's not the type of acid that you're thinking of,
this is ACID for databases, we're going to look at
how we can use ACID with DynamoDB,
and essentially it's all or nothing transactions.
We'll look at the different use cases,
and then the transactions themselves,
and we'll go on to our exam tips.
So ACID basically stands for
atomic, consistent, isolated, and durable,
and it's a methodology for using databases.
So we'll start with atomic, and this is where
all changes to the data must be performed
successfully or not at all.
So it's either all or nothing.
Consistent, so the data must be in a consistent state
before and after the transaction.
Isolated, so no other process can change the data
while the transaction is running.
And then durable, and the changes
made by a transaction must persist.
So going into the exam, you will get scenario questions
where they talk about the ACID methodology for databases,
and essentially it's basically all or nothing.
You're inserting a transaction into a database,
or you're not.
So you can use ACID with DynamoDB,
but you need to use DynamoDB Transactions.
And DynamoDB Transactions provides your developers
with atomicity, consistency, isolation, and durability,
or ACID, across one or more tables
within a single AWS account and region.
So you can use transactions when building applications
that require coordinated inserts, or deletes,
or updates to multiple items
as a part of a single, logical, business operation.
So essentially, if you get a scenario-based question,
where they're talking about using assets
or having atomicity, consistency, isolation, and durability,
and you want to do this with DynamoDB,
you need to enable DynamoDB Transactions.
And ACID basically just means all or nothing.
So if you were doing a transaction,
it either the succeeds across one or more tables.
But if that transaction, for some reason,
one of the tables doesn't respond or something,
then it fails, and the whole transaction fails.
So it stops you from updating just one table here
and not another table there, that's all ACID means.
So your use cases for DynamoDB Transactions
is doing things like processing financial transactions,
fulfilling and managing orders,
building multiplayer game engines,
or coordinating actions across
distributed components and services.
So in terms of transactions themselves,
like I said, it's multiple "all or nothing" operations.
You can use these for financial transactions,
or fulfilling orders, for example.
And you actually get three options for reads.
So you get eventual consistency,
strong consistency, and transactional.
And then you get 2 options for your writes,
which is standard and transactional.
And you're allowed up to 25 items per go,
as well as 4 MB of data per transaction.
So, onto my exam tips.
DynamoDB Transactions, if you see
any scenario question that mentions ACID requirements,
I want you to think of DynamoDB Transactions.
DynamoDB Transactions provides developers
with atomicity, consistency, isolation,
and durability, or ACID, across one or more tables
within a single AWS account and region.
And just remember the transactions are all or nothing.
So if you are designing an application,
and you want to make sure that every single table is updated
in one transaction and nothing fails,
then you want to use ACID,
and you need to use DynamoDB Transactions.
So that is it for this lecture, everyone.
If you have any questions, please let me know.
If not, feel free to move on to the next lecture.
Thank you.


Saving your data with DynamoDB Backups
========================================

Okay, hello Cloud Gurus
and welcome to this lecture.
This is a really quick lecture,
and we're going to look at backups
and saving your data with DynamoDB backups.
So we're going to look at our on-demand backup
and restore capabilities with DynamoDB
and then we're going to look at our point-in-time recovery
capabilities with DynamoDB.
So DynamoDB has on-demand backup and restore
so you can do full backups at any time.
And the cool thing about it is there's zero impact
on table performance or availability.
And you get consistency within seconds
and it's basically retained until deleted.
And it operates within the same region as the source tables.
So if you are creating an on-demand backup,
that backup is always going to be in the same region
as your source table.
So that's all you need to know about on-demand backups
going into your exam.
The next thing you should know about
is point-in-time recovery.
So this protects against accidental writes or deletes.
You can restore to any point in the last 35 days
and it's done using incremental backups.
Now, the thing to remember about point-in-time recovery
is it's not enabled by default.
You have to go in and turn it on.
So do remember that going into your exam.
And the latest restorable time is 5 minutes in the past.
So for some reason you have an accidental write or delete,
you will have to go back 5 minutes in the past
from that point-in-time.
So bear that in mind as well.
So like I said, this is a really quick lecture.
It just lets you know that you can go in
and do manual backups of DynamoDB
as well as turn on point-in-time recovery
and it will allow you to store your data
for any point in the last 35 days
and the latest restorable time is five minutes in the past.
So that is it for this lecture everyone,
if you have any questions, please let me know.
If not, feel free to move on to the next lecture, thank you.


Taking your data Global with DynamoDB Streams and Global Tables
===============================================================

Okay, hello Cloud Gurus
and welcome to this lecture.
In this lecture, we're going to look at how you can take
your data global using DynamoDB streams
and global tables.
So we'll start by exploring what DynamoDB streams are,
we'll then look at global tables,
I'll then go in and set this up in the console.
So we will set up a DynamoDB table in one region,
turn on global tables and stream that DynamoDB table
to another region and then we'll move on to my exam tips.
So let's start with streams,
what are DynamoDB streams?
Well, they're basically time-ordered sequence
of item-level changes in a table.
So you can think of it as basically first in, first out
streams of your data.
So every time your data changes,
it comes with a sequence number
and it's completely sequenced.
And these sequences are stored
in what we call DynamoDB streams
and they're actually stored for 24 hours.
Now your data or the sequences are broken up into shards.
So shard is just a bunch of data
that has sequential sequence numbers.
So every time you make a change to a DynamoDB table,
that data is going to be stored sequentially
in a stream record which is broken up into shards.
And changes are just things like inserts,
updates, and deletes.
Every time you insert a record or update a record
or delete a record, that's going to give you
a FIFO time sequence.
Those changes are stored in a DynamoDB stream
and to make it manageable,
we just basically split those sequences up into shards.
And you can actually combine it with Lambda functions,
for functionality like stored procedures
in traditional databases
and we will cover off what Lambda is later on in the course,
but essentially if you've ever done anything
like with SQL Server or MySQL
where you've got stored procedures in your database engine,
you can add that kind of functionality to DynamoDB
and you do that using Lambda.
But don't worry, that's not an exam topic,
it's just something really cool
that you can do with DynamoDB
and we will cover off what Lambda is later on in the course.
So now that you know what stream records are,
we'll move on to what global tables are.
And these are basically managed multi-master,
multi-region replications.
So it's a way of replicating your DynamoDB tables
from one region to another.
And this is great for globally distributed applications.
So if you've got an application
that spread all across the world
and you want to have consistent DynamoDB tables
in each region, then you can have global tables.
And the technology that drives this
is just basically DynamoDB streams.
So it's based on DynamoDB streams
and you will need DynamoDB streams turned on
in order to enable this.
Basically, it gives you multi-region redundancy
for disaster recovery or high availability.
And the cool thing about this
is it's built into DynamoDB natively,
you don't have to rewrite your applications,
you can just go into the console and turn it on.
And the replication latency is under one second
in most cases, which is just crazy if you think about it.
So you can have your DynamoDB table
updating on the east coast
and then have that replication go across to the west coast
of the US, let's say,
and that latency can be under one second.
Sometimes it could be longer
if you're doing something like let's say,
Australia to London, but it is really, really fast
in terms of latency.
So the best way to learn this is to go in
and have a look at how this works in the AWS console.
Okay, so here I am in the AWS console.
Now we'll find DynamoDB under database,
so click on here and we'll go in
and create a new DynamoDB table.
Now the table name, we'll just give it a simple name,
we'll call it my DynamoDB
and we'll call it global table one, something like that.
And our primary key, this is our partition case,
so this is basically what are we using in our primary key
to identify our records?
And in most cases, this might be like a user name
or it could be a student number or client number.
I'm just going to call it ID and that's it.
I'm going to leave everything else as default
and I'm going to go ahead and create my DynamoDB table.
So that table is now being generated.
If I click Close, I'll be able to see my table in here
and it is all ready to go.
So we can actually click in here
and we can go over to our items
and this is where we can put items into our table.
So what we can do is we can go in and hit Create Item
and we can see here, we've got our IDs,
this might be our client ID
so let's just put in a random number.
And in here we can create a different items underneath it
so we're going to go in
and add a string, we'll call this, name
and then in here, I'm just going to write
Hello Cloud Gurus.
So let's go ahead and save that item.
And that has now put an item with this ID number
into my DynamoDB table.
So if I go over from Northern Virginia,
let's go over to Northern California
and just change regions and have a look.
And because Northern California,
we haven't created any tables,
you can see it's completely blank at this stage.
So if we go back to Northern Virginia,
let's go in and we'll click in here
and you'll see that there is a tab that says Global Tables.
So what we can do is we can click on Global Tables
and it says: To create a global table,
ensure that DynamoDB streams are enabled.
So you can see that warning message there
very, very clearly.
Now this is always going to be a popular exam topic.
So if you are having trouble
creating a DynamoDB table in another region,
it's because you haven't enabled DynamoDB streams.
And then you just need to remember basically,
that you can have basically a read replica
of DynamoDB in another region.
The way you do that is by turning on global tables,
which means you need to turn on DynamoDB stream.
So let's go ahead and enable our streams.
And in here, it's just says: New and old images,
so this is both the new and old images of the item,
so you want to do that.
And then in here we now have an option
to add in global tables.
Now we can click in here and go to Add Region
and this will add a replica to a different region.
And not all regions are available,
so you can select some regions, but not others.
As more and more regions have more and more functionality,
You'll see more and more regions here.
So we could do Northern California.
The problem is, is this DynamoDB table
is under free tier right now.
If I go ahead and hit Create Replica,
I'm going to get this error message
which basically says my
write capacity should either be
pay-per-request or autoscaled.
The way you can fix that is if you just go back
over to capacity and we go over to auto-scaling
and just hit Read Capacity and Write Capacity in here.
We can go ahead and save that,
use the default values.
And then if we go back to our global tables,
click in Add Region,
and now we will go over and add a replica
to the California region, let's go ahead and do that.
So in Northern California Create Replica,
and that is now creating a replica
in our Californian region.
And this can take a couple of minutes to go in
and create the replica.
Let's go over to our different region.
So we'll go back over to Northern California
and let's have a look and see if it's up here.
So right now you can see that there is no table there.
I'm just going to pause the video,
wait for this table to come up online.
And now if I go ahead and hit Refresh,
we can now see that it is now active
so I can click on my table.
If I go in and click on my Items,
I'll be able to see my item in there.
Let's go back over to a us-east-1
and let's go in and add a second item
just so we can see the replication in action.
So go ahead and create an item.
Our ID, We'll just give it another number,
a much higher number.
We'll go in and add an appendage
and we'll add in a string and we'll just say, Name
and then in here we'll say, Hello Cloud Gurus Again.
And we'll go ahead and hit Save.
And so that is now adding in our second item in here
and you can see because the name is spelled with a capital
N, it's actually put in as a new column.
That's the great thing about DynamoDB,
is it updates its structure for you
as you add more variables
so it's not like a traditional relational database.
You can have all types of data in there
and you don't need to go in and append the database itself,
the structure of the table.
So if we click back over from Northern Virginia
and go over to Northern California,
we'll be able to see in here.
We've got our DynamoDB table, click on our Items
and you can see that it's replicated across automatically.
So we didn't have to go in and redesign our application
or anything like that,
DynamoDB did that just by turning on global tables
and enabling DynamoDB streams.
So let's go over to my exam tips.
So onto my exam tips.
So just remember going into your exam,
that DynamoDB global tables gives you multi-master,
multi region replication.
This is great If your applications are all over the world
because they can connect in it to other DynamoDB tables.
It's based on DynamoDB streams,
so this is going to be a very popular exam topic.
You might need to be turning on global tables
and it won't work.
What could be causing this?
Well you need to enable DynamoDB streams.
DynamoDB global tables gives you multi-region redundancy
for disaster recovery or high availability.
And you don't have to rewrite your application,
it's all built into DynamoDB natively as we saw
and the replication latency is under one second.
That replication we saw when we created our new item,
by the time we went over to Northern California,
it was already there.
So that is it for this lecture everyone.
You've done really, really well.
We're ready to move on and review everything we've learnt
in the databases section of our course
and then we're ready to move on to our next section.
So if you've got the time,
please join me in the next lecture, thank you.


Databases Exam Tips
====================

Okay. Hello, Cloud Gurus.
And welcome to this lecture.
In this lecture we're going
to review everything that we've just learned
about databases in this section of the course.
So, let's start with RDS.
So, RDS database types come in SQL server, Oracle, MySQL,
PostgreSQL, MariaDB, and Amazon Aurora.
RDS is for online transaction processing workloads.
So, this is where you are basically processing lots
of small transactions like customer orders,
banking transactions, payments, and booking systems.
It's not really suitable for online analytics processing
so for that you would use something
like Redshift for data warehousing
and OLAP tasks like analyzing large amounts	
of data reporting and sales forecasting.
And we're going to cover
off a Redshift later on in the course.
Now you'll always be quizzed
on where you should use Read Replicas
versus where you should use Multi-AZ.
So Read Replicas,
this is where you're scaling your read performance.
So, it's primarily used for scaling
not for disaster recovery.
It requires automatic backups to be turned on.
So, if you can't for some reason create a Read Replica
of your primary database,
it just means you haven't turned on automatic backups.
And then multiple Read Replicas are supported.
So, you can have up to 5 Read Replicas per MySQL,
MariaDB, PostgreSQL, Oracle, and SQL server.
In terms of Multi-AZ versus Read Replica,
like I said earlier,
so an exact copy of your production databases
in another availability zone for Multi-AZ.
It's used only for disaster recovery
and in the event of a failure,
RDS will automatically failover to the standby instance.
Whereas with Read Replicas read-only copy
of your primary database in the same availability zone,
cross availability zones or across regions is available.
And of course you use this to increase
or scale your read performance.
If you get a scenario question
about your database bottlenecking
and how you can get around it
think automatically of Read Replicas.
So, it's great for read heavy workloads
and it takes the load
off your primary database for read-only workloads.
So, this could be things
like business intelligence reporting jobs, et cetera.
So, moving onto Aurora, just remember going
into your exam that Aurora is Amazon's proprietary database
that's something that they have created themselves
it's compatible with MySQL, as well as PostgreSQL.
It always has 2 copies
of your data in each availability zone
with a minimum of 3 availability zones.
So, you always have 6 copies of your data.
So it's very, very redundant.
You can also share Aurora snapshots with other AWS accounts
and you have 3 types of replicas available with Aurora.
So, you've got a Aurora replicas themselves,
but then you can also create MySQL replicas,
and PostgreSQL replicas.
An automated failover is actually
only available when you have Aurora replicas.
And Aurora has automated backups turned on by default.
And you can also take snapshots with Auroras
and you can share these snapshots with other AWS accounts.
Now don't forget about Aurora Serverless
and what the use cases are for it.
So, it provides a relatively simple cost effective option
for infrequent intermittent or unpredictable workloads.
If you see any scenario based question where it's talking
about setting up a Serverless database
I want you to automatically think
of Aurora and we will cover
off what Serverless is later on in the course as well.
So moving on to DynamoDB, 4 facts about DynamoDB,
it's stored on SSD storage, it's spread
across 3 geographically distinct data centers,
and you can have eventually consistent reads
which is what you get by default,
but then you can also have strongly consistent reads.
And just in case you need a reminder
of the difference between eventually consistent reads
and strongly consistent reads.
Eventually consistent means that consistency
across all copies of data is usually reached
within about a second and repeating a read
after a short time should return the updated data.
And this gives you the best read performance.
However, if your application needs
to automatically get any changes to the data.
You want to turn on strongly consistent reads
and this will return a result that reflects all
rights that have received a successful response prior
to the read.
Moving on to DynamoDB transactions.
This is where you have multiple all-or-nothing operations.
So, it's good for things like financial transactions
or fulfilling orders.
And you have 3 options for your reads.
You have eventual consistency,
strong consistency and then transactional.
And you have 2 options for your writes,
we have standard and transactional.
And you can have 25 items using DynamoDB transactions
or 4 MB of data.
And if you see any scenario question
that mentions ACID requirements,
I want you to think of DynamoDB transactions.
So, DynamoDB transactions provides developers
with atomicity, consistency, isolation,
and durability across one or more tables
within a AWS account or AWS region.
So again, if you see something about ACID requirements
with DynamoDB, you need to use DynamoDB transactions.
And this is basically means,
you've got all-or-nothing transactions.
So, you either need to add an item
to all the tables in one transaction.
But if one of those tables fails, for whatever reason,
you don't want one item going into one table
and not into another table,
because that wouldn't have ACID consistency.
So, you need to make sure you've got DynamoDB
transactions turned on, and that way either,
the write will happen or it won't.
Moving on to DynamoDB On-Demand backups and restore.
So, this is where you can backup your DynamoDB database
with full backups at any time.
It has zero impact on your table performance
or availability.
And you have consistent
within seconds and your backups are retained until deleted.
And it operates within the same region as the source tables.
When you're creating a DynamoDB backup,
it will be in the same region as the source table.
And then we have DynamoDB Point-in-Time Recovery.
This is really cool technology.
It protects against accidental writes or deletes,
and you can restore your DynamoDB database to any point
in the last 35 days, the backups are incremental.
It's not enabled by default.
However, you will have to go and turn it on.
And the last restorable rate is going to be 5 minutes
in the past.
We then learned about DynamoDB streams.
And this is where you can maintain first
in first out records of your data.
So as you go and add transactions,
basically it's given a sequence number.
These sequences are stored in a stream,
and the data's broken up into shards.
So, it's just time audit sequences
of item level changes in a table.
Every shard is stored for 24 hours
and your stream records are going to consist
of things like inserts, updates, and deletes.
And you can combine this with a Lambda function
to add functionality like stored procedures.
So if you're SQL DBA or a MySQL DBA,
and you've used stored procedures before,
essentially you can have the same tech kind
of concept with DynamoDB.
You can do that using a Lambda function,
and we will explore what Lambda is later on in the course.
So with global tables,
this is where you have managed Multi-Master,
Multi-Region Replication.
It is designed where
you've got applications that are globally distributed.
You need to have DynamoDB streams turned on.
So, it's based off DynamoDB streams.
And this is basically allows you to have
Multi-region redundancy for disaster recovery
or high availability.
And you don't have to rewrite their application.
As we saw, this is all built in functionality
within DynamoDB.
And your application latency
typically will be under one second.
And the key thing here is to remember
if they're talking about
if you want to add redundancy to DynamoDB,
you need to turn on global tables.
If global tables isn't turning on,
you need to make sure that you've enabled DynamoDB streams.
So, that is it for this lecture everyone.
I hope you have learned an awful lot
about the databases in this section of the course.
We're now going to move on to VPCs.
So, if you've got the time,
please join me in the next lecture.
Thank you.


Virtual Private Cloud (VPC) Networking
======================================


VPC Overview
============

Hello Cloud Gurus,
and welcome to this lecture.
In this lecture we're going to look at VPCs.
Now, this is the networking component of the course.
This is going to be one of the most difficult sections
but we will take it easy.
And basically by the end of this section of the course
you need to know how to build a VPC from memory.
If you can do that,
you are absolutely going to ace every networking question
in your exam.
Go ahead, follow through this section and then try and--
it can take a couple of times,
but try and build a VPC from memory from scratch.
So let's look at what we're going to learn in this lesson.
So we're going to learn what VPCs are,
we're going to learn about networking, VPNs,
we're going to have a look at a network diagram
of the VPC that we're going to build in the next lecture,
then we're going to look at some features of VPCs,
we're going to compare VPCs,
and then we'll go on to my exam tips.
So what is a VPC?
We'll just think of a VPC
as a virtual data center in the cloud.
And you've already been using VPCs
when you provisioned EC2 instances
in the EC2 section of the course.
That goes into your default VPC.
And every region in every AWS account has a default VPC
that set up automatically.
So VPC is basically just a logically isolated part
of the AWS cloud, where you can define your own network.
And it gives you complete control of virtual networks
including your own IP address ranges, subnets, route tables,
and network gateways.
And we'll learn what all those are in one second.
So basically, it's a fully customizable network.
And you can leverage multiple layers of security
including security groups and network access control lists
to help you control access to your EC2 instances
in each subnet.
So typically, you would have a three-tier architecture.
So you've got your web servers
and your web servers need to be public-facing,
and by public-facing,
we simply mean that they're Internet accessible.
So people from the Internet can access those web servers.
And this would be done over port 80 or port 443,
so HTTP or HTTPS.
We then have our application tier.
And so this could be our application servers
and they're doing some form of business logic,
perhaps they're rendering an image or something like that.
And they're in a private subnet
and they can only speak to the web tier
and then to the database tier.
And then we have the database tier on the backend.
And this is a private subnet,
and they can only speak directly to the application tier
which then passes information back to the web tier.
So this is an example of a fully customizable network
and the only front-facing servers
are going to be your web servers.
Additionally, you can also create
hardware virtual private network connections
between your corporate data center and your VPC.
And essentially, this allows you to leverage the AWS cloud
as an extension of your corporate data center.
And we'll look at different methodologies
of how to do that later on in this section of the course.
Now, before we look at our network diagram,
I just want to show you a really cool website.
It's just called cidr.xyz.
And so this is how you calculate your IP address ranges.
And there's 3 common IP address ranges
which we'll cover off.
But throughout the rest of this section of the course,
we're going to use 10.0.0
and then it's going to be a forward slash
and then it's a 16 network.
So let's do a 16.
Now that will tell us straight away using this website
what our first usable IP address is,
what our last usable IP address is,
how many IP addresses in this network range.
And you can go ahead and you can make it a /24
if you want a smaller amount of IP addresses.
And the smallest network size
that you can have with AWS and VPCs is a slash 28,
which will give you 16 IP addresses.
Amazon do reserve some IP addresses themselves though.
So the largest we can have with VPCs is a /16.
That will give us 65,536 IP addresses.
And then using this website,
we can see what our first usable one is,
what our last usable one is,
and then what our subnet mask is.
So let's have a look at a typical network diagram for a VPC.
And this is pretty much what we're going to create
in the next lecture.
So, first of all, we have our regions.
Let's say we deploy this into us-east-1.
And what we need to do is when we create a VPC
is choose our IP address range.
So you can use 10.0.0.0, or you could use 172.16.0.0,
or you could use the most common one for home networking
which is 192.168.0.0.
Most corporations use 10.0.0.0.
So that's what we're going to do
throughout the rest of this course.
And we're going to use
the maximum addressable size for VPCs,
which is just a /16.
So that will give us the most number of IP addresses
inside our VPC.
So when we create a VPC, what's going to happen
is it's going to create a route table, a network ACL,
as well as a router.
And then what we're going to do is:
we're going to go in and create our own public subnets.
We're going to have an instance in there.
We're going to have a security group.
We're going to attach an Internet gateway to our VPC.
And as soon as we do that,
our instance in our public subnet
will become Internet accessible.
And this typically would be a web server or something.
We can then also build in a backend.
So we could build in a private subnet.
Note, the public subnet
has a IP address range of 10.0.1.0/24,
and then our private subnet could be 10.0.2.0/24.
And then if we wanted to be able to connect to our instances
in our private subnet
from let's say our corporate data center,
we could attach a virtual private gateway to our VPC,
establish a VPN connection,
and then we'd be able to communicate to our instances
in our private subnet as well.
So this is pretty much what we're going to build
in the next lecture.
Before we do that, though, let's just have a quick look
at what we can do with our VPCs.
So we can launch instances into a subnet of our choosing,
so we can launch them into private subnets
or into public subnets.
We can create custom IP address ranges for each subnet.
So like I said, we had 10.0.1.0 as our public,
and we could have 10.0.2.0 for our private.
We can configure route tables between subnets,
and we'll look at how we can do that in the next lecture.
We can attach an Internet gateway to our VPC.
The cool thing about VPCs is it just gives us more control
and better security controls over our AWS resources.
We can provision things like access control lists,
and this basically allows us
to create network access control lists,
and we can actually block specific IP addresses
using network access control lists.
And that is a very popular exam topic
that will come up around VPCs, is,
do you use security groups to block IP addresses
or do you use network ACLs?
Well, it's network ACLs,
and we'll have a lecture on that coming up as well.
So in terms of our default VPC versus our custom VPC.
Like I said, when you provision--
Every account within AWS comes with a default VPC
in every single region, so it's much more user-friendly.
It's basically a way for you to launch EC2 instances
and not have to worry
about any of the networking components whatsoever.
All subnets in default VPCs
have a route out to the Internet,
so that is to say that they're all public.
And each EC2 instance has both a public and private address
within a default VPC.
Now with our custom VPCs, they're fully customizable
but it does take some time to set up.
And we'll look at how we can set that up
in the next lecture.
So just at this stage, we'll just go through my exam tips.
So I just want you to remember
that a VPC is a logical data center in AWS.
It consists of Internet gateways
or virtual private gateways, consists of route tables,
network access control lists, subnets, and security groups.
And we'll look at how to create all of those
in the next lecture.
And then always remember that one subnet
is always in one availability zone.
And this is again, a very popular exam topic.
So when you create a subnet,
that subnet is defined in one availability zone.
So you can't have a subnet
that spans multiple availability zones.
And we will, like I said, look at that in the next lecture.
So if you've got the time,
please join me in the next lecture.
Thank you.


Demo : Provisioning a VPC - Part 1
====================================

Hello Cloud Gurus,
and welcome to this lecture.
In this lecture we're going to look
at how we can build our own custom VPC.
This is going to be very hands-on
and it's going to be split over 2 different lectures,
but by the end of this you should be able
to build a custom VPC from scratch.
And I would definitely go into the exam
being able to do it from memory.
So watch this a couple of times
and build your own VPC the day before you do your exam
and then you're going to ace any networking questions.
Okay. So here I am in the AWS Management Console
and if we scroll down,
we'll be able to see VPC under network and content delivery.
So let's go ahead and click in here.
Now this will give you the following splash screen.
So I'm in the Northern Virginia region right now.
You can actually go and see
all your VPCs for every other regions.
You can see I've got one default VPC in every other region.
And what we're going to do is we're going to go in
and create a VPC.
Now you can launch the VPC wizard.
That's cool, it does save you a lot of time,
but it means you won't know how to design a VPC
from scratch.
So we're not going to do that.
So to create or view our VPCs
what we need to do is we need to click over here
under Virtual Private Cloud where it says Your VPCs.
And we can see in here we've already got my default VPC
for Northern Virginia.
And so I can see that across
if I have a look across here
we can see we've got main route table, main network ACL.
The tenancy is the default tenancy
and we'll look at what that means in a second.
Is this the default VPC?
Yes. And then the owner ID.
So when we create our--when we have a default VPC,
we've already got a route table
and network ACLs associated with it.
We click into the Route Table.
We'll be able to see our routes table in here
and we'll be able to see all the different subnets
that are associated with our route
and you can see our different subnets in here.
We actually scroll up and go to Subnets.
It's here under Virtual Private Cloud
and we're going to click on Subnets in here.
And what you'll notice is every subnet
relates to one availability zone.
So you see in here, we've got different IP address ranges
and this subnet here for example, exists on us-east-1a.
Then we've got us-east-1b, us-east-1c,
us-east-1d etc.
So it's worth noting that a subnet
cannot span an availability zone,
a subnet always exists within one availability zone.
So that's worth knowing, going into your exam
because that can be a popular exam topic.
So let's start getting our hands dirty.
Let's go ahead and create our own custom VPC.
So we click on Your VPCs and you click on Create VPC
and then we're going to give our VPC a name in here.
So let's call it acloudguru-vpc.
In here is where we enter our CIDR address range.
So I'm going to do 10.0.0.0/
and then I'm going to use /16.
So the largest address range possible.
Now you could actually try and do /10.
And if you go ahead and hit Create VPC,
you're going to get error message.
And it says, hey, block sizes must be
between /16 and /28.
So, and that can be a popular topic that comes up.
It's certainly comes up in the network specialty exam.
So let's go ahead and choose a /16.
I'm not going to worry about IPv6
because it's beyond the scope of this course.
We do cover that off in the Certified
Network Specialty course.
Tenancy in here is whether or not we want dedicated hosts
that will cost you a fortune.
So let's just leave it as default.
And then we can have some tags in here.
I'm not going to to do tags
but I am just going to go ahead and create my VPC.
And you can see it's instantaneous.
So has created my VPC and you'll see over here
we've got our main route table
and we've got our main network ACLs.
So by just creating this VPC, we've created a route table
and we've created a network ACL automatically.
And let's just quickly check our security groups as well.
If we click in here, we'll have a look at our security
groups and we'll sort by our VPC ID.
You can see we've got our web DMZ security group leftover
from the EC2 section of the course.
We've got our default VPC security group over here
and that's for our default VPC.
But if we look up here, we've also got a new security group
and this is for our new VPC that we just created.
So when we create a VPC,
we're basically creating a route table,
a network ACL, and a security group.
Now, if we go back to our VPCs
and we actually go in and click in on our route table,
we'll be able to have a look at our subnet associations.
We'll be able to see that there are no subnets
associated with this VPC.
So we haven't created any subnets
when we created our VPC,
we've just created a route table and a network ACL.
Let's also have a quick look at our subnets.
So if you remember subnets
just basically a virtual firewall.
We click in here,
we can see we've got these different subnets
but it hasn't created any subnets.
It's only created a route table and a network ACL.
So the next thing we're going to do is go in
and provision some subnets.
So let's go in and create some subnets
and subnets are basically--we have public subnets
or private subnets.
A public subnet just means it's going to be
internet accessible.
A private subnet means that we're
going to put our database servers or something in there,
and it won't be directly internet accessible.
So let's go ahead and create a subnet.
First we start by selecting our VPC.
So you can see in here
we're going to choose the, acloudguru-vpc.
If we scroll down, we need to give the subnet a name.
Now what I like to do, first of all,
is just select an availability zone first.
So us-east- 1a,
and in here the name of my subnet
is going to be CIDR address range
and then the availability zone.
I just liked that as good naming convention
because it makes working with subnets a lot easier.
So I'm going to call this subnet 10.0.1.0.
So this is going to be my public subnet.
And then I'm going to make sure I know that this subnet
is in us and then east, and then it's 1a.
And so now I know my CIDR address range
and the availability zone
that this subnet is going to be provisioned in
just by looking at the subnet name.
In here, this is where we actually specify
the CIDR address range.
So we're going to do 10.0 and then .1.0
and then /24.
And that's going to give me a CIDR address range of 10.0.1.0
And I'm going to scroll down and that's it.
I'm going to go ahead and hit Create Subnet.
So that has now created this subnet.
It is already available and it exists within my VPC.
And when you have custom VPCs
you'll be able to see the name of the VPC in here
so acloudguru-vpc.
Notice the available IPv4 addresses is 251.
And if we go back to our website
that we're using in the previous lecture,
and if we type in that address range,
so 10.0.1.0 and then this was a /24.
You'll notice that it says that there's 256 available
IP addresses but in the VPC wizard,
it says that it's only 251.
Now, if we actually go to the AWS documentation
and you scroll down--and I'll leave a link to this
in the resources section.--
You'll see here it talks about the IP reservations.
So the reason we're missing 5
is that AWS reserved the first 4 IP addresses
and then the last IP address in each subnet CIDR block.
So the first one that they reserve is the network address
so .0 and then they reserved the .1 for the AWS VPC router.
And then they reserved .2, this is reserved by AWS
and it basically is the IP address of the DNS server.
And then they actually reserve .3 for future use.
So they don't actually use it, but they reserve it
just in case they need an extra IP address.
And then the 255 is the network broadcast address.
So they don't support a broadcast in a VPC
therefore they reserve this address.
So you always going to lose 5 IP addresses
from a traditional /24 network.
So that can be a popular exam question.
It used to be in the Solutions Architect Associate exam.
I don't know if it's still is
but it certainly is a question that will come up
in network specialty as well.
So just bear that in mind when you're sizing your VPCs.
Okay. So we created our first subnet.
Let's go ahead and create a second subnet.
So one is going to be public and one is going to be private.
The first one we'll have as public
and the second one we'll have as private.
I'm going to put our private one
in a separate availability zone.
So I'm going to put it into us-east-1b
and I'm going to call this 10.0.2.0 and then -
and then us, so us-east-1b1b.
And in here we actually specify the CIDR address range
so 10.0.2.0 and then /24
And now we're ready to go ahead and create that.
That has now created that subnet
and if just go back and click on my Subnets,
I'll be able to see these 2 new subnets in here.
Now to make your life easier,
you can sort via VPC so you can see it in there.
And again, have a look over here
we've got 251 available IP addresses
because we have lost those 5 reserved IP addresses.
Okay. So let's review what we've done so far.
So we went into us-east-1 and we provisioned a VPC
and we gave it a CIDR address range of 10.0.0.0/16.
As soon as we created that VPC, it created a route table,
a network ACL, and a security group.
And then we went in and we created 2 subnets:
a public subnet and a private subnet.
And really we've actually just given them IP addresses.
We haven't made either public or private yet
and that's what we're going to do in the next lecture.
So in the next lecture, what we're going to do
is we are going to assign an internet gateway.
We're going to add some more route tables, network ACLs,
and we're going to move some security groups
so that's there specifically within public subnets
and private subnets.
And then we'll have a fully working custom VPC
with instances in both public and private subnets.
So in terms of what we've learned so far,
just onto my exam tips, just think of a VPC
as a logical data center in AWS.
It consists of internet gateways
or virtual private gateways, route tables,
network access control lists, subnets, and security groups.
And one subnet is always anchored to one availability zone.
You can't have a subnet that spans
multiple availability zones.
So if you've got the time please come and join me
in the next lecture.
Thank you.


Demo : Provisioning a VPC - Part 2
==================================

Hello Cloud Gurus,
and welcome to this lecture.
In this lecture we're going to continue
on building our custom VPC.
Okay, so here I am in the AWS Management Console.
I'm going to go over to networking,
and I'm going to find VPC,
which is the first item under Networking.
Then I'm just going to go over to my VPCs,
and I'll be able to see the VPC we just created
in the last lecture in here.
And we can go ahead and click on the VPC ID
and have a look at all the different details.
Now there's a couple of things we need to do.
We need to assign an internet gateway to this VPC.
And we also--if we are going to go in
and make our subnets public and private,
we will need to auto assign IP addresses.
So if you have a look here
where we're going to make 10.0.1.0 as our public subnet.
If we scroll all the way over--
it can be a little bit difficult to see--but you see here
it says auto assign public IPv4 address.
Now we do want that to happen on our public subnet
because when we provision an EC2 instance into that subnet,
we want it to have an IPv4 public address.
So what we want to do is we just want to go back over
to our actions and hit Modify Auto-Assign IP Addresses.
And we just want to enable
auto-assign public IPv4 addresses
and that will make sure that any EC2 instance
that's put into 10.0.1.0
will automatically be assigned a public IP address,
so automatically be publicly available.
And you can see that option is now in there.
So the next thing we need to do is go
over to our internet gateways
and we're going to create an internet gateway.
We can call it myIGACG,
so a cloud guru or something like that.
So my internet gateway, a cloud guru,
and that's it really, all you have to do is hit Create.
It will create it straightaway.
And so if you click on your Internet Gateways
you'll see it in here,
but the state is automatically detached.
It's in a detached state.
So we need to go in and attach this to a VPC.
So all we do is click Attach to VPC,
select our VPC, and hit Attach Internet Gateway.
So that has now attached our internet gateway to our VPC.
Now you can only have one internet gateway per VPC.
Don't get confused on any scenario questions
where they're talking about increasing internet
throughput into your VPC
by adding multiple internet gateways,
it doesn't work like that,
you can only have one internet gateway per VPC.
Now, in order to get a route out to the internet,
we need to go ahead and have a look at our route tables.
Okay, so we can see our routes in here.
So we've got 2 route tables.
Both of these are main route tables,
and you can see that this one is associated
with the A Cloud Guru VPC.
Now, if we click in here,
we could go ahead and create a route out to the internet.
This route here basically just allows
all our subnets communicate to each other.
And if we actually click on our subnet associations,
we can see in here, it says the following subnets
have not been explicitly associated with any route tables
and are therefore associated with the main route table.
So that means every time you create a subnet,
it's going to be associated with the main route table.
And for that reason,
you probably don't want your main route table
to have a route out to the internet
because it means that every time you create a new subnet,
that subnet will automatically be internet addressable
or will have a route out to the internet.
And that's not really good security practice.
So what you want to do is create a new route table
and have that as the route out to the internet.
So let's go ahead and do that now.
We're going to create a route table,
and we'll just call it MyInternetRT,
something like that.
And I'm going to go ahead and hit Create.
And of course I have to select a VPC.
So let's click in there and hit Create.
And that has now created a new route table.
You can see it in here.
So it's my internet route.
And you can see it's no longer the main route table either.
So what we need to do now
is we need to go over to our routes.
And what we want to do is we want to go and hit Edit,
and we want to add a route.
And to add a route out to the internet,
all we do is go, 0.0.0.0/0,
the target is going to be to our internet gateway.
And then we select our internet gateway,
and then we go ahead and hit Save Routes.
And that has now created a route out to the internet.
So, 0.0.0.0/0,
and the target to the internet gateway.
So the next thing we want to do is associate a subnet
with this new route table.
So what we want to do is we want to go in
and hit Edit Subnet Associations,
and we're going to go in and add our subnets.
So I'm going to add this one,
which is a 10.0.1.0, so us-east-1.
And I'm going to go ahead and hit Save.
So that has now associated our subnet
to this route table
and this route table now has a route
out to the internet via our internet gateway.
And if we actually go back to our main route table,
and have a look at our subnet associations,
you'll see that it says the following subnets
have not been explicitly associated with any route tables
and they're associated with the main route table.
And it's just the 10.0.2.0.
So this will be our private subnet.
So the next thing we need to do is go ahead
and provision two EC2 instances.
We're going to put our EC2 instances,
one in the public subnet,
and then one in the private subnet.
And then we're going to have a look
at how we can connect into them.
So let's go over to EC2.
So we're going to go over to EC2 which is under Compute.
And I'm going to go in and click in here,
and I'm going to provision the first public instance.
So let's launch an instance, launch instance,
and we're just going to do the standard Amazon Linux 2 AMI.
And I'm going to go ahead and use a t2.micro.
And in here we're going to select our VPC.
So we're not going to use the default VPC.
We're going to use our custom VPC.
And in here we specify which subnet
that we want it to go into.
So I'm going to do it to us-east-1.
And you can see it actually tells me very helpfully
how many IP addresses are available in each subnet.
So I'm just going to leave everything else as default.
And we're going to go down and hit Next
so to add our storage,
and in here, I'm just going to leave everything as default.
I'm going to give this a name.
I'm going to call it WebServer01.
So name
and then here, WebServer01.
I'm going to go ahead and hit Next.
And we're going to have to create a new security group.
So the security group is going to be our WebDMZ,
because we don't have a WebDMZ in our new VPC,
we only have it in our default one.
And in here, I'm going to add a rule
and we're going to allow--
we're going to open up port 80 to this security group,
and we'll go ahead and hit Review and Launch
and Review and launch.
And I am going to just create a new key pair for this.
I'm going to call it MyCustomVPCKP.
So I'm going to go ahead and download that.
Then I'm going to go ahead and launch my instances
and that is now launching.
And while we wait for that instance to come up online,
let's go ahead and launch a second instance.
So we'll launch our instance.
We do an Amazon Linux 2 AMI,
I'm going to do t2.micro.
Again I'm going to choose my custom VPC,
and I'm going to launch this instance
into my private subnet.
Next I'm going to go ahead and add my storage.
Just going to leave everything as default.
I'm going to give this one a name,
and the name is going to be my DB server,
something like that.
So it's my database server.
Next we'll configure our security group.
I'm going to put it in our existing WebDMZ security group,
so that they'll be able to communicate straight away,
and then I'm going to go ahead and hit Launch.
And I'm going to use MyCustomKP that we created
in the last step.
So there we go.
Our instances are now launching.
So let's go ahead and view our instances.
So I'm just going to wait for the DB server
to come up online
and then we'll connect into these instances.
Okay. So both are up online and running.
So we've got our web server, our DB server,
and you can see that our web server
automatically has a public IP address,
it's automatically been assigned,
and that's because we changed that setting for that subnet.
So the next thing we need to do is go ahead
and what we're going to do is we're going to SSH into this.
I'm going to use a Mac OS terminal
but you can also just go in here and hit Connect
and you can use EC2 Instance Connect.
So I'm going to go ahead and SSH into this instance now.
So I'm in my terminal window now,
and I've just chmoded my custom key pair
so that it has the right permissions.
And then I'm just going to clear the screen.
I'm going to type ssh ec2-
user@
then I'm going to paste
in the public IP address in there,
then -i and then MyCustomerKP,
and go ahead and hit Enter.
And I'm going to go ahead and hit Yes.
And there we go.
I've now connected in to my web server.
So I'm going to elevate my privileges to route.
So sudo su.
I'm going to clear the screen.
And the first thing I'm going to do
is just run a yum update to make sure
that it has an internet connection out,
which it does,
it is now going ahead
and updating the kernel and all the operating systems.
So the web server is online, it's in my custom VPC,
and it has internet access.
So the next thing we're going to do is look
at how we can connect to our database server.
Now, when we created our database server,
we actually just put it in our WebDMZ security group.
It's probably not a good idea.
We probably want it to have it in its own security group.
So let's go ahead and set that up now.
So I'm back here in EC2,
and you can also get to security groups
in the EC2 section of AWS, as well as in the VPC section.
So in here, I'm going to go ahead
and create a new security group,
and I'm going to call this a security group myDBSG.
So my database security group,
and then I'll do the same for the description.
And in here, the VPC,
we want to create this in the acloudguru-vpc.
Now our inbound rules.
Well, essentially what we want to be able to do
is we want to be able to have our database servers
to be able to communicate to our web servers.
So we want to add in SSH,
we want to be able to SSH in
from our web server to our database server.
Now, in terms of our inbound rules,
we want our web server to be able
to communicate to our database server.
And we want to be able to communicate
over the MySQL port.
So in here, we can just specify the CIDR address range
so you can go 10.0.1.0, and then /24.
And you can even do it down to a specific IP address,
but I'm not going to do that.
I am going to copy this into my clipboard
to stop me typing it out every time.
And I'm going to add some more rules.
So let's add ping, so we'll be able to ping it.
So up here would just do all ICMP
and we'll go ahead and paste that in.
And then I'm going to go in
and add HTTP access as well, HTTP.
And then I'm just going to add just
for the purposes of this demonstration, SSH.
So I'll be able to SSH in from my web server
to my database server,
and it is purely just
for the purposes of this demonstration.
You probably don't want to do that in real life
because it means if your web server got compromised,
people could then SSH into your database server.
Let's go ahead and create our security group.
So that has now created our security group.
If we go back to our security groups,
we'll be able to see it in here.
So MyDBSG.
Next thing we need to do is go
and associate our database instance
with this new security group.
We would just want to go over to actions,
we'll go to security,
and we want to hit change security groups.
And then in here what we want to do
is we want to select our security groups,
so we would want to add it to my DB security group
and hit Add Security Group.
And then we'll just remove it
from our WebDMZ and go ahead and hit Save.
So this database server is now
under our database security group,
and we've configured it
so that our security groups can talk to each other.
So what I want to do is I'm just going to grab
the private IPv4 address, which is just in here.
So you can see it's 10.0.2.168
and I'm going to go ahead and copy that into my clipboard.
So I'm back in my terminal window.
Just going to clear the screen.
Let's go ahead and type in ping
and then we're going to ping that over
to our internal IP address.
And you can see,
we are getting a response from that EC2 instance.
So very last thing I want to do is just
for the purposes of this demonstration show you
that that EC2 instance is in a private security group.
So we're going to SSH over to it.
Before we do that,
I need to create a key pair on this EC2 instance.
So I'm just going to nano,
and then call it mycustomKP.pm.
And I've just opened up my key pair in a text editor
and I'm going back over to my terminal window
and I'm just going to paste it in here.
Then I'm going to save it.
So just type, yes, enter.
Going to clear the screen now,
and then I'd just need to chmod this again.
So mycustomKP.pm.
And so now I should be able to SSH
into my instance in the other subnets.
So let's go ahead and type ssh ec2-user@
and then it's the internal IP address.
And then we do it -i,
and then mycustomKP.pm.
Go ahead and type in yes.
And there we go,
I am now SSHed into my EC2 instance
that's in my private subnet.
So let's go ahead and clear the screen.
I'm going to go ahead and elevate my privileges to root.
And then what I'm going to do
is I'm going to run a yum update and let's see what happens.
Now it's going to pause here for quite a bit,
but eventually it's just going to time out.
And we can see that that yum update has failed.
And the reason for that
is there's no route out to the internet.
So this database server is completely isolated
from the internet.
Now that's a good thing and a bad thing.
I mean, it's a bad thing
because we can't update and apply our security patches.
So in the next lecture, what we're going to do
is go ahead and have a look at how we can maintain security
but still have this database server be able
to run a yum update and communicate out to the internet,
because we are going to need to install things
like MySQL on this database server anyway.
So let's just summarize what we've learned
in these last 2 lessons.
So what we've learned is just think
of a VPC as a logical data center in AWS.
It consists of internet gateways
or virtual private gateways,
route tables, network access control lists, subnets,
and security groups.
And one subnet is always in one availability zone.
And that is to say that a subnet cannot span
across multiple availability zones.
So just to remind you what we just did,
we basically built a custom VPC in us-east-1.
As soon as we created our VPC, it created a route table
a network ACL, and a default security group.
We then added in a public subnet and a private subnet.
We then attached to an internet gateway
and then we went in and we added some more route tables,
another security group,
and then we deployed some instances in our public subnet
and our private subnet,
and then we SSHed into our public subnet,
and then we used a connection
to SSH into our private subnet from our public subnet.
So we've done an awful lot.
The issue we have right now is, like I said,
the instance in our private subnet has no way
of installing software or doing updates.
So what we're going to look at in the next lesson
is how we can resolve this.
So if you've got the time,
please join me in the next lecture.
Thank you.


Using NAT Gateways for Internet Access
=======================================

Hello Cloud Gurus,
and welcome to this lecture.
In this lecture we're going to look at how we can use
NAT gateway to get internet access
for our instances that are in a private subnet.
So start with what is a NAT gateway,
we'll then have a look at a network diagram of how it works.
We'll then go onto my NAT gateway tips
and then I'll show you how to set it up in a console
and we'll move on to my exam tips.
So what is a NAT gateway?
Well, you can use Network Address Translation
or NAT gateways to enable instances in a private subnet
to connect to the internet or other AWS services
while preventing the internet from initiating
a connection with those instances.
And that's exactly what we want to do
with our database server that is in the private subnet
from the last lecture.
So this was a summary of our environment
that we created in the last lecture.
And let's have a look at what happens when an instance
in our public subnet tries and does a yum update.
Well, essentially it's sending traffic
through the network ACL through the route table.
The route table has a route out to the internet
and it points to the internet gateway,
and it can then get that update.
Now, when our instance in the private subnet runs yum
update, it's going out through the network ACL
and then it hits the route table,
but there's no route out to the public internet.
So that's why it can't speak to anything on the internet,
but likewise, the internet can't go in
and connect directly to our instances in our private subnet.
So what is the solution?
Well, the solution is to basically provision a NAT gateway
and we provision that NAT gateway in the public subnet
and remember one subnet always
equals one availability zone.
You can't have subnets that span across availability zones.
So the NAT gateway is going to be
in the same availability zone as your public subnet.
And the NAT gateway is basically redundant.
It's more than one EC2 instance.
It's basically a collection of EC2 instances
that Amazon basically handled for you behind the scenes.
So you don't--can't log into the NAT gateway, for example,
the NAT gateway acts as a means for internet traffic
to go out through the NAT gateway, to the network ACL
to the route table,
which then goes to the internet gateway.
So it's a way we can get our instances
in our private subnets, internet access
without the internet being able to directly
access our instances in our private subnet.
So let's have a look at some facts
to remember about NAT gateway.
So like I said, it's redundant inside the availability zone.
They start at 5 gigabits per second throughput
and they scale up to 45 gigabits per second.
You don't need to patch NAT gateways;
Amazon handle it all for you.
And NAT gateways are not associated with security groups.
You don't need to worry about opening up a port
on your security group to on NAT gateway
and they're automatically assigned a public IP address.
So that's not something you have to worry about either.
So let's go ahead and provision our NAT gateway.
So just a reminder from the last lecture
this is where we've logged in to our EC2 instance
in our private subnet.
We've run a yum update and it can't find a valid baseline
URL that's because it has no internet access.
So this is the problem we're trying to solve.
So I'm logged into the AWS console
and I'm in the VPC section in Northern Virginia.
And what I want to do is I want to go over
to my NAT gateways,
which is here under virtual private cloud,
and what we're going to do--
it says no NAT gateways found.
Well let's go ahead and create a NAT gateway.
So the name I'm just going to call it
my-nat-gateway seems sensible.
And in here the subnet. Now, the subnet obviously
you have to put this into a public subnet,
so we're going to choose this subnet
and this is why I like this naming convention
because I can see exactly which availability zone it's in
and which subnet the IP address range
or the CIDR address range, so
I can identify straight away that this is a public subnet.
And here we've got our Elastic allocation ID.
So assign an Elastic IP address.
You can just click in there and go allocate Elastic IP
and that will now have an Elastic IP address
and then we're ready to go.
So let's go ahead and create our NAT gateway.
So it does take a little bit of time
to create a NAT gateway, can take up to 5 or 10 minutes.
What you do is you can just click on your NAT gateway
in here and you'll see the state is pending.
So I'm just going to pause the video
and wait for this NAT gateway to come up online.
Okay, so this instance is now changed at state to available.
We can see here it's got an Elastic IP address,
a private IP address,
and we can see that in our A Cloud Guru VPC
and we can see that it's in our public subnet.
Now just provisioning a NAT gateway is not enough.
What we need to do is we need to go into our route to table
and make a route out to the internet.
So let's click on Route Tables.
Now, if you remember, we had our internet route table.
So this is our way out to the internet
but it's not our main route table.
And then we have this one, which is our main route table.
So we go in here
and we're going to go in and edit our routes
and we're going to go ahead and add a route.
And we're going to say,
hey anything in the main route table
if you need a pathway out to the internet,
then the target is,
and then we're going to go over to our NAT gateway
and then we'll be able to select our NAT gateway in here.
And then all we have to do is go ahead and hit Save
and that gives us a route out to the internet
now via our NAT gateway.
So let's go ahead and test this.
Okay, so I'm back in my EC2 instance
I'm going to go ahead and clear the screen
and then I'm just going to run a yum update
and then minus yes
and there we go it is now updating.
So we have a route out to the internet
and we can now go in and patch this server.
We can also go in
and install mySQL and any other tools that we want.
So onto my exam tips.
Just remember that NAT gateways are redundant
inside the availability zone.
They start at 5 gigabits per second and
is currently scaled to 45 gigabits per second.
You don't need to go in and run yum updates on NAT gateways.
You don't need to patch them. Amazon dos it for you
and they're not associated with any security groups.
So we didn't have to go in
and open up any ports on our security groups.
We didn't have to attach them to a security group
and it's automatically assigned a public IP address
when we provision it.
So that is it for this lecture everyone.
If you have any questions, please let me know. If not,
feel free to move on to the next lecture. Thank you.


Protecting Your Resources with Security Groups
===============================================

Hello Cloud Gurus,
and welcome to this lecture.
So this is really just a refresher lecture
on security groups and what they are and how they work.
And the reason we're teaching you about this
is because we're about to cover off network ACLs
and you need to know the difference between security groups
and network ACLs.
But I appreciate you pretty much have learned
all of this stuff in the EC2 section of the course,
we're just going to review it.
So we'll do a review of security groups,
we'll have a look at a quick network diagram,
talk about what security groups are,
and then most importantly, we'll go on to my exam tips.
So if you remember in the EC2 section of the course,
we started exploring security groups
and I love to use this analogy about basic human senses.
So we see using light, we basically use our eyes
to detect light, with sound, we use our ears
to detect sound, and then with heat,
we can actually feel heat using our skin.
So computers communicate to each other
over certain protocols and a protocol is dedicated
to a particular port.
So for example, if we want to communicate over using
the SSH protocol, we're going to use port 22.
RDP is a way of administrating Windows remotely
and that works over port 3389.
HTTP is on port 80, HTTPS is on port 443.
So in terms of where security groups fit
within our network diagram, we've got our internet gateway
and then if we have an internet connection coming in,
it's going to go through our route table.
It's going to hit our network ACL first
and then it's going to hit our security group.
And likewise, if we've got a virtual private gateway
so we've got a VPN connection and we're trying to connect
into our private subnet,
it's going to go through out route table.
It's going to hit the network ACL first
and then it will hit the security group.
So the security groups are essentially
the last line of defense.
So in terms of troubleshooting
any internet connectivity issues,
always start with route tables.
Do you actually have a route in or out to the internet?
Then look at network ACLs
and then finally check the security groups.
And it's generally going to be one of those 3 things
that is causing you issues.
So that's the methodology you should use in real life
as well as in the exam.
So security groups are basically just virtual firewalls
for an EC2 instance and by default, everything is blocked
and to let everything in, you just basically open up,
0.0.0.0/0, so you'll use
that CIDR address range and then you just select
the protocol that you want.
So it could be SSH, RDP, HTTP, etc.
and we've been doing it all the way through this course.
Now onto the real tip and the whole point of this lecture
is about statefulness, so security groups are stateful.
If you send a request from your instance,
the response traffic for that request
is allowed to flow back in
regardless of the inbound security group rules.
So essentially if you are communicating over port 80
and port 80 is open on your security group,
it's going to allow communication back and forth.
So it means that they're stateful.
So in other words responses to allowed inbound traffic
are allowed to flow out regardless of the outbound rules.
If you have an inbound rule on port 80
and you're opening it up to the world, and then you try
and create an outbound rule blocking port 80, it won't work.
So security groups are stateful.
And this is key to know going into your exam
because network ACLs are stateless.
And we're going to have a look at that in the next lecture.
So if you've got the time please join me
in the next lecture.


Controlling Subnet Traffic with Network ACLs
===============================================

Hello Cloud Gurus,
and welcome to this lecture.
In this lecture, we're going to look at Controlling
Subnet Traffic with Network ACLs.
So, we're going to look at what a network ACL is.
We'll then look at a network diagram to remind us
of what our VPC environment looks like.
We'll then do a network ACL overview
and I'll give you some tips about network ACLs.
We'll then go into the console and I'll show you
how the evaluation logic works with the network ACLs,
and then we will move on to my exam tips.
So, what is a network ACL?
Well, basically it's your first line of defense.
So, network ACL is an optional layer of security
for your VPC, basically acts as a firewall
for controlling traffic in and out of one or more subnets.
And you might set up a network ACL
with rules similar to your security groups
in order to add another layer of security to your VPC.
So, we provisioned our VPC in us-east-1
and in terms of the CIDR address range, we chose 10.0.0.0.
We then created our VPC
and we attached our internet gateway.
And then we set up our public subnets
with a security group and we put it in web server in there.
And then we also set up our private subnets,
so we put our database server in there.
Now, in terms of the actual traffic flow,
traffic comes through the internet gateway, hits our router,
and then we route based on the rules in our route table,
it then goes through a network ACL,
so that's why we say it's the first line of defense.
And then the traffic goes through the network ACL
to the subnet that's associated to that network ACL.
So, in this case it will be our public subnet
and then it will go through our security group,
and if it's still allowed,
the traffic will go to our EC2 instance.
So, the thing to take away from this is always to remember
that your network ACLs are your first line of defense.
So, your VPC automatically comes with a default network ACL,
and by default it allows all inbound and outbound traffic.
Now, you can create custom network ACLs.
But by default, each custom network ACL
is going to deny all inbound and outbound traffic
until you add rules.
And we'll have a look at that in the demo in a little bit.
And each subnet in your VPC
must be associated with a network ACL.
And if you don't specifically associate a subnet
with a network ACL, then what's going to happen
is the subnet is automatically associated
with your default network ACL.
And then finally, moving on to blocking IP addresses.
So, you can block IP addresses using network ACLs,
not security groups.
And that is a very popular exam topic
that will come up time and time again.
It'll basically be saying,
you need to block a specific IP address from this,
you know, attacker.
How do you do it?
Well, you use network ACLs, not security groups.
So, network ACLs,
you can associate a network ACL with multiple subnets.
However, a subnet itself can only be associated
with one network ACL at a time.
And when you associated network ACL with a subnet,
the previous association is removed.
And network ACLs contain a numbered list of rules
that are evaluated in order,
starting with the lowest numbered rule,
and we'll have a look at how that works in our demo.
And then network ACLs also have a separate inbound
and outbound rules, and each rule can
either allow or deny traffic.
And that really brings us on to this fact,
that network ACLs are stateless.
So, responses to allow inbound traffic
are subject to rules for outbound traffic, and vice versa.
So basically, you have to go in and allow rules
for both your inbound traffic and your outbound traffic.
You don't have to do that with security groups
because they're stateful.
So again, you might get this in an interview question,
you know, which is stateless and which is stateful.
Well network ACLs are stateless,
whereas security groups are stateful.
Okay, so here I am in the AWS console
and just going to go over to EC2 under Compute.
And you'll see that I've got 2 instances running
from one of the last lectures. I've got my WebServer
and MyDBServer.
Now, the problem with my WebServer
is I do have a public IP address
but I haven't actually gone in and installed
Apache or created a webpage.
So, I'm just going to quickly do that now.
Okay, so I'm logged into my web server.
I'm just going to do yum install and then httpd,
so I'm installing Apache, and just going to hit yes.
And now, I'm just going to say a sudo service httpd
and then start, that's going to basically start
the Apache service.
Then I'm going to go over to my /var/www/html directory.
Just type in ls. You can see
that there's nothing in there.
I'm just going to quickly elevate my privileges to root
so I can create a webpage.
Then I'm just going to type in echo
and then, it's going to be &lt;html&gt;&lt;body&gt;
and then &lt;h1&gt;, and it would just say,
Hello Cloud Gurus.
Then we're going to end our header,
and then we're going to end our body,
and then we're going to end our html.
And we're going to output this to index.html.
Oops, fix that up, html.
And then if we type in ls, we can see that file
has now been created.
And if we cat the file, we'll be able to see
that it says, Hello Cloud Gurus.
So, that's very, very simple.
Now, I'm going to go back over to the AWS Console.
Okay, and you can see if I just go to my public IP address
that webpage is now up and live.
So, now I'm back in the AWS Console
and we just go back over to our VPC.
So, if we go down to Network Content and Delivery.
And in here, we've got our VPCs.
And what we're going to do is we're going to go ahead
and we're going to create a network ACL,
which is under Security.
So, we can see our network ACLs in here.
And this is our default network ACLs,
which is associated with 2 subnets.
And if I clicked in here,
I'll be able to see my inbound and outbound rules.
You can click up here to make it a bit bigger.
So, we click on our Inbound Rules.
You can see here, we've got rule number 100
and it's basically allowing everything in.
And then we've got a deny rule after that.
And we have the exact same thing.
So by default, when we create our VPC,
we create a network ACL automatically,
which allows all traffic in and out.
Let's go ahead and create a new network ACL.
I'm going to call up MyWebNACL,
and the VPC we're going to create it
in is our acloudguru-vpc, and that's it.
Let's go ahead and hit Create.
Okay, so that has now created successfully.
If we click in here and have a look at the inbound rules,
you'll be able to see it's denied everything by default,
both inbound and outbound.
So, it has denied everything by default.
And in here, we have no subnets associated with it.
If we hit Edit Subnet Association,
what we can do is we can go in
and associate our public subnet with this network ACL.
So, I'm going to go ahead and hit Save,
and that has now updated the subnet association.
And so we click in here
and have a look at our subnet associations,
you can see it as now updated
and associated our public subnet with this network ACL.
Now, the problem we're going to have,
is that now there's no route in for port 80.
So, our web servers should stop working.
Okay, I pause the video and you can see
now that the site can't be reached,
and that's because we have no pathway
through our network ACL for port 80.
So, let's go ahead and figure out how we can rectify this.
Okay, so we're back in the console
and we're under network ACLs
and we can see our network ACL in here.
And we can go in and have a look at our inbound rules.
So, let's go in and edit these inbound rules.
We're going to add a new rule.
And best practice is you always start with 100
and then you use increments of 100.
That's what Amazon always recommend.
So in here, I'm going to go and select HTTP.
And in here, I'm just going to leave my source as
everything and I'm going to say Allow.
And then we could do the same for SSH.
So, we could type in 200
and then go down to SSH and hit Allow.
We're going to go ahead and save that change.
And then once we have a look at our rules,
we can now see our rule numbers,
so we've got 100 and 200.
And the evaluation logic for this
is rule number 100 is done first, then rule number 200,
and then with the wildcard at the end.
So, you can keep adding rules
and we'll have a look at what happens
when rules conflict in just one second.
So, those are our inbound rules.
Now, we learned in a little bit earlier
on that network ACLs are stateless.
So, it's not good enough just adding our inbound rules.
We also have to add some outbound rules
and we can see in here,
that we don't have any outbound rules right now.
So, let's go ahead and hit Edit
and we're going to add a new rule.
And we're going to add rule 100.
So, we're just going to add in here port 80,
so as rule 100
and we're going to go ahead and add a new rule.
We're going to add a rule for 200
and we're going to add in SSH.
And then we need to go in and add our ephemeral ports.
And I'll show you how this works in one second
but essentially, you just do it as rule number 300.
And then we put our port range as 1024,
and we go up to 65535, and let's go ahead and hit Save.
And that has now saved it.
So, if you go over to the Amazon guide about network ACLs
and click on Ephemeral Ports,
you can read in here about ephemeral ports
and how they work, and what the suggested
ephemeral ports are.
You go down to the bottom it says,
in practice to cover the different use of clients
that might initiate traffic to public
facing instances in your VPC,
you can open up ephemeral ports 1024 to 65535.
That's essentially what we've done.
The way ephemeral ports basically work
is a client sends a request on let's say, port 80,
the servers then going to choose
a random report to reply on.
And that random port is only going to be open
for the temporary length of the session.
And I added a link to the AWS documentation
in the resources section, as well as a link to Wikipedia
just explaining ephemeral ports in a bit more detail.
So, now that we've done that,
let's go ahead and refresh our website.
And you can see straight away it works.
Hello Cloud Gurus.
So, we're allowing port 80 in and out
through our network ACL.
So, I'm back in my network ACLs.
Let's go in and do a bit of experimenting here.
So, my inbound rules.
I'm going to create a rule at number 300.
And what I'm going to do is I'm going to deny
access to port 80.
So, 300 in here, we're going to go to HTTP,
so down here, and then we're going to hit Deny.
So, what do you think will happen now?
We've got rule number 100, 200 and 300.
So, this is allowing in traffic on port 80.
It's allowing in traffic on SSH.
And then it's denying traffic on port 80.
How do you think the evaluation logic
of this is going to work?
Let's go back to our web page.
So, here's our webpage here.
And if I hit refresh, nothing happens.
So, we are not getting blocked on that rule.
And that's because the rules are evaluated
in chronological order.
And my allow has come before my deny.
Let's go in and have one more look at what else we can do.
So back here, if we go to Edit our inbound rules.
Let's go in and remove this,
and we'll add a new rule and we'll call this rule 50.
And what we're going to do
is we're going to do this on port 80, so HTTP,
and we're going to go ahead and hit Deny,
and let's go ahead and hit Save Changes.
Now, you'll be able to see on our inbound rules here,
we've got a Deny and then an Allow.
Let's go over to our web page.
You can see our webpage in here.
If I hit Refresh,
you can see it says waiting, waiting, waiting.
I'm just going to pause the video
and you can see that my deny is now overriding my allow
because it's evaluated first.
So, I've just automatically been denied
and it's overridden the allow.
So, let's go ahead and remove that.
I'm just going to go edit my rules
and then I'm just going to remove that in here.
And the very last thing I want to show you
is another deny that we can do.
So, if you want to block a specific IP address,
you can definitely do it
but you need to make sure the rule number is before
where you're allowing all your traffic to come in.
So, we could block, let's say we create rule number 99
and we want to block port 80
to this person who's being malicious towards our website.
So, let's say it's 23.24.25.26,
and then you do /32.
So, that's how you block a specific IP address.
This is a scenario based question
that will come up time and time again
in your exam.
You use network ACLs to block specific IP addresses,
not security groups.
So, let's go onto my exam tips.
So, onto my exam tips.
So, default network ACLs, your VPC automatically comes
with the default network ACL,
and by default it allows all outbound and inbound traffic.
But when we create a custom network ACL,
by default, it denies all inbound and outbound traffic.
And we just saw that in the demo.
And then you need to go in and add your own rules.
In terms of subnet associations,
each subnet in your VPC must be associated
with a network ACL.
If you don't explicitly associate a subnet
with a network ACL, then the subnet
is automatically associated with the default network ACL.
And then blocking IP addresses, as we just saw
you can block IP addresses using network ACLs,
not security groups.
Remember, that you can associate a network ACL
with multiple subnets.
However, subnet can be associated
with only one network ACL at a time.
And when you associate a network ACL with a subnet,
the previous association is removed
and we saw that happen with our public subnet.
Network ACLs contain a numbered list of rules
that are evaluated in order,
starting with the lowest numbered rule first.
And network ACLs have separate inbound and outbound rules,
and each rule can basically,
either allow or deny traffic.
And then finally, network ACLs are stateless
and responses to allowed inbound rules
are subject to rules for outbound traffic, and vice versa.
So, that is it for network ACLs.
If you have any questions, please let me know.
If not, feel free to move on to the next lecture.
Thank you.


Private Communication Using VPC Endpoints
=========================================

Hello Cloud Gurus,
and welcome to this lecture.
In this lecture we're going to look at private communication
using VPC endpoints.
So we're going to look at what VPC endpoints are,
the different VPC endpoint types.
We're going to show VPC endpoints in action
and then I'm going to show you how to set this
up in the console.
And then we will go on to my exam tips.
So what our VPC endpoints?
Well, they basically enable you
to privately connect your VPC to supported AWS services
and a VPC endpoint service is powered by PrivateLink
without requiring an internet gateway,
NAT device, VPN connection, or an AWS Direct connection.
So essentially they're kind of like a NAT gateway,
except instead of it traversing outside the external
internet, it's going via a VPC endpoint.
So it's going by Amazon's backbone network.
So it's not leaving the AWS environment.
So instances in your VPC
do not require public IP addresses
to communicate with resources in the servers.
So traffic between your VPC
and the other service does not leave the Amazon network.
That's what VPC endpoints essentially are.
So think of them just like NAT gateways
except it's a way of internally communicating
with other AWS services.
So endpoints are virtual devices.
They are horizontally scaled,
they are redundant,
and highly available VPC components
that allow communication between instances in your VPC
and services without imposing availability risk
or bandwidth constraints on your network traffic.
And I guess what was--
if you remember from that NAT gateway lecture,
NAT gateways can have a maximum amount of bandwidth.
And if you've got these EC2 instances
that are writing to S3 for example,
you pretty much don't want that to go over your NAT gateway
because it's going to affect
the bandwidth requirements on the NAT gateway.
You'd rather it go through a virtual endpoint
and that's where we come up with VPC endpoints.
So there are 2 different types of endpoints.
You've got your interface endpoints
and an interface endpoint is basically
an elastic network interface with a private IP address.
And that serves as an entry point for the traffic
headed to a supported AWS service
and they support a large number of AWS services.
And then we have our gateway endpoints.
And these are very similar to NAT gateways.
A gateway endpoint is basically a virtual device
that you provision and that supports a connection
to currently S3 and DynamoDB.
So let's have a look at a network diagram
and this is basically a VPC endpoint in action.
So if you remember from some previous lectures,
we still have that database server
that's sitting in our private subnet,
10.0.2.0.
And it does have an internet gateway out
but now if we want it to communicate directly to S3,
then what we'll need to do is go in
and provision a VPC endpoint.
And this will be an S3 VPC endpoint
and that will give us a direct connection to S3.
And then we don't have to traverse the NAT gateway
and go out via the internet.
So let's go ahead and have a look at this in action.
Okay, so here I am in the AWS console.
The first thing I actually need to do,
is I need to go over to EC2
and we're going to go and get our database instance
and we're going to apply the S3 role to it
that we created in the roles section of the course.
So to do this, we just go over to actions,
we go over to security group and we go Modify IAM role.
And then we're just going to choose our role in here.
So it's S3 full access,
which we created in the EC2 section of the course.
Let's go ahead and hit Save.
And that has now assigned the role to our database server.
So let's go back over to services.
We're going to go to VPC,
which has always is under network content and delivery.
And in here, we're going to go in
and we're going to go to our VPCs.
And what we're going to do,
is we're going to go ahead and provision our gateway.
So you'll find VPC endpoints under Endpoints just over here.
So go ahead and click on Endpoints.
Beause we haven't created one before, it's empty.
So let's go ahead and create one.
And now you can filter by the AWS service.
So if we just type in S3 in here,
you see that we get the option for both a gateway
and an interface.
The same would exist for DynamoDB as well.
But if you wanted another AWS service
where gateways aren't supported
then you'd have to select interface.
So let's go ahead and select our gateway.
In here it shows us our different routes tables
and different VPCs.
So we want to put this in our VPC,
our custom VPC that we built.
And then what we want to do
is we want to configure our route tables
and it's going to put a rule in there for us automatically.
Now of course, we want to put this in the route table
that's not public.
So 10.0.2.0.
So let's go ahead and put that in there.
And then we're able to create our endpoint.
So that is now creating our endpoint.
It says the endpoint is created.
Let's go ahead and have a look.
If we just hit Refresh in here,
it is automatically available straight away.
So it was much faster than our NAT gateways,
and this gateway endpoint
will allow us to communicate to S3.
So what I'm going to do
is I'm going to SSH over to my database instance
and I'm going to run aws s3 ls.
Okay, so here I am, in that instance,
I'm just going to clear the screen.
And then I'm going to type in aws s3 ls.
Now this instance has a role attached to it,
which should allow us to talk to S3.
And if it does, there you go.
You can see my three S3 buckets.
So this is traversing a VPC endpoint.
It's an S3 gateway and it is not traversing the internet.
So onto my exam tips.
Just remember the use cases for VPC endpoints.
This is when you want connect AWS services
without leaving the Amazon internal network.
There's 2 types of VPC endpoints available,
interface end points and gateway end points,
and gateway endpoints support S3
and DynamoDB as of the time of recording.
So that's it for this lecture everyone,
if you have any questions, please let me know,
if not, feel free to move on to the next lecture.
Thank you.


Network Privacy with AWS PrivateLink
====================================

Hello Cloud Gurus, and welcome to this lecture.
In this lecture we're going to look at network privacy
with AWS PrivateLink.
So we're going to look at opening up our services
in a VPC to another VPC.
I realize that we covered that off in VPC peering
but we'll just remind you of that,
then we'll look at how we can share applications across VPCs
and then we'll look at how we can use PrivateLink
and then we'll go onto my exam tips.
So as we learned in the last lesson,
you can open up your services in a VPC to another VPC
and you can do this in really 2 ways.
You can do this by opening the VPC up to the internet
or you can use VPC peering.
So it's possible to have basically have
your resources publicly accessible or use VPC peering
to share your services in a VPC to another VPC.
So to open our applications up to other VPCs
we can either open the VPC up to the internet
but this comes with different security considerations.
So everything in the public subnet is going to be public.
And it also means there's a lot more to manage
because we have to manage internet gateways,
route tables, etc.
We can also use VPC peering and you have to create
and manage many different peering relationships,
especially if you think about scaling this VPC.
Let's say you wanted to scale it up
so that you were sharing the resources in this VPC
to a hundred different VPCs.
Yes, you can use VPC peering
but you're going to have to go in and create and manage
many different peering relationships.
And don't forget the whole network will be accessible
and this wouldn't be good if you have multiple applications
within your VPC.
So the best way to expose a service VPC to tens, hundreds
or thousands of customers is by using PrivateLink
and that's what it was designed to do.
It doesn't require VPC peering, there's no route tables,
NAT gateways, internet gateways, etc.
But what it does is it requires a Network Load Balancer
on the service VPC and then you need
an Elastic Network Interface or ENI on the customer VPC.
So this is a way of scaling up, basically,
it's like VPC peering,
except you're doing it at scale
and you're not actually creating a peering relationship,
you're using PrivateLink.
So it's just a way to open up your VPC to, you know,
hundreds or thousands of customer VPCs.
So that's all it is.
So going into the exam, just remember what PrivateLink is.
If you see a question asking about peering VPCs
to tens, hundreds, or thousands of customers VPCs
then I want you to think of that AWS PrivateLink.
Remember it doesn't require VPC peering,
there's no route tables, NAT gateways
or internet gateways et cetera,
but it does require a Network Load Balancer
in the service VPC and Elastic Network Interface
on the customer VPC.
So that's it for this lecture everyone.
If you have any questions, please let me know.
If not, feel free to move on to the next lecture. Thank you.


Building Solutions across VPCs with Peering
============================================

Hello Cloud Gurus,
and welcome to this lecture.
In this lecture we're going to look at VPC peering.
So, we're going to look at why we might need multiple VPCs.
We'll look at VPC peering and how it works.
We'll talk about transitive peering.
I'll give you a demo of how to do this in the console,
it's super easy,
and then we will move on to my exam tips.
So, sometimes you might have to have multiple VPCs
for different environments,
and it may be necessary to connect these VPCs to each other.
So, we could have our production Web VPC,
so this is our external website,
so it's maybe an online store.
And then we have our Content VPC,
this is where we upload all our product information to,
and then we have our internal Intranet
that we use to do all the backend stuff for sales.
And then it might be necessary
for all of these 3 different VPCs
to be able to talk to each other.
And that's all VPC peering is,
it allows you to connect one VPC with another,
via direct network route using private IP addresses.
And instances behave
as if they were on the same private network.
And you peer VPCs with other AWS accounts
as well as other VPCs in the same account.
And peering is done in a star configuration.
So you always have one central VPC with 4 others.
You can't do transitive peering.
And if that doesn't make sense,
I'll show you on the next slide.
The other thing you just need to know
is you can peer between regions.
So, this is what I was trying to explain before
with transitive peering.
So, if you've got VPC A,
it's connected to VPC B, C, D, and E.
But let's say we need VPC B to be able to talk to VPC C,
it can't do it through VPC A,
we need to establish a direct peering connection
between VPC B and VPC C.
So, that's all transitive peering is,
we basically can't let VPC B talk to VPC C
through VPC A.
Transitive peering is not supported with VPCs,
so you just need to create a new connection
between VPC B and VPC C.
So let's go ahead and have a look
at how we can create a peering connection
in the AWS Console.
Okay, so here I am on my VPC Dashboard
and if we scroll down, you'll see Peering Connections
right at the bottom, it's above the Security.
And what we're going to do is go ahead
and create a peering connection,
and I'm just going to call it MyPeer, something like that.
And here it says, select a local VPC to peer with.
So, let's start with our acloudguru-vpc.
You can see our CIDR address range in here
and select another VPC to peer with,
so in here, we're going to select our default VPC,
and you can see that the CIDR address range
is separate to this.
If we had the same CIDR address range,
this peering would not work,
you can't have overlapping CIDR address ranges.
That's the only thing you need to remember.
So go ahead and hit Create,
and that's it. It really is that simple.
So that will go in and create a peering connection
between the 2.
You see here it says Pending Acceptance.
All we need to do is go to actions and hit Accept Request,
and then we go ahead and hit Yes.
And then that has now an active peer
between our default VPC and our new VPC.
So, onto my exam tips.
Just remember what VPC peering is,
it allows you to connect one VPC with another
via a direct network route using private IP addresses.
Transitive peering is not supported,
it must always be in a hub-and-spoke model.
You can peer between regions
and you can't have overlapping CIDR address ranges.
So that's it for this lecture everyone,
if you have any questions, please let me know.
If not, feel free to move on to the next lecture.
Thank you!


Securing Your Network with VPN CloudHub
========================================

Hello Cloud Gurus,
and welcome to this very quick lecture
on securing your network with VPN CloudHub.
So we'll talk about what VPN CloudHub is,
we'll look at a network diagram as to how it works
and then we'll go on to my exam tips.
So VPN CloudHub is really useful
if you have multiple sites each with its own VPN connection
and you can use AWS VPN CloudHub
to connect those sites together.
So it works on a hub and spoke model just like VPC peering.
It's low cost and very easy to manage
and it operates over the public internet
but all traffic between the customer gateway
and the AWS VPN CloudHub is encrypted.
So this is how it works.
Let's say we've got a VPC
and we've got 2 different subnets
in 2 different availability zones,
and we've got a virtual private gateway
for our VPN connections.
And then we've got our customer
and our customer has offices in New York,
in Los Angeles, in Miami.
If they're using VPN CloudHub,
essentially what it does
is it aggregates the VPN connections
so the customers in New York can use that VPN connection
to talk directly to Miami using VPN CloudHub.
So it just works on a hub and spoke model
very similar to VPC peering.
So with Amazon VPN CloudHub, if you have multiple sites
each with its own VPN connection,
you can use VPN CloudHub to connect those sites together.
Like I said, very similar to VPC peering
in that it works on a hub and spoke model.
And AWS VPN CloudHub is low cost and easy to manage.
Though it operates over the public internet,
all traffic between the customer gateway
and AWS VPN CloudHub is encrypted.
So if you see a scenario based question
where they're literally just talking about
a way of aggregating your VPN connections
from all your different customer sites
all over the world,
I want you to think of AWS VPN CloudHub.
So that is it for this lecture everyone.
If you have any questions, please let me know.
If not, feel free to move on to the next lecture.
Thank you.


Connecting On-Premise with Direct Connect
===========================================

Hello Cloud Gurus,
and welcome to this lecture.
In this lecture, we're going to talk
about Direct Connect and how we can use Direct Connect
to connect our on-premise data centers directly into AWS.
So we'll basically explore what Direct Connect is,
we'll then look at private connectivity.
We'll look at the 2 different types of connection
that you can get with Direct Connect.
We'll look at how Direct Connect actually works.
We'll compare VPNs to Direct Connect
and then I will give you my exam tips.
So what is Direct Connect?
Well, basically it's a cloud service solution
that makes it easy to establish
a dedicated network connection from your premises to AWS.
So that's all it is,
is a dedicated network connection
from your data center into AWS.
So in terms of private connectivity,
using AWS Direct Connect,
you can establish private connectivity
between AWS and your data center or office.
And in many cases, you can actually reduce
your network costs, increase the bandwidth throughput
and provide a more consistent network experience
than internet-based connections.
So rather than just having a VPN,
you'll have a dedicated line into AWS and for that reason,
you're going to have more consistent network experience.
Now there's 2 different types
of Direct Connect connections.
There's the dedicated connection,
and this is actually a physical ethernet connection
associated with a single customer and customers
can request dedicated connections
through the AWS Direct Connect console
or you can use the command line or the API.
I'm sure most people do it through the console
and not through command line.
And then you can also have a hosted connection,
and this would be someone like Verizon or AT&amp;T
and basically it's still a physical ethernet connection
but it's provided by an AWS Direct Connect partner
and they provision on behalf of the customer.
So customers can request a hosted connection
by contacting a partner
in the AWS Direct Connect partner program
who will then go ahead and provision the connection.
So let's have a look at Direct Connect in action.
So we've got our AWS Cloud or VPC
and we've got a public VPC
and a private VPC in a region somewhere.
Then we've got our Direct Connect locations.
So these are scattered all across the world
and it's basically is a physical location.
There's one actually just down the road from me in London.
Then we have our customer-wide area network or LAN
and then we have a cage inside
our Direct Connect connection location.
And basically this will be a cage that's owned by AWS
and they'll have their own Direct Connect routers
and then we'll have the customer or partner cage.
So if you are doing dedicated,
you're going to have your own dedicated cage.
If you're using someone like AT&amp;T or Verizon or whoever,
then you're going to use the Partner Cage
and they'll have their own routers in there.
And essentially what happens is,
is you've got your customer router
and then you have a customer provided link,
so this could be an MPLS circuit
or this could be a dedicated line whatever
that connects in to your cage,
so either the customer cage or the partner cage.
And then what we have is the AWS backbone network
which basically goes and connects up our VPCs
to AWS's cage in that Direct Connect location.
And so AWS have a Direct Connect connection
running over the AWS backbone network
from the data center where your VPCs are directly
to the Direct Connect location.
And then all you really need to do is run a cross connect.
So you're running a cross connect between either your cage
or the partner cage directly into the AWS cage.
So it's just an ethernet cable physical connection
and then we have that last mile LAN extension.
And basically we can then create connections
from our customer site directly to our VPC
so this could be a public VPC or a private VPC.
So that's how Direct Connect works.
So VPNs versus Direct Connects well,
VPNs allow private communication
but it still traverses the public internet
to get the data delivered,
and while secure, it can be painfully slow.
Direct Connect is fast, it's secure, it's reliable
and it's able to take massive through-puts.
And ultimately, if you want the final level of security,
what you would do is run a VPN
over a Direct Connect connection.
So onto my exam tips, just remember what Direct Connect is,
it basically directly connects your data center to AWS.
It's useful for high-throughput workloads.
So lots of network traffic
and it's helpful for when you need a stable
and reliable secure connection
and you will get scenario-based questions
where it's talking about your VPN keeps dropping out.
You need to reduce your network costs
and you need to increase your network throughput.
I want you to think straight away when you see a scenario
like that about Direct Connect.
So that's it for this lecture everyone,
if you have any questions, please let me know.
If not, feel free to move on to the next lecture.
Thank you.


Simplifying Networks with Transit Gateway
===========================================

Hello Cloud Gurus
and welcome to this lecture. In this lecture,
we're going to look
at how we can simplify our networks using transit gateway.
So I'm going to show you a network architecture diagram
of how networks can suddenly get very, very complex
when you add more and more components to it. I'm
going to show you how transit gateway solves that problem
and basically how you can simplify your network
topology, look at some facts about transit gateway,
and then we'll move on to my exam tips.
So let's have a look
at a typical network architecture diagram.
So you might just start out small and you have your own VPC.
And then you want to basically expand a little bit
and have separate VPCs
and open your main VPC up to those separate VPCs.
So you create 2 VPC peering connections
but then you might want to add in more VPCs
and VPN connections, AWS direct connect connections,
and you can see that it all starts getting very complicated.
So that's what transit gateway was designed to solve.
It basically connects VPCs
and on-premise networks using a central hub.
And this basically simplifies your network
and puts an end to complex peering relationship.
So it acts as a cloud router
and each new connection is only made once.
So we can take that very complicated network diagram
and we can simplify it.
So by creating a AWS Transit Gateway
we can connect our VPCs to transit gateway.
We only need to do that connection once
and then it's the same
with direct connect and our VPN connections
and then everything that's connected into transit gateway
will be able to talk to each other directly.
So it's a way of really simplifying your network topology.
So in terms of transit gateway facts
it allows you to have transitive peering between thousands
of VPCs as well as on-premise data centers.
It works on a hub and spoke model, similar to VPC peering,
and it works on a regional basis
but you can have it across multiple regions
and you can use it across multiple AWS accounts using RAM,
which is resource access manager.
So onto my exam tips about transit gateway.
You can use route tables to limit how VPCs talk to one
another. Transit gateway works with direct connect
as well as VPN connections.
And it also supports IP multicast,
which is not supported by any other AWS service.
If you want to know what that is,
basically it allows a host to send a single packet
to thousands of hosts across a routed network.
So like for example, the internet,
and it's mostly just used for audio,
so like radio as well as a video distributions.
So if you see transit gateway, and
if you see any scenario-based questions where it's talking
about simplifying your network topology,
I want you to think of transit gateway.
So that is it for this lecture
Everyone, if you have any questions, please let me know
if not, feel free to move on to the next lecture.
Thank you.


VPC Networking Exam Tips
=========================

Hello Cloud Gurus
and congratulations. You're at the end of the VPC section
of the course.
This is actually one of the more difficult sections.
But if you know VPC inside out,
then you're going to do very, very well on the exam.
So let's go through what we learned in this section
of the course.
So just think of a VPC as a logical data center inside AWS.
It consists of internet gateways
or virtual private gateways, route tables,
network access control lists, subnets and security groups.
Remember that one subnet is always in one availability zone.
You can't have a subnet that spans
multiple availability zones.
And let's move on to NAT gateways.
So, we remember that it is redundant
inside the availability zones.
So you don't need to worry about that.
It starts at 5 gigabits per second
and scales currently to 45 gigabits per second.
You'll never actually be quizzed about that on the exam,
like it's individual throughput levels,
but just remember that it does give you sufficient
network throughput.
You don't need to go in and patch the operating system
or anything for your NAT gateway
and it's not associated with any security groups.
And it automatically is assigned a public IP address
when you create it.
Now in terms of high availability for NAT gateways--
and this can be a very popular scenario based question--
if you have resources in multiple availability zones
and they share in that gateway in the event
that that NAT gateway's availability zone is down,
then the resources in the other availability zones
are going to lose internet access.
So to create an availability zone independent architecture,
you need to create NAT gateways in each availability zone.
And then you just need to configure your routing to ensure
that the resources use the NAT gateway
in the same availability zone.
We looked at security groups and we learned
that they are stateful.
If you send a request from your instance,
the response traffic to that request is allowed to flow in
regardless of the inbound security group rules.
And responses to allowed inbound traffic are allowed
to flow out regardless of the outbound rules.
We then looked at network ACL's.
So when you create a VPC,
you get a default network ACL automatically.
And that by default also allows all inbound
and outbound traffic.
We can then go and create our own custom network ACLs.
And we did this to basically create a custom network ACL
that was public.
So essentially anything that we associated with that network
ACL would be publicly accessible via the internet.
So by default, each custom network ACL denies all inbound
and outbound traffic until you add rules.
So it's the opposite of your default network ACL.
When you create a custom network ACL by default
it denies all inbound and outbound traffic
until you go in and add those rules.
We learnd about subnet associations.
Each subnet in VPC must be associated with a network ACL.
If you don't explicitly associate a subnet with a network
ACL, then that subnet is automatically associated
with the default network ACL.
And for that reason you probably don't want your default
network ACL to have a public route out to the internet,
because it means as soon as you create a subnet,
that subnet is always going to be public.
Remember, you can also block IP addresses
using network ACL's
and you don't do that using security groups.
And that can be a common exam question
or scenario based question.
We'll be talking about a hacker or something
and you need to block access.
What level do you do it at?
We'll you always do that at the network ACL level.
You can associate a network ACL with multiple subnets.
However, a subnet itself can be associated
with only one network ACL at a time.
So when you associate a network ACL with a subnet,
the previous association is removed.
Network ACL's contain a numbered list of rules
that are evaluated in order
starting with the lowest number first.
And network ACL's have separate inbound and outbound rules
and each rule can either allow or deny traffic.
And most importantly,
just remember network ACLs are stateless.
So responses to allowed inbound traffic subject to the rules
for outbound traffic and vice versa.
We then learned about direct connect.
And this is a way of directly connecting
your data center to AWS.
It's useful for high-throughput workloads,
so, you know, for example lots of network traffic.
And it's helpful for when you need a stable and reliable,
secure connection.
So you will see scenario questions where they're talking
about you've got a VPN connection keeps dropping out.
You need a stable and reliable connection
that can handle high-throughput.
I want you to think of direct connect.
Moving on to VPC endpoints.
So this is when you want to connect AWS services
without leaving the Amazon's internal network.
There's 2 types of VPC endpoints.
There's interface endpoints and gateway endpoints.
And gateway endpoints basically, at the moment,
just support S3 and DynamoDB.
Moving on to peerings.
So this allows you to connect one VPC with another
via a direct network route using private IP addresses,
instances behave as if they were on the same
private network,
and you can peer VPCs with other AWS accounts
as well as other VPCs in the same account.
Peering is always done in a star configuration.
So this is a hub and spoke model.
So you have one central VPC that peers with 4 others.
You can't have transitive peering.
You can peer VPCs between regions.
And like I said, VPCs is just a way of one VPC being able
to talk to another one.
And here is a good network diagram of VPCs.
If you want VPC B to talk to VPC C
and it's paired with VPC A,
you can't do transitive peering. So you
need to create a peering connection between VBC B and VPC C.
Moving onto AWS PrivateLink.
So if you see a question asking about peering VPCs
to tens, hundreds, and thousands of customer VPCs,
I want you to think of AWS PrivateLink.
It doesn't require VPC peering.
There's no route tables, NAT gateways,
internet gateways, etc.
But it does require Network Load Balancer
on the service VPC.
And it requires an ENI on the customer VPC.
Moving on to transit gateways.
You can use route tables to limit how VPCs
talk to one another.
Transit gateway works with Direct Connect
as well as VPN connections.
And it supports IP multicast,
which is basically a way of allowing a host
to send a single packet to thousands of hosts
across a routed network.
So for example, the internet, is mostly used for audio,
so radio, for example, as well as video distribution.
Also going into the exam if you just see a question about
simplifying network topology
or they're talking about IP multicasting,
just think of transit gateway.
Transit gateway's fantastic technology that basically stops
you from having to have all these different
peering connections, etc.
You just set up transit gateway,
you connect one thing to it at once.
So it could be a VPN connection or VPC peering connection
or whatever,
and then transit gateway will allow you to communicate
directly to everything else.
So it's just a way of simplifying your network topology.
And the very last thing we learned about was securing
your network with VPN Hub,
and this is essentially it just a way of allowing
your customers.
Let's say you've got an office in New York
and you've got an office in Miami
and you're connecting in through a VPN connection
to your VPC hosted with AWS.
If you use VPN Hub, it means that you can basically
have the New York office talk to Miami via the VPN Hub.
So again it's a way of simplifying
your VPN network topology.
So if you see a scenario-based question
where they're talking about that.
How can you simplify your VPN network topology?
I want you to think of VPN Hub.
So that is it for this section of the course.
You've done really, really well.
I would definitely before taking the exam,
just try it and build a VPC by yourself from memory.
If you can do that, you're going to absolutely smash
all the networking questions.
So that's it for this section of the course.
If you have any questions, please let me know.
If not, feel free to move on to the next section.
Thank you.



Route 53
========

Route 53 Overview
=================

Okay, hello, Cloud Gurus,
and welcome to this lecture.
In this lecture, we're going to look at Route 53,
and we're just going to give you a broad overview
before we go and deep dive
into all the different routing algorithms
that we can use with the AWS platform.
So the first thing we're going to do
is just explain what DNS is.
We're going to look at IPV4 versus IPV6,
we're then going to talk about Top-Level Domains.
We'll talk about Domain Registrars,
so where you can go ahead and register a domain name.
We'll then talk about common DNS record types.
We'll talk about what a Time To Live is,
we'll then cover off Alias Records,
Routing Policies that are available to us with Route 53
and then we will go on to my exam tips.
And then in the next lecture, what we're going to do
is we'll look at how to register domain name
get set up for this next section of the course
and then we'll actually go in
and demonstrate each of the individual routing policies.
So we've got a lot to cover in this section of the course.
So let's start with what is DNS.
So if you've used the internet before, you've used DNS,
and DNS is basically used
to convert human friendly domain names.
So for example, acloud.guru into an internet protocol
or IP address, and this is just going to be a number
like 82.124.53.1.
So IP addresses are used by computers
to identify each other on the network
and IP addresses commonly come into different forms,
IPV4 and IPV6.
So basically DNS is something that computers use
to convert domain names into IP addresses.
And an example I like to use
is when you go and look up a telephone number
in a phone book, that's all you're doing,
is you're basically converting the business name
or the person's name into a telephone number,
basically a phone book was the original form of DNS.
So a great analogy I like to use to remember how DNS works.
So IPV4 addresses are running out.
So IPV4 was created a while ago, space is 32-bit friendly
and has over four billion different addresses.
And that was fine before the internet became widespread.
But of course, we're gonna have
more than four billion devices online now,
for that reason, we are starting to run out
of IPV4 addresses.
Now to visualize an IPV4 address,
if every single IPV4 address was a grain of sand
four billion would basically be enough
to fill up a dump truck,
so that's how many IPV4 addresses there are.
So IPV6 was created to solve the depletion issue
and it has an address space of 128 bits.
And in theory, there's 340 undecillion addresses,
and to, again, to put that into perspective,
if every single IPV6 address was a grain of sand
and you bought them all together,
it would be enough to fill the entire sun.
So we go from having a IPV4 address range
which would fill a dump truck
all the way to an IPV6 address range that fills the sun.
Unfortunately it is taking a bit longer to move over
to just universally using IPV6 addresses,
I'm not gonna get into why that is
but eventually we will completely have migrated
off IPV4 to IPV6.
So Route 53 supports both IPV4 and IPV6
and you just need to remember that going into your exam.
So we'll move on to Top-Level Domains.
And if we look at common domain names,
things like google.com or bbc.co.uk or acloud.guru,
you'll notice a string of characters
that are separated by dots or periods.
And so the last word in a domain name
represents the Top-Level domain name.
The second word in domain name
is known as the second-level domain name
and this is optional and depends on the domain name.
So like a.co.uk, the .co is the second-level,
whereas the top-level is .uk.
So that's how Top-Level domains work.
You obviously everyone's familiar with .com
but then you can get things like .gov or .gov.uk,
and these will be reserved for governments
such as the UK government or U.S. government.
So in here we've got our Top-Level domain names
like I said .gov, .gov.uk, .com.au is another popular one,
or .edu.
So these Top-Level domain names are controlled
by the Internet Assigned Numbers Authority, or IANA
in a root zone database, which essentially is a database
of all available Top-Level domain names.
And you can actually go and view this database
just by visiting iana.org/domains/root/db
So because all the names in a given domain name
must be unique, there needs to be a way
to organize this all so that domain names aren't duplicated
and this is where domain registrars come in.
So registrar is basically an authority
that can assign domain names directly
under one or more Top-Level domains.
And these domains are then registered with internet
which is a service of ICANN
and it basically enforces the uniqueness of domain names
across the internet.
And each domain name becomes registered
in a central database known as the WHOIS database.
So some five popular domain registrars.
Some of these you've probably heard of.
So domain.com, GoDaddy, there's Hoover, there's AWS,
which has a domain registrar now
and there's Namecheap as well.
So in this course,
we're going to be using AWS as our domain registrar
because it's all integrated into the AWS platform.
Moving on to some common DNS record types.
And we're going to start with what we should start with
which is the start of authority or SOA record.
And this basically stores information
about the name of the server
that supplied the data for the zone,
who the administrator is of the zone,
the current version of the data file,
and then the default number of seconds
for the time-to-live file on the resource records.
So we'll move on to look at how the SOA works in action.
And to do that, we need to have a look at our NS records.
So our NS just stands for name server records
and name server records are used by Top-Level domain servers
to direct traffic to content DNS servers
that contain the authoritative DNS records.
So let's say we've got our user and our user types,
hellocloudgurus.com into their browser.
Now, what will happen is that browser
will basically go to the .com record
and it will look up hellocloudgurus,
and it'll give it an NS record.
In this case, it's going to be ns.awsdns.com.
So this is where our SOA will be stored.
So then our browser will basically browse
over to the NS records
and we're going to go to ns.awsdns.com,
and we're going to get our SOA.
And our SOA is just our start over authority
and this is what's going to contain
all our DNS records in there.
So things like A records, MX records, CNAME, etc.,
and we're gonna cover off
all the different types of DNS records now,
or just a brief overview.
So what's an A record.
This is the most common DNS record
and this is basically what the computer uses
to look up the name of the domain
and translate that into an IP address.
So if I was to go to hellocloudgurus.com
and we look up the A record,
it's going to have a IPV4 or IPV6 address.
So acloud.guru might point
to http://13.10.10.80, for example,
so that's all an A record is,
it's basically the most common type of DNS record.
Now, when we update our A records,
when we move our IP addresses over
we're going to come across this thing called TTLs.
So what is a TTL?
TTL just stands for time to live?
So the length that a DNS record is cached
either on the resolving server or on the user's own local PC
is equal to the value of the time to live in seconds.
So say we've got acloud guru.com
and it goes to 123.10.80.80,
and then for some reason,
we have set up a new web server somewhere else
and we want to migrate the DNS records
from one web server to another.
So we're just going in and updating our IPV4 address.
We can go and update that address immediately
but that effect won't take place immediately
because some other browsers would have cached it,
some of the DNS servers would have cached it
which basically means that that record
will just sit in the cache
until the time to live has expired.
So the lower the time to live, the faster the changes
to DNS records take to propagate throughout the internet.
So when I worked as a sysadmin, for example,
if we were planning for a server migration
we would lower our TTLs from 48 hours
basically down to five minutes,
and we do that a couple of days before the migration.
That way, when we updated our A records,
the migration would only take five minutes
as opposed to 48 hours.
Now, this is one of my favorite (indistinct),
I have tweeted this, I live and died by this
when I was a sysadmin,
DNS, if you don't set your time to live correctly
or if you don't reduce them for doing a migration
it can take days for DNS to propagate.
You know, you've got customers calling you up,
saying what's going on and this haiku sums it all up
basically as a sysadmin, you like, it's not DNS,
there's no way it's DNS,
and then 48 hours later, it was DNS.
So that's one of my favorite haikus on the internet.
So TTLs basically is just how long
it's going to cache your DNS records
in the cloud, essentially.
So what's a CNAME?
A CNAME is a canonical name,
and this can be used to resolve one domain name to another.
So for example, you might have a mobile website
with the domain name m.acloud.guru,
and that's useful when users browse
to your domain name on their mobile devices.
So it's just a way of mapping one domain name to another.
You may also want the name mobile.acloud.guru
to resolve to the same address.
So you could set up a CNAME where m.acloud.guru
maps to mobile.acloud.guru.
And we'll have a look at how we can do this
a little bit later on
but essentially you're just going in to the Console
and you're setting up a CNAME, so you can see it in here,
we've got m and then this domain name,
certifiedcloudpractitioner.com,
and we're going to basically map that
to mobile.certifiedcloudpractitioner.com.
So CNAME is just a way
of translating one web address to another.
Moving on to Alias records.
Alias records are used to map resource records sets
in your hosted zones to load balances,
CloudFront distributions, or S3 buckets
that are configured as websites.
So an Alias record really exists within the AWS ecosystem.
It's not a type of DNS record that you could set up
with GoDaddy or on your own web servers, for example.
Alias is just a way of mapping resources within AWS,
within your hosted zones to other AWS resources
such as load balancers, CloudFront distributions
S3 buckets, etc.
So Aliases basically work like a CNAME record
in that you can map one DNS name to another target DNS name.
So that's all an Alias record is,
just remember that CNAMEs
cannot be used for naked domain names.
So this is your zone apex record.
So for example you cannot have a CNAME
just for acloudguru.com.
You need to have it for something like m.acloudguru.com.
And also remember that your Alias records
can be used for naked domain names or zone apex records
'cause these are mapping to individual AWS services.
So there are seven different routing policies
that are available with Route 53
and we're going to cover off every single one of these
in this section of the course.
And we're going to give you demos of them.
So we'll start with simple routing,
we'll then move on to weighted routing,
latency-based routing, failover routing,
geolocation routing, geoproximity routing,
and then we'll move on finally
to multivalue answer routing.
So I'm gonna explain what each of these are
in the individual lectures
and then we'll go through and demo them.
So just on to my exam tips, just understand the difference
between an alias record and a CNAME.
Remember an alias record can map onto a naked domain name,
so if I wanna go to acloud.guru.com,
I can set up an alias record, which will map back
to an elastic load balancer, for example, or an S3 bucket,
whereas with a CNAME, you cannot do that.
And if you get a scenario-based question
where you're given the choice between a alias record
versus a CNAME for anything to do with AWS,
always choose an alias record over a CNAME.
And then finally going into your exam,
you should just understand the common DNS record types
'cause they might come up in scenario based questions.
So an SOA is just the start of authority record.
This is where your DNS starts,
CNAME record is a way
you can map one domain name to another,
NS records are just your name server records
of where is the DNS information stored,
and then your A records are your most common DNS types,
and this just basically maps your domain name
back to an IP address.
So in the next lecture,
we're going to look at how we can go ahead
and register domain name with AWS.
We're then going to provision a couple of EC2 instances
around the world.
And then that will get us set up
to explore all the different routing policy types
and we're going to look at each one individually
with our setup using our domain name that we register
and using the EC2 instances that we've provisioned.
So if you've got the time,
please join me in the next lecture, thank you.


Register a Domain Name
======================

Okay. Hello Cloud Gurus,
and welcome to this lecture.
In this lecture, we're going to look
at how we can register a domain name with AWS
and then we're going to go in and provision 2 EC2 instances,
one in Northern Virginia and one in Tokyo.
And that will gear us up for the rest
of the lectures in this section of the course.
So like I said, you can register a domain name
directly with AWS.
This is the preferred method because it integrates
with all of the AWS services,
and they'll automatically set up the hosted zone
for you when you buy a domain through AWS.
But it can take up to 3 days
to register your domain name
depending on the circumstances.
In most cases, you should be able to get it in an hour,
but it can take up to 3 days.
So let's go ahead and log into the AWS console.
Okay. So here I am in the AWS Management Console.
If we scroll down to Networking and Content Delivery,
you'll be able to see Route 53 in here.
So let's go ahead and click in here.
And if you've never used it before
you're going to get this landing screen
or this splash screen.
So straight away, we can see
that we can register a domain name.
Note up here that the region is global.
So Route 53 is at a global level.
You don't have to specify what region.
Because it's the DNS service.
Of course, it's going to be global.
And in here I'm going to register a domain name.
So I'm going to call it hellocloudgurus123.com
something like that.
But go ahead and hit Check.
That will then do a search to see
if hellocouldgurus123.com is available and it is.
And then what I'm going to do is go ahead and hit Continue.
And in here we need to enter in our registration details
and note down here, you've got privacy protection.
So it says privacy protection,
hide some contact details for .com domains.
So I'm just going to go in
and fill this in and then go ahead and hit Continue.
So I've gone ahead and filled in my details, hit Next.
I'm going to automatically renew my domain.
I have to just click in here
that I've read the terms and conditions.
It's also sent me an email which is waiting
for verification, and I'm going to go ahead
and hit Complete Order.
You can see that that has been submitted successfully.
If we go to our domains,
it will take some time for this domain to register.
So this can take up to an hour, to be honest,
it does actually say it can take up to 3 days
but it totally depends on your circumstances,
and what domain you're trying to register.
So while we wait for that, let's go over to AWS.
And what we're going to do is provision
some EC2 instances in different areas of the globe.
So if we go into EC2, and you can see that we're
in Northern Virginia right now, I've actually
got 2 instances running from a previous lecture.
So I'm just going to go through and go ahead
and delete that.
So instance state, terminate instances, and terminate.
And then what I'm going to do is I'm going to go in
and launch a new instance,
and it's going to be a web server,
so I'm going to use my Amazon Linux 2 AMI.
And in here, we're going to choose a t2.micro.
Now in here, I'm going to use a bootstrap script,
and you will be able to get this in the resources section
of the course.
Essentially all it does is it installs,
it runs a yum update,
it installs Apache,
it starts the Apache service and then goes
to the /var/www/html directory,
so the root directory for our web server.
And then basically it creates an index.html, which says
Hello Cloud Gurus.
In here, I'm just going to change it
So we could do
Hello Cloud Gurus This is Web Server 1
in Northern Virginia, so in NV.
So what I'm going to do is just leave that as is
and I'm going to go ahead and hit Next
and then we're going to just leave everything as default.
We'll give it a name.
So just in here
we'll call it name web server 01,
something like that.
And then I'm going to go ahead
and configure my security groups.
I'm going to put this into our web DMZ security group.
This is on our default VPC
and I'm going to go ahead and hit Review and Launch,
and I'm going to go ahead and launch it.
And I'm going to use just my normal key pair instance.
Hopefully we won't need to log into these web servers.
So that is now launching.
And you will be able to see it launching in here
so its status is currently pending. So next
thing I'm going to to do is go and change my regions.
So I'm currently recording this in London.
Northern Virginia is across the Atlantic to me.
What I'm going to do is I'm going
to go and deploy another server in the Pacific.
So I'm going to choose Tokyo, and I'm going to go in
and launch a web server in there.
And then all we're going to do is change the bootstrap
script so that it will identify as Web Server 2.
So we know that Web Server 1 is in Northern Virginia,
and Web Server 2 is going to be in Tokyo.
So let's go in and put in our bootstrap script in here.
So again, we've got a yum update.
It installs Apache, starts the Apache service, et cetera
et cetera.
So Hello, Cloud Gurus.
So I'm just going to edit this and put
in This is Web Server 2 in Tokyo.
So this is Web Server 2 in Tokyo.
So there we go.
I'm going to go in next, add my storage, leave it
as default, add a tag, give it a name,
and we'll call it web server and then 2,
and then we'll go ahead and hit Next.
We're going to have to create a new security group for this
because I've never used the Tokyo region before.
So let's just go ahead
and call this web DMZ and then web DMZ.
And I'm just going to make it open to HTTP.
So HTTP, and I'm going to make it open to HTTPS as well.
So in here.
And so then I'm going to go ahead and hit Review and Launch
and I'm going to go ahead and launch
and I'm going to have to create a new key pair.
So just call it Tokyo key pair,
something like that and download my key pair.
And now I'm ready to go ahead and launch my instance.
Okay. So I'm back in the EC2 console,
if we click on Instances, we'll be able to see
that this is now up and running.
And if I click in here
I'll be able to get my public IP address.
So I am just going to copy that to my clipboard
and then I'm going to browse to it in my browser.
And we can see that that has worked.
So it says, Hello Cloud Gurus
This is Web Server 2 in Tokyo.
Let's go test the Northern Virginia one.
So all we're going to do is change our region
from Tokyo all the way back to Northern Virginia.
And in here we can see our Web Server 01.
And if we grab this public IP address
and I paste that into my clipboard and then into my new tab,
and you can see here, it says, Hello Cloud Gurus.
This is a Web Server 1 in Northern Virginia,
and This is Web Server 2 in Tokyo.
So I can go backwards and forwards between the 2.
Now, the reason we moved them so far apart
is we're going to look at all the different ways
in which a Route 53 can route
our DNS requests around the world.
So you will need to have this set up
if you're going to participate
in this section of the course, you will need
2 EC2 instances in different regions in the world.
And you'll also need to have registered a domain name,
but it's entirely up to you if you just want to watch
and don't want to do it yourself, that's fine as well.
You can save yourself some money.
So let's go back over to Network and Content Delivery
and we'll just go over to Route 53.
And we'll see
if that domain registration has been successful, and yes
it has been successful.
So the very last thing we need to do is go ahead
and go into our hosted zone
and what we're going to do as you can see it is in here.
So it has created a hosted zone for our domain name
and we can click in here and now we can see our
basically, our NS records and our SOA records.
And now we can go in and start creating records.
And that's what we're going to do all
in the next few lectures in the course.
So if you've got the time
please join me in the next lecture.
Thank you.


Demo : Using a Simple Routing Policy
====================================

Okay, Hello Cloud Gurus
and welcome to this lecture.
In this lecture we're going to look at our very first
routing policy, and we're going to start
with simple routing policy.
So look at what simple routing policy is,
we'll look at how to set it up in the console
and then we'll just go to my exam tips.
So what is simple routing policies?
Well, basically, if you choose the simple routing policy
you can only have one record with multiple IP addresses.
If you specify multiple values in a record,
then Route 53 returns all values
to that user in a random order.
So we've got our user, they're basically
typing in our domain name, it's going to Route 53
and it could be that it goes to 30.0.0.1,
or it could go to 30.0.0.2,
it's just returned in a random order.
So that's all simple routing policy is.
Let's go and have a look at how
we can set this up in the console.
Okay, so here I am in the AWS console.
I'm going to go to find Route 53,
which is under Network and Content Delivery, and
what I'm going to do is I'm going to go into my hosted zone.
And we can see my hosted zone in here,
it's hellocloudgurus123, and we can see
that we've only got 2 record types in here.
So what we're going to do is we're going to go ahead
and hit Create Record, and this will allow us
to create our very first record.
Now you see that we've got 2 different views,
we've got Quick Create, recommended for expert users,
and then we've got our wizard,
which is recommended for our new users.
We're always going to use the Quick Create record.
And, in here, we can select our different record types.
So we've got our A records, we've got our CNAMEs,
we've got MX records, which does our mail servers,
we've got TXT records, we've got NS records,
and we've got all kinds of different records in here.
Now we're going to be using our A records,
so let's click in there and we're going to use
the naked domain name, so it's just going to be
hellocloudgurus123.com.
In here we've got our routing policies,
and this is where we've got all our different
routing policies that are available to us,
so we're going to start with simple routing policy.
In here we've got our time to live,
so how long is this record going to be cached
on other people's servers or in their browsers?
I am always going to have it as 1 minute,
that way, basically, when we go and make changes to our DNS
we only have to wait about 60 seconds.
Note, you can have up to 2 days, but let's not do that,
let's just start with 60 seconds.
And in here we've got our values.
So what IP address do we want?
Well let's go in and open up a new tab,
and we'll go over to EC2, and we're going to open up a tab
and grab our IP addresses for both Northern Virginia
and then we're going to get it for Tokyo.
And it says, here, enter multiple values
on our separate lines.
Okay, so here I am in the EC2 console,
I'm in Northern Virginia, and all I want to do
is just click on my running instance, click in here,
and copy this public IP address to my clipboard
and then going to paste that in here for Route 53,
and then all I want to do is change
from Northern Virginia all the way back to Tokyo.
I'm going to add in Tokyo in here,
and then I'm just going to grab the public IP address
for my Tokyo web server. So it's this one
here, and I'm going to paste that in there.
So we're now going to create this record,
it's just an A record, it's using simple routing,
our TTL is 60, we're not using an alias,
we're going to put it towards these 2 public IP addresses,
and I'm going to go ahead and hit Create Record.
That has now created our A record, which we can see in here,
and we can see that it's an A record, rooting is simple,
and we're going to send our traffic
to these 2 different IP addresses.
So let's go ahead and test this out
by going to hellocloudgurus123.com.
Okay, so I've just gone to hellocloudgurus123.com.
We can see it's directing us to Northern Virginia.
And, so, I've just opened up a new browser
in incognito mode and this has now directed me to Tokyo,
and you will be able to just test it randomly.
Just go ahead and open up your browser
and go to the domain names in incognito mode,
but in different browsers, and you'll be able to see
that sometimes it will hit Tokyo,
sometimes it will hit Northern Virginia.
So onto my exam tips.
Just to remember, going into your exam
what simple routing policy is and how it works.
So if you choose the simple routing policy
you can only have one record with multiple IP addresses.
If you specify multiple values in a record,
Route 53 returns all values to a user in a random order.
So that is it for this lecture everyone.
If you've got the time,
please join me in the next lecture, thank you.


Demo : Using a Weighted Routing policy
======================================

Okay. Hello Cloud Gurus
and welcome to this lecture.
In this lecture we're going
to look at Weighted Routing Policies.
So we're going to explore what a Weighted Routing Policy is.
I'll show you how to set this up in the Console,
and then we'll move on to my exam tips.
So what is a Weighted Routing Policy?
Well, basically it allows you to split your traffic
based on different weights assigned.
So for example, you could send 10% of your traffic
to go to us-east-1 and 90% to go to us-west-1.
So it's just a way of splitting your traffic
based on particular weights.
Now, before we get into how that all works,
let's also just have a quick look at Health Checks.
So you can set health checks on individual record sets.
So an individual EC2 Instances
or on individual load balancers,
but you can also set them on individual records,
such as weightings.
So if we set up weight a at 10% and then weight b at 90%,
and let's say us-east-1 is at 10%,
and it goes down for some reason,
then it basically will fail the health check
and it send 100% of our traffic to us-west-1.
So that's all a health check is.
if a record set fails a health check,
it will be removed from Route 53
until it passes the health check.
And you can actually set up SNS notifications
to alert you about failed health checks.
So if us-east-1 goes down,
you can set up an SNS notification
where it will basically send a message to your phone
or to your email telling you that it has gone down.
So let's go ahead and have a look at how we can do this
in the AWS Console.
Okay. So here I am in the AWS Management Console,
I want to find Route 53
which is always under Network Content and Delivery,
and what I want to do is go into my hosted zones.
Now, the first thing I'm going to do
is I'm going to go into my domain name
and I'm going to delete my simple record set.
So I'm going to go ahead and hit Delete Record.
That's going to delete this one,
and then we're going to go ahead and create a new record.
Now, before we do that,
let's just go back over to the splash screen of Route 53.
The UI is a little bit tricky here.
They don't make it easy for you to browse to health checks
unless you're on this landing page.
So up here we've got our health checks,
availability monitoring.
So what we want to do is create a health check
for each weighted route that we're going to create.
So in other words, we're going to create a health check
for our EC2 Instance, that's in Northern Virginia,
and then we're going to do it
for our EC2 Instance that's in Tokyo.
So in here, we might just say NV Health Check,
something like that.
And here it says what to monitor.
So do we want to monitor the endpoint,
which will be the IP address,
the status of other health checks
or state of a CloudWatch alarm?
Well, I'm just going to monitor an endpoint.
I'm going to specify my endpoint by IP address,
and my IP address is just going to be
this IP address in here,
and which I've just double-checked
is definitely the IP address
of my Northern Virginia EC2 Instance.
And here we've got our hostname,
so we could type index.HTML, for example, in here.
And then in here we've got ports,
so it's going to do this on port 80,
and in here we've got our path
so we could have different sub-directories et cetera.
So let's go ahead and hit Next,
and do we want to get notified when this health check fails
and create an SNS notification?
I'm just going to say no, but you can do that as well.
So I've gone in and I've created my health check.
Next thing I'm going to do is create another one for Tokyo,
so we've got NVHC.
Let's go ahead and just type in Tokyo,
and then HC health check.
Again, it's going to be my endpoint.
And what IP address am I going to use?
Well, I'm going to use the IP address
for my Tokyo EC2 Instance,
and here we can type index.HTML,
because it's going to be monitoring our index page.
We're going to be doing this on port 80,
I don't need any additional paths,
and let's go ahead and create my health check.
So we've got two health checks,
we can see now that the Northern Virginia Health Check
has come back as healthy,
the Tokyo health check, we could just refresh this.
It can take up to a minute also for it to populate
but that should come back as healthy as well.
So we go back and click on our hosted zones in here.
We will be able to click on
hellocloudgurus123.com,
and we'll be able to go in and create our record sets.
So now we're going to create a record.
We're going to do an A record,
so we're going to be translating our domain name
into our IP address.
We're just going to leave it as blank,
so it's going to be the naked domain name
or the zone apex record.
And in here, what I want to do is change my routing policy
from simple to weighted.
Now, my weighted routing policy
it allows me to enter in a weight.
Basically what it will do is add up
all the weights that you have on a weighted routing policy
and then divide the individual record
by the sum of those weights.
So to keep it really simple
we just want it to add up to 100.
So let's say I'm going to do 30 to this record.
So this is going to Route 30 to Northern Virginia,
and here I can click in my health check
and just do my Northern Virginia health check.
And what I want to do is
I want to go and grab my IP address.
I'm just going to paste it in as a value up here.
You can have multiple values on the line
but don't forget that this is just a weight of 30.
So we want a separate record sets for our weighted records
which means we're going to have different IP address
in those separate records sets.
So this is our very first record.
Let's go ahead and hit Create Record.
Oh, I forgot to add my record ID.
So you have to give this a record ID,
so we can say Northern Virginia
and then weight, something like that.
So this is basically just saying
we're going to create a record for Northern Virginia.
Let's go ahead and hit Create.
And that has now created my weight,
and you can actually see the weight in here.
So it's weighted, differentiator is 30,
and it's basically going to send 30% of our traffic
to this IP address.
Now, right now because there's only 30 in there,
it's going to send 100% of our traffic in there
because what it's doing is it's taking the number 30
adding it up, and then dividing this record by 30
which of course is going to be 100%.
So let's go ahead and create another record.
So in here, I'm going to go and change this to weighted.
And then in here, I'm going to paste the value
of my Japanese or my Tokyo IP address.
So pasted that in there,
I forgot to reduce my TTL in the last one to 60 seconds.
That is always a good thing to do
because it's going to cache it for 5 minutes by default.
And so I'm just going to drop that down to 60,
we can go in and fix that later.
And here we've got my weight,
so I'm going to put 70% of my traffic to Tokyo,
and here I've got my health check,
so I'm going to do use my Tokyo health check
and then in here, my record ID,
so just call it Tokyo and then record something like that.
So let's go ahead and create that record
and that is now creating our record.
And if we look into our hosted zones
we can see 70% of our traffic is going to go to Tokyo,
30% of our traffic is going to go to Northern Virginia.
So let's go ahead and test this.
Okay, so I've gone to hellocloudgurus123.com,
and I'm just going to hit Refresh.
Now I am getting a DNS error,
and this is that classic Haiku.
We just need to be a little bit patient
while this DNS changes, propagate out to the internet.
So at maximum, it's going to take 5 minutes
because I left the 5 minute TTL in there.
At least the minimum it's going to take 60 seconds.
So I'm just going to pause the video.
Okay. So I've waited just over 5 minutes,
I'm going to hit refresh now and there we go.
Hello Cloud Gurus, this is web server,
and it's sending me to Northern Virginia
and I've just done a hard refresh,
and it's now sending me over to my a web server in Tokyo.
So 70% of the time will end up in Tokyo,
30% of the time it end up in Northern Virginia.
You do need to be a bit patient
probably wait about 5 or 10 minutes before trying it
but you will be able to see it working.
So let's go onto my exam tips.
So onto my exam tips
just remember what weighted routing policy is.
So in this example, Route 53 sent 20% of traffic
to us-east-1 and 80% of traffic to us-west-1.
And we did that just using weights.
In the console demo we did 30% over to Northern Virginia
and 70% over to Tokyo or Asia Pacific.
So in your exam, you're going to get scenario questions,
and if that scenario question
talks about sending a certain amount of your traffic
or a certain percentage of your traffic to specific regions
then I want you to immediately think
of weighted routing policies.
We also learn what health checks are,
so you can set health checks on individual record sets.
If a record set fails a health check
it will be removed from Route 53
until it passes the health check,
and you can also set up SNS notifications to alert you
about failed health checks.
So that is it for this lecture everyone.
If you've got the time
please join me in the next lecture.
Thank you.


Failover Routing Policy
========================

Okay, hello, Cloud Gurus.
And welcome to this lecture.
In this lecture we're going to look at
the Failover Routing Policy.
So explore what it is,
I'll show you how to set it up in the console
and then we'll go onto my exam tips.
So Failover Routing Policies
are used when you want to create an active/passive setups.
So for example you might want your primary site
to be an eu-west-2 and your secondary
Disaster Recovery site in ap-southeast-2.
And essentially what's going to happen
is Route 53 will monitor the health
of your primary site using a health check.
And if you remember a health check
just basically monitors the health of your endpoints.
Let's have a look at how this would work in practice.
Let's say we've got our user and we've designed our website
so that we've got an active and a passive site.
And let's say that the active site is in eu-west-2
and that might be a load balancer
with 1000 EC2 instances behind it.
And then just in case eu-west-2 goes down,
we might have a passive site in Asia Pacific
so ap-southeast-2.
And that might be a load balancer
we'll just say 100 EC2 instances behind it
and it's just there in case eu-west-2 fails.
So if we do get a failure, what will happen
is basically Route 53 will automatically redirect our traffic
from eu-west-2 over to ap-southeast-2.
So that's the way it works.
So let's go ahead and have a look
at how we can set this up in the AWS console.
Okay, so here I am in the AWS console.
As always I will find Route 53 under Network
Content and Delivery.
And then what I want to do is I want to go in
to our hosted zones under DNS Management,
and I'm going to click on our A Cloud Guru ones
and I'm going to get a go ahead
and delete the latency-based routing.
So just go in and hit Delete records and hit Delete
and that has now deleted our records.
So let's go ahead and create a new record.
So in here, we're going to drop the TTL down to 60 seconds.
The routing policy, we're going to go ahead
and select Failover.
So we're going to have Failover records.
So here we've got our Failover record type.
Do we want this to be our primary or our secondary?
Well, we're going to point our primary to Northern Virginia
and we'll do our secondary overt to Tokyo.
So I'm just going to call this record ID Northern Virginia.
And I'm going to paste my Northern Virginia IP address in
here and I'm going to go ahead and create this record.
So I've now created my primary failover record.
I need to go in and create my secondary record,
so again, I'm going to drop my TTL to 60 seconds.
I'm going to do failover.
I'm going to choose my secondary,
my health check I'm going to use the Tokyo one.
And here I'm just going to type in Tokyo.
And in here, I'm going to paste in my Tokyo IP address
and hit Create Records.
So we can now see that we've got our routing policies
for hellocloudgurus123.com, failover
and we've got our primary and secondary
primary points to Northern Virginia
secondary points to Tokyo.
So I could go in and delete the EC2 instance.
I don't want to do that because then I have to reinstall it
with another bootstrap script.
So the way I'm going to simulate a failover
is I'm just going to go over to Services.
I'm going to go over to EC2 under Compute.
And then what I'm going to do is I'm going to go in
and I'm going to go to my security groups.
And what I'm going to do is I'm going to go
to my WebDMZ security group
and I'm just going to delete the inbound rule
allowing HTTP on both IPv4 and IPv6.
And essentially what that's going to do
is it's not going to allow any HTTP traffic in on that
and what will happen is Route 53 won't be able to resolve
it. The health check will fail and then automatically
it should fail us over to our Asia Pacific region.
So before I do that I'm just going to hit
a hard refresh on here and we can see
it is automatically going to our web server
in Northern Virginia when I resolve hellocloudgurus123.com.
Okay, so I'm in the AWS console.
I want to go over to EC2 under Compute,
and I want to go over to my security groups
which has under Network and Security.
Make sure you're in the right region.
I accidentally did this in Tokyo
so I'm just trying to undo that now.
So I'm going to do this, make sure I'm in Northern Virginia
and I'm going to do this using my WebDMZ security group
which is registered to my normal default VPC.
So I'm going to scroll down, click on my Inbound Rules.
And we can see in here, we've got our rules
which are basically allow HTTP and HTTPS.
So what I'm going to do is I'm just going to go ahead
and click inside this security group
I'm going to edit our inbound rules
and I'm just going to remove HTTP
and HTTP for both IPv4 and IPv6.
I'm going to go ahead and hit Save.
And that is now revoking my rules and it will make it
so that this web server is no longer accessible.
It's still there, but now we're not allowing
port 80 in through our security group.
We go over to All Services.
And if we go down to Network and Content Delivery
and go to Route 53,
I'll show you where you can see the health checks.
If you just click on this little icon up here
you'll be able to bring off this side dashboard
and you'll be able to see your health checks here
under Dashboard.
And in here, like I said, I accidentally did this
on the Tokyo security group and not Northern Virginia.
So I'm just going to pause the video for a couple of minutes
and wait for Tokyo to be healthy
and Northern Virginia to be unhealthy.
Okay, so I've hit Refresh and we can see here now
that Northern Virginia is unhealthy
whereas Tokyo is healthy.
So let's go ahead and browse over to hellocloudgurus123.com.
Okay, so I'm just going to hit refresh
on hellocloudgurus123.com.
And you can see straight away
it has now failed over to Tokyo
because the Northern Virginia
is failing that health check.
So just so we can go ahead and fix everything
before the next lecture, let's go back over to EC2.
We'll go over to our Security Groups
which is under Network and Security. I'm going
to go over to my WebDMZ which is on my default VPC.
And in here we can see it's got no HTTP.
So we're going to edit our inbound rules.
Let's go and add some inbound rules.
So we're going to go in and add HTTP.
And I'm going to add it again for IPv6.
So add HTTP, and then you can just basically copy and paste.
And then basically you can just click in here
and do IPv4 and IPv6 and go ahead and hit Save Rules.
So that instance will now become healthy again,
that health check will no longer fail
if I give it enough time.
And then the last thing I just want to do
is just go back over to Route 53
and we'll just go through and delete those record sets.
So we go back into our hosted zone
and we'll go ahead in here
and I'm just going to delete my failover record sets.
So let's go ahead and have a look at my exam tips.
So onto my exam tips, let's say you've got an application
and you've architected it so that you've got an active site
and a passive site.
And your active site is in eu-west-2
and your passive site might be in ap-southeast-2.
And then you lose your active site.
Then what will happen is Route 53
with Failover Routing Policy
will automatically failover your active site
to your passive site.
So if you get a scenario-based question
where it's talking about active passive
I want you to think of Failover Routing Policies
and likewise, if they also mentioned the word failover
then that should give you a very big hint
that you're looking for Failover Routing Policies
with Route 53.
So that is it for this lecture everyone.
If you've got the time please join me in the next lecture.
Thank you.


Demo : Using a Geolocation Routing policy
============================================

Okay. Hello, Cloud Gurus,
and welcome to this lecture, in this lecture
we're going to look at geo location routing policies.
So we'll explore what that is,
I'll show you how to set it up,
and then we'll go onto my exam tips.
So geolocation routing policies lets you choose where
your traffic will be sent based
on the geographic location of your users.
So this is the location
from which the DNS queries originate.
So it's the end location of your end users.
So good use case for this
is you might want all queries
from Europe to be routed to a fleet
of EC2 instances that are specifically configured
for your European customers.
So this might have the local languages
of your European customers,
this might be French or Italian web servers
and it would display all the prices
in Euros as opposed to US dollars, for example.
So let's have a look at an example.
So we've got our European customers
and we want Route 53 to automatically direct them
to eu-west-1.
And for our US customers, we want Route 53 to
automatically redirect all our US customers to US east one.
So let's go ahead and have a look
at how we can set this up in the AWS console.
Okay. So here I am in the AWS console,
I'm going to go to Route 53
which is, as always as under Network Content and Delivery,
and then I'm going to go into my hosted zone,
and we're going to create some more record sets.
So let's go ahead and create a record,
and essentially I'm just going to do it, all
my European customers will go over to Tokyo
and all my US customers will go over to the US. So in here
we're going to use an A record drop my TTL to 60 seconds.
We're going to choose geo location
and here we can choose our location.
Now we can do this on continents,
or we can do it by individual countries.
I'm going to do it by continent,
so I'm going to say, I want all my European customers to go
to my IPv4 address in Tokyo, my health check,
I'm going to make sure that Tokyo is actually up online,
and then in here, my record ID
I'm just going to type in Tokyo.
So that's going ahead and creating this record,
and then if I want to create a record
for my US customers, going to do exactly the same thing,
so our routing policy is geo location,
our location is going to be North America,
we're going to do it to our Northern Virginia health check.
I'm just going to type in Northern Virginia in here,
and then I'm just going to paste
in the IPv4 address of my Northern Virginia web server.
So we'll now be able to see our 2 record sets in here.
This is our routing policy,
So it's geo location,
and you can see the differentiators one's
for North America and one is for Europe.
So let's go ahead and test this.
Okay. So I'm just going to hit Refresh on here
and you can see, it has directed me to Tokyo
which is what we said
because all our European customers is going
to the Asia Pacific region
and all our US customers are going over to Northern America.
So onto my exam tips, you're basically going to
get a scenario based question where you've got your users
in different parts of the world
and you need to control where you're going to
send them based on their end locations.
You want to send your European customers to eu-west-1
or you might want to send your US customers to us-east-1.
If you see any kind
of scenario based question where it's talking
about the location of your users and you need to send them
to a particular country, maybe for regulatory requirements
and it's not talking about latency, for example,
then you probably want to use geo location routing rather
than latency based routing.
So that is it for this lecture,
everyone, if you have any questions, please let me know.
If not, feel free to move on to the next lecture. Thank you.


Demo : Using a Geoproximity Routing Policy
==========================================

Okay. Hello, Cloud Gurus
and welcome to this lecture.
In this lecture, we're going to look
at using a geoproximity routing policy.
So, we'll look at what they are.
We'll then do a demo and we'll go onto our exam tips.
So, let's talk about Route 53 traffic flow.
And basically you can use Route 53 traffic flow
to build a routing system that uses a combination
of both your geographic location, your latency,
and availability to route traffic
from your users to your cloud or on-premise endpoints.
And it's basically Route 53 on acid.
It's a way of doing very highly complex routing
of your DNS traffic
across multiple assets,
across multiple regions in the world.
And you basically build it out using a GUI
like what we see off to the right.
So, you can build your traffic routing policies from scratch
or you can also actually go in
and pick a template from a library and then customize it.
So, geoproximity routing is only available
under when you're using traffic flow,
so when you've created a traffic flow routing policy.
So, geoproximity routing lets
Amazon's Route 53 route traffic
to your resources based on the geographic locations
of your users and your resources.
But you can also optionally choose to route more traffic
or less to a given resource
by specifying a value known as a bias.
So, you can set biases and say, "Okay.
Well, I want the majority
of my traffic to go to Northern America
because that's where the majority of my customers are."
But if there's latency issues or
if maybe there's an embargo or something, and you know,
you need to route customers to particular regions,
then you can do this using geoproximity routing.
So, it all gets very complicated.
And to be honest, the reason we do cover this
off is because we've covered
off every other routing policy in Route 53,
but you pretty much won't see this in your exam.
And we aren't going to go into a lot of detail as
to how to set this up because it does get extremely complex.
And it's just beyond the scope of this course.
The main thing you need to know is a bias expands
or shrinks the size of the geographic region
from which traffic is routed to resource.
So, you can have a bias in, if you look at this example
we've got a massive bias towards Europe
and a smaller bias towards, let's say, South Africa.
So, that's all biases are.
It allows you to expand
or shrink the size of the geographic region
for which the traffic is routed to in a resource.
So, let's go ahead and have a look
at how this works in the AWS console.
Okay. So, I'm in the AWS management console.
I need to go down to Route 53, which is always
under Network Content and Delivery.
Over here, we can see traffic flows.
So, we're going to go ahead and create a traffic policy
and we're going to create our traffic policy
and we'll just call it test policy.
And then we're going to go ahead and hit Next.
You don't need to enter a version description.
And in here, we've got our start point.
So DNS type, so it can do our A records,
MX records, CNAMEs, et cetera.
So, I'm going to start with an A record
and I'm going to say connect to, and then I can say, "Okay.
I want to connect to a weighted rule,
to a fail over rule, to a geolocation rule,
to a new endpoint, et cetera, et cetera."
So, we could say, "Let's connect it to a weighted rule."
And then we can start putting our weights in here.
And then we can add more connect to,
so we can have fail over rules in here.
We could have latency rules
and you can see how complicated it gets.
You can build out your own custom traffic policies based
on a whole number of different things.
You can also go ahead and hit Import Traffic Policy in here
and you can import your traffic policies using JSON.
I'm not going to do it. It gets very complicated.
And like I said we're really starting
to go beyond the scope of the exam.
So, let's go ahead and have a look at my exam tips.
So really going into the exam, you just need to
know what geoproximity routing policies are.
It lets Amazon Route 53 route traffic
to your resources based on the geographic location
of your users and resources.
And you can also optionally choose to route more traffic
or less traffic to a given resource
by specifying a value known as a bias.
And a bias basically expands or shrinks the size
of your geographic region from which traffic is routed
to a resource.
And to use geoproximity routing,
you must have Route 53 traffic flow set up.
So, we have to go in and set up traffic flow
and then you can use geoproximity routing.
That's why you never see geoproximity routing
as a record set.
When you're creating records just inside normal Route 53,
you can only do this through traffic flow.
So, that is it for this lecture, everyone.
If you've got any questions, please let me know.
If not, feel free to move on to the next lecture.
Thank you.


Demo : Using a Latency Routing Policy
======================================

Hello, Cloud Gurus.
And welcome to this lecture,
in this lecture, we're going to look
at how we can use latency-based routing policies.
So what is a latency routing policy?
We're then going to look at how we can set this up
in the console,
and then we'll look at my exam tips.
So latency routing policies allow you
to route your traffic based
on the lowest network latency for your end user.
And this will be the region that will give them
the fastest response times.
So that's all latency-based routing policies are.
So to use latency-based routing,
you create a latency-based resource record set for EC2,
or for your elastic load balancer resource
in each region that hosts your website.
And when Route 53 receives a query for your site,
it selects the latency resource records set
for the region that gives the user
the lowest latency possible.
So Route 53 then responds with the value associated
with that resource record set.
So in this case, it will be the IPv4 address for one
of our EC2 instances.
So let's have a look at this in action.
So in this example, Route 53 will send the traffic
to eu-west-2 because it has a much lower latency
for the user, the users in South Africa.
And the reason it's going to send it to eu-west-2
is because it has a lower latency than ap-southeast-2.
So their latency might be 54 milliseconds
versus 300 milliseconds.
54 milliseconds to visit eu-west-2,
where it's 300 milliseconds to visit ap-southeast-2.
Now, for the purposes of this demo,
I am currently based in London.
So it will be very interesting to see
if it's going to be quicker for me to cross the Atlantic
or go over the continent, all the way over to Japan.
I'm very interested to see what Route 53 will do.
So let's go ahead and have a look at how we can set this up
in the AWS console.
Okay, so here I am in the AWS Management Console.
Again, I want to get to Route 53.
So it's under Network Content and Delivery.
I want to go into my hosted zones.
I'm going to go ahead
and delete the last record that we created.
So we're going to delete our weightings.
So let's go in here and hit Delete, and hit Delete.
And then in here, I'm going to go ahead
and delete this record as well.
Once we've deleted it,
we need to go in and create a new record.
So let's go ahead and hit, Create Record.
Going to do it to my naked domain name.
I'm going to set my TTL to 60 seconds.
It's going to be an A record.
And instead of simple routine,
I'm going to go ahead and click on Latency-based Routing.
And here, we're going to choose our region.
So we're going to do this
for Northern Virginia to begin with.
So let's go and find Northern Virginia.
So us-east-1 in here.
I'm going to put in my IP address for Northern Virginia.
So just pasting it in there.
And I'm going to give it the optional health check
of Northern Virginia.
And in here, we can say Northern Virginia is my record ID.
So let's go ahead and create this record.
And that has now created the record.
It will take some time for DNS to propagate.
Let's go ahead and create a second record.
And in here, I'm just going to choose a TTL of 60 seconds.
I'm going to change my routing policy over to latency-based.
And in here, I'm going to choose my Asia Pacific.
And I'm going to go to northeast-1.
So it's going to Tokyo.
Grabbed my Tokyo IP address.
Pasting it in there.
Health check, I'm going to do Tokyo health check.
And then in here,
I'm just going to write to Tokyo is my record ID.
And that is now creating my record.
So in here, we can see we've got our A records.
It's using the routing policy of latency-based routing.
And the differentiator is one is going to us-east-1.
So Northern Virginia.
The other one is going to Asia Pacific in Tokyo.
I am just going to pause the video.
I'm going to wait for about 5 minutes
for DNS to propagate, even though I set a TTL to 60 seconds,
I always like to wait just a little bit longer.
And then we'll go in and test this.
Okay, so it's been about 5 minutes.
I'm going to go ahead and hit Refresh.
And that has now refreshed.
And it has sent me to Northern Virginia.
So Northern Virginia has a lower latency for me
to reach from London than it would for me to reach to Tokyo.
And if I keep refreshing,
it's just going to keep sending me to Northern Virginia.
Now I could also test this by using a VPN
and using my VPN in Tokyo, for example.
And then I would expect it to basically redirect me
to the Tokyo web server.
So let's go ahead and have a look at my exam tips.
So onto my exam tips.
And just remember what latency-based routing is.
You will get scenario questions
where you'll have a user somewhere in the world,
and you need to give them the lowest latency possible.
As soon as you see the word latency,
I want you to think of latency-based routing.
So go to our South African user.
They're in Cape Town, for example.
They're visiting our website, hellocloudgurus123.com.
Route 53 determines
that they're going to get 2 different response times.
54 milliseconds and 300 milliseconds.
54 milliseconds to eu-west-2.
Whereas it's 300 milliseconds to ap-southeast-2.
So Route 53 would then determine that the lowest latency
would be eu-west-2.
And it would redirect the user
to the web server that is there.
So that's all latency-based routing is.
Like I said, going into the exam,
if you have any scenario questions where it's talking
about lowering your users latency
or giving them the lowest latency possible,
I want you to think of latency-based routing.
So that is it for this lecture everyone.
If you've got the time, please join me in the next lecture.
Thank you.


Demo : Using a Multivalue Answer Routing policy
================================================

Hello, Cloud Gurus, and welcome to this lecture.
So congratulations, you're on the very last routing policy
and this is multivalue answer routing policy.
It actually sounds quite complex
but actually it's just simple routing with health checks.
That's all it is.
So let's go through what multivalue answer routing is,
then we'll have a look at how we set this up in the console
and then we'll go into my exam tips.
So multivalue answer routing
lets you configure Amazon Route 53
to return multiple values such as IP addresses
for your web servers in response to DNS queries,
just does the same as what simple routing does.
And you can specify multiple values for almost any record
but multivalue answer routing also allows you
to check the health of each resource.
So Route 53 returns only the values for healthy resources.
So like I said, it's basically simple routing
but with health checks.
So it's similar to simple routing,
however, it allows you to put health checks
on each record set.
And just remember
that a health check monitors the health of your endpoints.
So this is the way it works.
We've got our user,
they want to visit our website hellocloudgurus123.com.
So they visit Route 53
and then we have our first IP address
which is 30.0.0.1.
And we have our second IP address
which is 30.0.0.2.
Now if one of those IP addresses becomes unavailable,
multivalue answer routing will only send us
to the healthy ones.
So it sent us to 30.0.0.2.
It will basically not send us to 30.0.0.1.
So it's very similar to simple routing.
It's just got health checks built in.
So let's have a look
at how we do this in the AWS Management Console.
So we're going to go over to Route 53 under Network
and Content Delivery.
We're going to go to our hosted zone.
Can't remember if we have a record sets in there.
Yes, we must still do.
So we've got our geo location
and then from one of the last lectures,
what I'm going to do is just copy this into my clipboard
so I have my first IP address.
And then I'm going to just go through
and delete these 2 record sets.
So let's go ahead and delete our records.
And then I'm going to go in and create a new record.
And instead of using simple routing,
we're going to use multivalue answer routing.
So I'm going to put my first value in here.
Now, normally what we would do,
is with simple routing
we'd have just multiple IP addresses in here
and we only have one record set.
Now with multivalue answer,
we actually want to create a record for each health check.
So for each IP address essentially.
So let's drop our TTL down to 60.
In here, we're going to choose our health check
and it's going to be our Northern Virginia health check.
And in here I'm just going to type Northern Virginia.
And I'm going to go ahead and create my record set.
So that has now created my multivalue answer one
but because I want to add a second one, I can go in here.
I'm going to change my routing again to multivalue answer,
drop my TTL down to 60 seconds,
going to paste in my IP address from Tokyo in here.
And I'm going to use my Tokyo health check.
And again, I'm just going to write in my record ID Tokyo.
So that has now created
our 2 multivalue answer routing policies.
And then what we can do is just go through
and again simulate a fail over.
So if we just go over to services
and we can go over to EC2.
Let's check what region we're in.
So we're in Northern Virginia,
just go over to my security groups.
And again, we're just going to do the same thing
that we did last time.
So we're going to select our security group.
We're going to go to our inbound rules.
And in here I'm just going to go
and hit Edit Inbound Rules.
And then what I'm going to do is go through
and just delete my HTTP
for both IPv4 and IPv6.
And I'm going to go ahead and hit Save.
So that will now make my health check fail.
I can go back over to Services
and I can watch this failure happen in action.
If I just go over to Route 53,
you remember this annoying little UI issue.
You can click in here
and then you get your health checks in there.
And you can see right now it says it's healthy.
But if I just pause the video, wait a couple of minutes,
you'll see that one of them,
that Northern Virginia will become unhealthy.
So I'm going to go ahead and hit Refresh
and we can see that that instance is now unhealthy.
If I hit Refresh on hellocloudgurus123
it's just redirecting me straight to Tokyo.
It's not giving me any kind of failure or DNS failure
or anything like that.
It's not timing out.
So going into your exam, just remember
that multivalue answer routing
is essentially just simple routing, but with a health check.
So when your user sends a request to Route 53
and you've got 2 multi value answers set up in there.
So 30.0.0.1 and 30.0.0.2.
If one of them becomes unavailable,
then multivalue answer routing
will only send the DNS request
to the healthy instance or instances.
So that is it for this lecture everyone.
In the next lecture, we're just going to review everything
we've learned in this section.
So if you've got the time,
please join me in the next lecture.
Thank you.


Routing 53 Exam Tips
====================

Okay.
Hello Cloud Gurus and welcome to this lecture.
In this lecture, we're going to just review
everything we've learned in this section of the course
around Route 53.
So the first thing I'd remember going into the exam
is just understand the difference between an alias
record and a CNAME.
And given the choice in a scenario-based question
always choose an alias record over a CNAME.
Just remember the alias records are unique to AWS.
So it's a way of translating your naked domain name
to a resource.
Could be an Elastic load balancer, an S3 bucket, et cetera.
And it doesn't just have to be the naked domain name.
It could also be a sub domain.
So it could be mobile.hellocloudguru123.com, et cetera.
Whereas a CNAME only allows you to translate
a sub domain names from one to another.
So M might go M.Acloudguru.com.
It might go to mobile.Acloudguru.com, et cetera.
But like I said,
given the choice always choose an alias record
over a CNAME When you see mention of it in your exam.
Also remember going into the exam,
the common DNS record types.
So we've got our start of authority record,
we've got CNAME, we've got our NS records
or name server records
and the record that we've been using throughout
the whole part of this course is our A Records.
And A Records essentially just turn our web addresses
in to IP addresses.
Remember you have 7 different routing policies
available with Route 53
and you will need to know definitely 6 of them.
So simple routing, weighted routing, latency based routing,
failover routing, geolocation routing,
geoproximity routing.
This is using traffic flow only.
Like I said that's a bit beyond the scope of this exam.
So that's why I said you really only need to know 6,
but we cover it off just to be fully transparent
of all the different routing algorithms with Route 53.
And then we have multi value answer routing.
Also remember going into your exam
that registering a domain name.
You can buy domain names directly with AWS
and it can take up to 3 days to register.
But it just depends on the circumstances
of the domain names.
So, before we go on to all our different routing policies
and cover off what they are just in a bit more detail,
remember what health checks are.
You can set a health check on individual record sets
and if a record sets fails a health check
it's going to be removed from Route 53
until it passes the health check.
And you can even configure SNS notifications to alert you
about failed health checks.
So let's review our routing policies
and we'll start with simple routing policy.
So if you choose simple routing policy,
you can only have one record with multiple IP addresses.
And if you specify multiple values in a record,
Route 53 returns all values to the user in a random order.
So you've got our user,
they're going to hellocloudgurus123.com.
And we basically have different IP addresses
in this one record set.
So we've got 30.0.0.1 and 30.0.0.2.
And basically Route 53 is going to pick one of them randomly
and return it to to our user.
So that's all simple routing policies are they do not work
with health checks and we'll come back to that later.
Weighted routing policies.
So in this example, we've got our user.
They visit Route 53 and we want to send basically 20%
of our user base off to us-east-1
and 80% of our user base off to us-west-1.
So to do this, we basically use weighted routing.
And I like to keep it simple.
I like it all to add up to a hundred.
But essentially it would just take the weighting
and add up the sum of all the weighting
and then divide by the individual weighting by that.
And that's how it calculates the percentages.
So if you see any scenario based questions
that's talking about you want to direct a certain percentage
of your traffic to a particular region
or availability zone or whatever.
Then, I want you to think of weighted routing policies.
Moving on to latency based routing policies.
So we've got our user they're in South Africa.
They connecting into Route 53.
They get a 54 millisecond latency when connecting
into eu-west-2 or 300 millisecond latency
when connected into ap-southeast-2.
So Route 53 will give them the lowest latency possible
and then will redirect them to eu-west-2.
So if you see an exam question that's talking about latency,
I want you to think about latency routing.
That's going to be the correct answer.
Failover routing.
Now if you remember, we have our user
and basically we've built our applications
so that we've got an active passive setup.
So our active might be in us-west-1 or us-west-2.
And our passive setup might be an ap-souteast-2.
And essentially fail over routing
will have a health check on each region.
And if we lose a region, it will switch from our active site
to our passive site automatically.
So it's a way of just failingover.
So if you see a mention in the scenario based question
about how to failover from one site to another,
I want you to think of failover routing.
Next we have Geolocation routing.
So this is where we've got a group of customers.
They might be European customers
and we want to only send them to our European web servers.
It might be that those European web servers
have been configured with Euros or with the local languages.
So it might be in French or Italian.
And then we have our US-based customers.
And then they basically only want to go to our web servers
in the US.
So again you might get a scenario based question
where it's talking about how can you make sure
that a group of users in a particular location
only go to a group of web servers in a particular location.
And as soon as you hear the word location
in a scenario based question talking about Route 53,
I want you to think of Geolocation routing.
Geoproximity routing.
This basically lets Amazon Route 53 route your traffic
to resources based on the geographic location
of your users and resources.
But you can optionally choose to route more traffic
or less traffic to given resources by specifying a value
known as a bias.
And a bias basically expands or shrinks the size
of a geographic region from which traffic is routed
to a resource.
And to use geo proximity routing
you must be using Route 53 Traffic Flow.
Route 53 Traffic Flow was that GUI where we can do all kinds
of complicated routing architectures with Route 53.
And then finally, Multivalue Answer Routing.
So, what is Multivalue Answer Routing?
Well, we've got our user they're connecting
into hellocloudgurus.com or hellocloudgurus123.com.
Our first IP address is 30.0.0.1.
We then have a second record which is got another IP address
of 30.0.0.2.
And we have health checks on each record.
And so if we lose our first first record,
Route 53 will automatically just redirect all our traffic
to healthy instance at 30.0.0.2.
So that is it for this section of the course,
you've done really, really well.
Hope you enjoyed it.
And it's now time to move on to the next section.
So if you've got the time,
please join me in the next section.
Thank you.



Elastic Load Balancing (ELB)
==============================

ELB Overview
=============

Okay, hello, Cloud Gurus
and welcome to this section of the course.
In this section of the course,
we're going to look at Elastic Load Balancers.
In this lecture, it's a very quick introductory lecture.
We're going to look at what is elastic load balancing.
We're going to look
at the different types of load balancers.
We're going to explore what a health check is.
And then, we'll go on to my exam tips.
And essentially, we'll start deep diving
on the 3 different types of elastic load balancers
in the next 3 lectures.
What is an Elastic Load Balancer?
Well basically,
it automatically distributes incoming application traffic
across multiple targets, such as EC2 instances,
and this can be done across multiple availability zones.
What does that mean?
Well, essentially a load balancer
balances the load of your web servers.
As you have traffic coming in from your different users,
a load balancer will help balance that load
across different EC2 instances.
And there's 3 different types of load balancers.
And I like to think of it in terms of cars.
You've got your application load balancer.
This is best suited for load balancing
of HTTP and HTTPS traffic.
It operates at Layer 7 and it's application-aware,
and this is an intelligent load balancer.
Think of a thing like a Tesla,
maybe a Model 3 or a Model S or Model X,
but essentially, it's intelligent load balancing.
Now, if you need extreme performance,
think of a network load balancer as a Lamborghini.
It operates at the connection level, so at Layer 4,
and network load balancers are capable
of handling millions of requests per second,
while maintaining ultra low latencies.
This is your performance load balancer.
And then we have our classic load balancer,
and these are our legacy load balancers.
And you can load balance HTTP and HTTPS applications
and use Layer 7-specific features,
such as your X Forwarded and sticky sessions,
and we'll cover off what that means in another lecture.
Basically, this is your classic load balancer.
It's what you would use for test and dev, et cetera.
When you're going into production,
you probably want to either use an application load balancer
or a network load balancer.
The classic load balancers have been around
for almost as long as AWS has been.
And so, a lot of devs do use them
for test and dev environments.
Now that we know the 3 different types
of load balancers, let's move on to health checks.
Just remember that all AWS load balancers
can be configured with health checks,
and a health check is exactly what it sounds like.
It basically periodically sends requests
to the load balancers' registered instances
to test their status.
Essentially it's just querying the EC2 instances
that are behind the load balancer.
And what it's doing is checking
that they're still up and running.
And the status of the instances that are unhealthy
at the time of the health check is InService.
Now, the status of any instances that are unhealthy
at the time of the health check is OutOfService.
And the load balancer performs health checks
on all registered instances,
and whether the instance is in a healthy state
or an unhealthy state.
The load balancer routes requests only
to the healthy instances.
If an instance becomes unhealthy, then basically,
the load balancer is going to stop sending requests to it.
Just remember that going into your exams.
And we're going to use health checks quite a bit
in this section of the course.
Now, the load balancer will resume routing requests
to the instance when it has been restored
to a healthy state.
On to my exam tips, just remember
the 3 different elastic load balancer types.
We've got application load balancers.
Remember that these are intelligent.
We've got network load balancers.
Remember that these would be used
where you need extreme performance.
And then we have our classic load balancers.
And this is basically for things
like test and dev environments.
Now that we know the 3 different load balancers,
in the next few lectures,
we're going to deep dive into each individual one
and I'll show you how to provision them.
Just also remember going into your exam
what health checks are.
You can use a health check to route traffic
to your instances or targets that are healthy.
That's it for this lecture, everyone.
If you have any questions, please let me know.
If not, feel free to move on to the next lecture.
Thank you.


Using Application Load Balancers
=================================

Okay, hello, Cloud Gurus
and welcome to this lecture.
In this lecture we're going to look at how we can use
our Application Load Balancers.
Now, if you remember from the last lecture,
Application Load Balancers are Layer 7 aware,
they're intelligent Load Balancers.
So again, I use the example of like a Tesla, for example.
So let's go and have a look
at what application aware Load Balancing means,
so Layer 7 Load Balancing.
We'll then look at listeners, rules, and target groups.
We'll then talk about path-based routing
and then we'll move on to an Application Load Balancer
diagram, and then we'll do the limitations
of our Application Load Balancers.
So what can't they do?
Then we'll go into the AWS Console
and do a demo of how to set up an Application Load Balancer
and then we'll move on to my exam tips.
So what is a Layer 7 Load Balancer?
Well, an Application Load Balancer functions
at the application layer, and this is the 7th layer
of a system that's called the Open Systems Interconnection
or OSI model,
and I would really highly recommend that you go
and review what the OSI model is
before you go through any job interview
if you're going to try and get a job interview
or a job as a tech,
they'll always ask you what layer does this operate on?
What layer does that operate on?
But essentially the application layer is Layer 7
in the model,
and basically after the Load Balancer receives a request,
it evaluates the listener rules in priority order
to determine which rule to apply
and then it's going to select a target
from the target group for the rule action.
So listeners basically check for connection requests
coming from clients using the protocol
and port that you configure.
So obviously the first port that we're going to configure
is port 80 because that's what we're going to be using
to browse to our web servers.
You could also configure port 443.
We're not going to do 443 in this lecture
because we'd need to go and install SSL certificates
and we'd have to set up domain names, et cetera.
So we're just going to keep it simple and do everything
on port 80.
But basically you define the rules that determine
how the Load Balancer routes requests
to its registered targets.
And each rule consists of a priority, 1 or more actions
and 1 or more conditions,
and don't worry if this isn't making a lot of sense
right now, as soon as we go into the Console
and I show you how it works, it'll make a lot more sense.
So essentially when the conditions for a rule are met,
then its actions are performed.
So it's a bit like, if then statements,
essentially, that's all it is.
So you could say like, if you get a request
from this IP address,
then route that request to this target group.
So like I said, it will make a lot more sense
when we get into the Console,
but rules are basically when the conditions for rule
are met, then its actions are going to be performed,
and you must define a default rule for each listener,
and then you can optionally define additional rules as well.
And so your target groups basically route requests
to 1 or more registered targets.
So you could have a target group for your web servers
or you could have a target group for your media servers
which might have all your images on it
or could have a target group
for your database servers, et cetera.
And basically the registered targets
are just going to be EC2 Instances
that sit within that target group
and then you're going to be using the protocol
and port number you specify.
So if it's a web server, it's going to be going in
over port 80, if it's MySQL,
then it's going to be going over port 3306.
So here is a network diagram of what it looks like.
So we've got our Application Load Balancer at the top
and then we have our different listeners.
So perhaps 1 listener is for port 80
and another listener is for port 443.
And then we've got some rules that are attached
to our listeners.
And on the first 1, we've got just 1 rule,
which is basically just send all of port 80 traffic
to our web server group,
and then on the other listener,
we could have multiple rules.
So when port 80 traffic comes in on our listener
and we've just had that 1 rule,
it's going to send all our traffic to our targets
inside our target group
and we're going to have a health check attached to it.
With port 443, we could have multiple rules in there.
We could have a rule that targets a specific IP address.
So perhaps you've got a 1 end customer
who has a static IP address
and you need to send them to their internet.
Well, that will be a completely separate target group
to some other customers perhaps,
who you need to forward to another target group.
So you can have multiple rules on your listeners
and then you can basically intelligently go in
and balance the load between different target groups
and different targets
and you can check them all the instances,
or all the targets using a health check.
So let's move on to path based routing
which is a common scenario in your exam.
So let's say we've got our user and they're browsing
to a URL that hits route 53 and that's sending traffic
to an Application Load Balancer, and we've got my url.com.
So that's the URL that they're visiting,
the Application Load Balancer needs to send traffic
to the web servers in us-east-1
but if they've got a path that says my url.com/images,
then the Application Load Balancer needs to send it
to some other EC2 Instances in another target group
in another availability zone.
So to enable that, basically we just enable path patterns.
So your Load Balancer is intelligent.
It can actually look at the pathway
and it can make routing decisions
or Load Balancing decisions based on that path.
So the my url.com will go to all your web servers
in us-east-1a, however, you might have media servers,
us-east-1b that have much bigger capacity
because you've got all your images and videos on there,
so the Application Load Balancer will then you route traffic
to my url.com/images to your media servers in us-east-1b.
So that's a very common scenario based question.
Just remember that Application Load Balancers
are Layer 7 aware and you can do intelligent routing
based on your paths and to do that,
you just need to enable path patterns.
Let's just talk about 1 last thing
which is the limitations of Application Load Balancers.
So essentially an Application Load Balancer
only supports HTTP and HTTPS,
it doesn't support any other listeners.
So moving on to HTTPS, this is the last thing
before we go onto our demo,
to use the HTTPS listener, you must deploy at least 1 SSL
or TLS service certificate on your Load Balancer
and the Load Balancer uses a service certificate
to terminate the frontend connection
and then decrypt requests from clients
before sending them to the target.
So the actual decryption is done
on the Load Balancer itself.
Now, if you use Amazon's Route 53 to register
your own domain name, Amazon make all this very,
very simple for you.
They will essentially generate the SSL certificate for you.
They are a certificate signing authority
and then you can add that to your Application Load Balancer.
I'm going to keep the demo really simple.
We're just going to use HTTP, but feel free,
if you want to go register your own domain name,
see if you can set up HTTPS
on your Application Load Balancer or on your own.
So let's go ahead to the AWS Console.
Okay, so here I am in the AWS Management Console
the first thing we're going to do
is we're going to go over to EC2
and we're going to create 3 web servers.
So let's go into EC2, we're going to go ahead
and launch an instance and I'm going to use an Amazon Linux
2 AMI, and then I'm just going to use my t2.micro.
And in here, we're going to use our default VPC.
So all we are going to do
is I'm going to put each of our 3 web servers
in different availability zones.
I'm actually in Ireland at the moment,
just so I can use 3 availability zones
to keep it simple.
So I'm going to put the first 1 in eu-west-1a,
down here I'm going to use a bootstrap script
which is in the resources section of the course,
but essentially all it's going to do
is it's going to go ahead and run a yum update.
So it's going to apply our security patches,
it's going to install Apache, start Apache,
and then it's going to create a webpage
saying, Hello, Cloud Gurus, this is web server 1
and output that to index.html.
And what I want you to do as we go through this,
I'm not going to do, show you how to do this 3 times,
but basically as you provision web server 2,
just remember to change that to web server 2
and then to web server 3.
So then I'm going to go ahead and add our storage.
I'm going to leave it as all the default options.
And in here, I'm just going to do name and then web server
and this will be web server 1, and I'm
going to go in and select my existing security group,
so I'm going to use a Web-DMZ security group
and then I'm going to go ahead and hit Launch
and use my existing key pair.
Now I'm going to go through and do this
with another 2 EC2 instances.
I don't really want to waste your time
because it's just repetitive,
so I'm just going to pause the video
and wait until we have 3 web servers up online.
Okay, so we can see now that web server 1,
web server 2 and web server 3 is online
and I've actually just clicked in
and just tested each public IP address
and made sure that they're actually are resolving
and they, all 3 of them work.
So the next thing we need to do is go ahead
and create our Load Balancers
and you'll be able to find Load Balancers down here
under Load Balancing and there's our target groups.
So let's go ahead and create our Load Balancer first.
So we're going to go in and hit Create Load Balancer.
Now see here, we've got 4 different types
of Load Balancers, so we didn't cover off
Gateway Load Balancers, and basically,
it's quite an obscure Load Balancer,
it's layer 3 Load Balancer
of third-party virtual appliances.
It does not come up in the exam at all,
and really you just need to know
Application Load Balancers, Network Load Balancers,
and Classic Load Balancers going into your exam.
So here we've got our Application Load Balancers
as our first option.
Let's go ahead and hit Create.
Now in here, we're going to give it a name,
so I'm going to call it my ALB, Application Load Balancer.
It's going to be internet facing.
You could have it internal,
so you could have it Load Balancing just to private subnets
but I'm going to do it as internet facing.
In here we've got whether we're going to just use IPV4
or IPv4 and IPv6.
Here, we've got our listeners.
So if we go and add our listener,
you can see that you only have 2 options, HTTP and HTTPS.
Now I'm not going to choose HTTPS,
because I don't want to have to faff around
with SSL certificates, so I'm just going to do HTTP.
In here we've got our availability zones,
so what availability zones are we going to enable
for our Application Load Balancer?
And which VPC do we want to deploy it into?
Well, I'm going to obviously put it into all 3
availability zones within eu-west-1.
And then here, we've got our add on services,
so you can add on AWS Global Accelerator.
We will cover that off in the caching section of the course,
right now, we're just going to leave it.
And so we're going to go ahead and configure
our security groups.
In here we get a warning message just saying,
Hey, you're not using HTTPS, what are you doing?
Don't worry about that.
Let's go ahead and hit Next
and in here we're going to put our Load Balancer
into our Web-DMZ security group
and in here we've got our target groups.
So let's go and create a new target group,
and we're going to call this target group, Web Servers.
So all of the instances in our web servers target group,
this is where we're going to send our traffic.
It's going to be over port 80,
and then down here we can do our health checks.
Now, what I like to do is reduce this
to the minimum possible amount.
So basically you do 3 for your healthy threshold,
two for your unhealthy threshold, I think it's 2,
and I think this is 3.
Let's just have a quick look and no,
your interval has to be between 5 and 300 seconds.
So make that a 5 and there we go,
so it will basically pass our health checks in record time.
In here we need to go in and register our targets,
so if you just click here and go to Add Registered Targets,
it's going to register these targets to port 80.
So it's our targets in eu-west-1, eu-west-1a,
1b, and then 1c.
So let's go ahead and hit Next to review
and let's go ahead and create our Load Balancer.
and that is now successfully creating
our Application Load Balancer.
You can see in here that the state is provisioning,
and then if we just go over to our target groups,
we'll be able to see our web servers target groups in here.
And you can actually go down and have a look
at your targets.
So these are the registered targets
and right now we don't have a status
because we're doing the target registration,
but it's basically going to be performing health checks
against our targets,
and so we'll just have to wait for this to come up online.
So I'm just going to go back to Load Balancers.
I'm going to pause the video.
I'm going to wait for this state to go from provisioning
so that it's actually provisioned.
Okay, so we can see that my status has now changed to active
and I've got a DNS address for my Elastic Load Balancer.
So you can actually see it down here.
So I'm going to copy that into my clipboard.
Before I go and resolve that DNS address,
what I want to do is just go over to my target groups.
We can see my target group is in here, and if I click on it,
we'll be able to see my different targets.
So I've got 3 targets, web server 1,
web server 2 and web server 3,
all in 3 different availability zones.
Up here we can see that we've got 3 healthy instances
and 0 unhealthy instances.
So let's go ahead and resolve that DNS address.
Okay, so I've resolved the DNS address.
We've got Hello, Cloud Gurus. This is web server 3.
If I hit refresh, it changes to 2, 3,
1, 2, 3, et cetera, et cetera.
So you can see that it is Load Balancing equally
across my 3 different web servers.
So you might be wondering how can we actually set rules
which we discussed earlier on in this lesson?
Well, it can be a little bit difficult to find
but you basically have to go down to listeners.
in here, We've got our listeners,
so we've got HTTP and we can also add HTTPS. So I'm not
going to do that because I don't want to have to go
manage some SSL certificates,
but in here you'll be able to see View Edit Rules.
And it will bring you up basically a little GUI like this.
So you can see here, basically you've got HTTP 80,
so it's listening on port 80.
If requests otherwise not rooted
then forward to web servers.
So it's going to forward it to our web servers.
We can actually go in and add additional rules
and the rules will be above this.
So basically this is the default action.
So if a request comes in then it's going to route it
to our web servers,
we can actually go in here and add different conditions.
So we could say, Hey, if there's a query string
or if there's a path or a source IP, then add this action.
We can forward to, redirect, or return to a fixed response.
And you can actually go ahead
and create complicated rules structures
within your Application Load Balancer.
And that's what we talk about when we say it's intelligent
or Layer 7 aware, you can create very intelligent
and complex routing instructions inside your Load Balancer.
So onto my exam tips,
just remember going into your exam about your listeners
are listener checks for connection requests from clients
using the protocol and port you configure.
So that's either going to be port 80 or port 443.
Rules determine how the Load Balancer routes requests
to its registered targets,
so just looked at that.
So you could do it either on your host header,
you could do it on an IP address,
you could do it on the path,
you could do it on query string, et cetera, et cetera.
And then we have our target groups
and each target group routes request to 1
or more registered targets,
so this will be our EC2 Instances,
using the protocol and the port number that you specify.
Just remember with Application Load Balancers,
you are limited to HTTP and HTTPS
and to use the HTTPS listener
you must deploy at least 1 SSL
or TLS service certificate on your Load Balancer
and the Load Balancer uses a server certificate
to terminate the frontend connection
and then decrypt requests from clients
before sending them to targets.
So the actual decryption is done on the Load Balancer itself
and then the traffic is sent to your EC2 Instances.
So that is it for this lecture everyone.
If you have any questions, please let me know,
if not, feel free to move on to the next lecture.
Thank you.

Extreme Performance with Network Load Balancers
================================================

Okay. Hello, Cloud Gurus.
And welcome to this lecture. In this lecture,
we're going to look at Extreme Performance
with Network Load balancers.
So again, I use the analogy of a Lamborghini
but essentially,
if you want a layer 4 connection
or you need thousands or even tens of thousands
of concurrent connections you probably
want to look at a Network Load Balancer.
So cover off what layer 4 load balancing is,
we'll then talk about Request Received,
Listeners, and Target Groups.
Then talk about different Ports and Protocols.
We'll look at our different Use Cases
for a Network Load Balancer.
We'll look at Encryption
and then we'll just go onto my exam tips.
So Layer 4 load Balancing.
Basically a Network Load Balancer
of functions that the fourth layer
of the Open Systems Interconnection or OSI model
and it can handle
millions of requests per second.
So let's talk about how
a Network Load balancer Works.
Basically,
after the load balancer receives
a connection request,
it's going to select a target
from the target group for the default rule.
And it's going to attempt
to open a TCP connection
to the selected target on the port specified
in the listener configuration.
And we'll have a look at
what to ports are supported in one second.
But essentially, a listener checks
for connection requests from clients
using the protocols
and the port that you specify.
And the listener on a Network Load Balancer then
forwards the request to the target group.
So there are no rules.
Unlike with Application Load Balancers
you can't do intelligent based routing.
If you want to do intelligence based routing,
you need a layer 7 Load Balancer.
However, we do have target groups
just like with Application Load Balancers
and each target group
then routes requests to one or more
registered targets such as EC2 instances
using the protocol
and the port number you specify.
So what are the protocols
and ports that are supported?
Well, the protocols that are supported
are TCP, TLS, UDP, and TCP_UDP,
and the Network Load Balancer supports
ports from 1-65535.
So basically, any port and protocol.
And then finally moving on to Encryption.
So you can use a TLS listener to offload the work
of encryption and decryption
to your load balancer
so your applications can focus
on their business logic.
So in other words, your EC2 instances don't
need to worry about encryption or decryption.
But if the listener protocol is TLS
you must deploy exactly one SSL certificate
on the listener.
So going into your exam,
just remember the use cases
for a Network Load Balancer.
So they're best suited for load balancing
of TCP traffic
where extreme performance is required.
They're operating at the connection level.
So operating at layer 4.
And network Load balancers are capable
of handling millions of requests per second
while maintaining ultra low latencies.
And you basically want to use
a Network Load Balancer for extreme performance.
So just remember these four tips
going into your exam.
I've already covered off two of them.
So Network Load Balancers operate at layer 4,
use where you need extreme performance,
and then other use cases
are where you need protocols
that are not supported
by Application Load Balancers.
So basically anything that's not port 80
or 443.
And Network Load balancers can decrypt traffic
but you will need to install the certificate
on the Load Balancer itself.
That is it for this lecture everyone.
If you have any questions, please let me know
If not, feel free to move on to the next lecture.
Thank you.


Using Classic Load Balancers
==============================

Okay, hello, Cloud Gurus,
and welcome to this lecture.
In this lecture we're going to look
at using Classic Load Balancers.
So explore what a Classic Load Balancers is,
we'll then look at the X-Forwarded-for header,
and we'll do a network diagram as well.
Then look at gateway timeouts,
and we'll then go on and do a demo in the console
so you can see how to set up a classic load balancer
and then we will move on to my exam tips.
So, classic load balances are basically
the legacy load balancers,
and you can load balance HTTP and HTTPS applications.
And then you can also use layer 7 specific features
such as X-Forwarded for and sticky sessions.
You can also use strict layer 4 load balancing
for applications that rely purely on the TCP protocol.
So let's explore what an X-Forwarded-For header is.
Essentially when traffic is sent from a load balancer,
the server access logs contain the IP address
of the proxy or of the load balancer only.
So when your EC2 instances are basically logging
all the data that's coming in from your load balancer,
it's actually just getting the internal IP address
of the load balancers.
You don't know where this traffic is coming from,
and to see the original IP address of the client,
the X-Forwarded-For request header is used.
So let's have a quick look at how this works.
Here's our user, they've got an external IP address
of 124.12.3.231,
and that might be an IP address
the US on the Eastern seaboard or something.
You want to collect that data
because you need to know where your customers
are coming from.
They then browse to your website
and they hit your classic load balancer.
And the classic load balancer has an internal IP address
of 10.0.0.23.
And so what happens is, as your EC2 instance
that acts as a web server in the logs,
it's just going to see the classic load balancer's
internal IP address.
And you're not going to be able to see
where your user is coming from.
But you can find it in the X-Forwarded-For header.
So that's where you look for the external IP address
of your users.
And this can be a very popular exam topic
that comes up on classic load balancers.
So just remember if you need an external IP address
for your end user, you're going to do it
in the X-Forwarded-For header.
Moving on to gateway timeouts.
If your application stops responding,
the classic load balancer will respond with a 504 error.
And this basically means
either your web server is having an issue,
or your database servers having an issue.
But essentially it means
that the load balancer is up and running
but it can't make a connection or something is going wrong
to the things that are behind it.
So again, if you see an error 504 as a exam,
a scenario based question,
you need to go in and troubleshoot either the web layer
or the database layer of your applications.
So let's go ahead and provision a classic load balancer
in the AWS console.
Okay, so here I am in the AWS management console
to provision my classic load balancer,
I just needs to go over to EC2 under Compute,
and then of course we go over to load balancers,
and in here I'm going to create a new load balancer
and we're going to create a classic load balancer
and you can see it's very clear
that it says previous generation,
so let's go ahead and hit Create.
And in here we just have a basic wizard,
so we need to give our load balancer a name,
so call it my classic load balancer, something like that.
We're going to create it within our default VPC.
We can create internal load balancer,
that's essentially just means
that it's in a private subnet
and it's essentially just load balancing
to other EC2 instances that are in a private subnet as well,
so it's not internet facing.
We're not going to do that,
and in here, we've got our listener configuration
so we can go in and add different listeners.
So we're listening in on port 80,
we can choose a different protocols so we could do HTTPS,
we could do TCP, or we could do SSL. I'm not
going to do that, I'm just going to leave it as HTTP,
and then what we're going to do is go ahead
and assign our security groups
going to put this in my web DMZ security group.
And then I'm going to go ahead and just hit Next,
and in here we can configure our health checks.
I'm going to leave everything as default
and it's just going to go to our index.HTMLdirectory.
And now we go in and we add our EC2 instances
so we can see I've still got my 3 EC2 instances in here.
I'm going to click on that and enable them,
in here we say enable cross zone load balancing.
This is essentially allows us to load balance
across multiple availability zones.
And in here we've got enable connection draining
and this is simply the number of seconds
to allow existing traffic to keep continue flowing.
So let's go ahead and add our tags,
I don't need any tags,
I'm just going to go hit Review and Create,
and now I'm going to go ahead and create my load balancer.
So that is now creating,
I'm going to pause the video
and wait for the state to come up online
and our instances to pass their health checks.
Right now you can say that it says status out of service
because it hasn't passed the health check just yet,
but it will do so in a couple of minutes,
so I'm just going to pause the video.
Okay, we can now see that the status has changed
to in-service, so I've got all my 3 instances
passing the health check.
And if I just go over to my description,
I can grab the DNS name of my load balancer
and I can copy that into my clipboard
and then I'll be able to browse to that in my browser.
So if I browse to that in my browser,
if I repetitively refresh,
you can see that it's going across all 3 EC2 instances.
So as you can see,
classic load balances are super easy to set up
and that's probably why a lot of people
just use them for Test and Dev purposes.
So let's go onto my exam tips.
Going into your exam, just remember that a 504 error
means that the gateway has timed out.
And this basically means
that the application is not responding
within the idle timeout period.
And you basically just have to go in
and troubleshoot the application.
Is it the web server or is it a database server,
what's going on?
But it's not going to be your classic load balancer,
so 504 error just simply means your application
has stopped working.
And then the other thing you need to remember
going into your exam
is just if you need the IPv4 address of your end user,
you need to look for the X-Forwarded-For header.
So that is it for this lecture everyone,
if you have any questions, please let me know.
If not, feel free to move on to the next lecture, thank you.


Getting "Stuck" with Sticky Sessions
=====================================

Okay. Hello, Cloud Gurus.
And welcome to this lecture.
This is a really quick lecture
but it does come up in your exam quite often.
And it's all about sticky sessions.
So we'll look at what sticky sessions are.
We'll look at a network diagram as to how it works.
We'll then go onto my exam tips.
So what are sticky sessions?
Well basically a classic Load balancer routes
each request independently to the registered EC2 instance.
with the smallest load
and sticky sessions allow you to bind a user's session
to a specific EC2 instance.
So it could be
that you're saving data locally on that EC2 instance.
And you basically, always want to make sure when
that user is visiting the website
they're always going to the same EC2 instance
because that's where the data is being saved.
Now, this ensures all requests from the user
during the session are sent to the same instance
that's all a sticky session is.
It's sticking it to that individual EC2 instance.
So where can this cause problems?
Well, let's say we've got our user
and they're connecting into our classic load balancer
and that's connecting into a specific EC2 instance.
Let's say it's the bottom one.
Now what happens if we remove
that EC2 instance from the elastic load balancing pool?
Well, it means that the load balancer is still
going to route the request of our end user
to that EC2 instance,
but it's no longer there.
Let's say we terminate the instance or something
and then our users are going to get an error.
So the way to stop that is we just need to
disable sticky sessions.
So that's the way we stop it.
Now we can use a sticky sessions
with application load balancers as well.
So you can enable sticky sessions
for your application load balancer
but the traffic is going to be sent
at the target group level.
So if you've got multiple EC2 instances
in that target group
it will go to other EC2 instances.
So that's all it is.
It's really, really simple.
Just remember going into your exam
what sticky sessions are, they enable your users to stick
to the same EC2 instance and it can be useful
if you're storing information locally to that instance.
You may see a scenario based question where
you remove an EC2 instance from a pool
but the load balancer continues to direct traffic
to that EC2 instance, and to solve a scenario like this
you simply disable sticky sessions.
So that's it for this lecture everyone.
If you have any questions, please let me know.
If not, feel free to move on to the next lecture.
Thank you.


Leaving the Load Balancer with Deregistration Delay
=======================================================

Okay, hello, Cloud Gurus,
and welcome to this lecture.
In this lecture, we're going to look
at Leaving the Load Balancer with Deregistration Delay.
With classic load balancers,
it's actually called Connection Draining
but it's all the same thing.
So we'll look at what deregistration delay is,
we'll then look at how we can do this in the console,
and then we'll go onto my exam tips.
So what is Deregistration Delay or Connection Draining?
Well basically it allows Load Balancers
to keep existing connections open
if the EC2 instances are deregistered or become unhealthy.
And this enables the load balancer
to complete in-flight requests made to the instances
that are deregistered or unhealthy.
So if you go in and deregister EC2 instance,
deregistration delay will just keep that connection open
for people who are already using that EC2 instance.
Now you can disable deregistration delay
if you want your load balancer
to immediately close the connections
to the instances that are deregistered
or have become unhealthy.
So let's have a look at how we can do this
in the AWS console.
Okay, so here I am in the AWS console,
I've got my classic load balancer
and my application load balancer.
With my classic load balancer, it's super simple,
you just click on Instances and you can see here,
we've got Connection Draining,
it's the same thing as deregistration delay.
You can go in and hit Edit,
and then it basically uncheck this checkbox
and that will disable connection draining,
so it's super, super simple.
With application load balancers,
it's a little bit more complex.
Instead of going into the application load balancer itself,
you need to go over to your Target Groups,
and then under Target Groups,
you need to go to your Attributes
and then you'll be able to hit in Edit,
and then you can see deregistration delay in there.
To disable it, just do 0 seconds.
So just onto my exam tips,
you will probably see a scenario question
where it's talking about deregistration delay
or connection draining,
just remember to enable deregistration delay,
it's going to keep existing connections open
if the EC2 instance becomes unhealthy.
And if you disable deregistration delay,
you're going to do this if you want your load balancer
to immediately close the connections to the instances
that are deregistered or have become unhealthy.
So that is it for this lecture, everyone.
In the next lecture, we're just going to review
everything that we have learnt
in this section of the course,
and then you're free to move on to the next section.


ELB Exam Tips
===============

Okay, hello Cloud Gurus
and welcome to this lecture.
In this lecture, we're just going to review
everything that you have learned in this section
of the course.
So, going into your exam,
just remember the different Elastic Load Balancer types.
There's 4, but we're just going to focus on the 3
that come up in the exam.
So, Application Load Balancers, which is Layer 7.
We've got Network Load Balancers, which are Layer 4.
And we've got Classic Load Balancers,
which operate between Layer 4 and Layer 7.
Remember what a health check is.
Basically, you can use health checks to route your traffic
to instances or targets that are healthy.
Remember, with the Application Load Balancers
just remember what a listener is.
A listener checks for connection requests
from clients using the protocol
and the port that you configure.
So, it's either going to be port 80 or port 443.
And then you have rules,
and these determine how the load balancer
routes your request to its registered target.
And each rule consists of a priority,
one or more actions, and one or more conditions.
And then we have our target groups.
And our target groups,
basically each target group routes requests
to one or more registered targets,
such as our EC2 instances,
using the protocol and port numbers that you specify.
Remember that there is a limitation
to Application Load Balancers,
it does only support HTTP and HTTPS.
And to use an HTTPS listener,
you must deploy at least one SSL or TLS
server certificate on your load balancer.
And the load balancer uses the server certificate
to terminate the frontend connection
and decrypt the requests from your clients
before sending them to the targets.
Moving on to Network Load Balancers.
Remember they operate at Layer 4.
You're going to use them when you need extreme performance.
Other use cases are where you need protocols
that are not supported by Application Load Balancers.
And Network Load Balancers can also decrypt your traffic,
but you're going to need to install
the certificate on the load balancer.
Moving on to Classic Load Balancers.
Remember what a 504 error means.
It just means that the gateway has timed out.
And this means your application is not responding
within the idle timeout period.
And to basically solve this,
you just need to troubleshoot the application.
Is your web server down
or is it your database server, et cetera?
And remember if you need the IPv4 address of your end user,
you're going to be looking for the X-Forwarded-For header.
Moving on to sticky sessions.
So, just remember what sticky sessions are.
They enable your users to stick to the same EC2 instance.
And this can be useful
if you're storing information locally to that instance.
And you may see scenario based questions
where you remove an EC2 instance from a pool
that the load balancer continues to direct traffic
to that EC2 instance.
In a scenario like that,
you just need to disable sticky sessions.
Remember the Application Load Balancers
also have sticky sessions, so you can enable sticky sessions
for your Application Load Balancer,
but it's going to be sent to the target group level,
not to to the individual EC2 instances.
In that target group,
you do want it to go to individual EC2 instances
then you basically just need
one target group per EC2 instance.
Also remember deregistration delay or connection draining,
depending on if it's an Application Load Balancer
or a Classic Load Balancer,
they're exactly the same thing.
So, you basically want to enable deregistration delay
and this will keep existing connections open
if the EC2 instance becomes unhealthy.
Or you could choose to disable deregistration delay
or connection draining, and you do this
if you want your load balancer to immediately
close connections to the instances that deregistered
or that have become unhealthy.
So, that is it for this lecture.
And this is the end of the load balancing section.
So, if you've got the time,
I'll see you in the next section.


Monitoring 
===========

Cloudwatch Overview
===================

Hello, Cloud Gurus, and welcome back.
In this lesson, we're going to be taking a look at
what is CloudWatch,
and why would we want to use it
inside of our AWS environment?
We're going to start off with a very important question.
Namely, why do we monitor?
And what is the point of building out
all of these alarms and monitoring metrics?
We'll define what exactly is CloudWatch.
And then we'll take a look at some
of the features that we need to be aware of.
From there, we'll see the different kinds of metrics
that CloudWatch can monitor for us,
before we jump into that console and get to see it hands-on.
And of course, we'll wrap it up
with some of those handy-dandy exam tips.
So let's go ahead and dive right on in.
So for our very first question,
we have to ask ourselves, why do we monitor?
Well we monitor for the same reason
that if you were driving this car you
would probably want to make sure that your brakes worked,
and that the gas pedal was functioning,
and that the steering wheel worked.
Because as you approach that cliff,
that's not the time that you want to find out
that things are broken.
If we don't monitor, if we don't create alarms,
if we don't pay attention to our application,
if we don't make sure that that car is working,
we're only going to figure it out when it's too late.
So let's avoid anything catching fire.
Let's avoid our application breaking down.
We want to create alarms.
We want to create checks
to make sure that we catch those warning signs
before catastrophe strikes.
Now that was a little bit dramatic,
so let's take a look here at CloudWatch itself.
Our official definition is that CloudWatch
is a monitoring and observability platform
that was designed to give us insight
into our AWS architecture.
The whole point of this is,
it allows us to identify potential issues
before they become bigger problems.
So I can take a look at trends inside of my environment,
and say, hmm, why is the CPU utilization elevated?
I'm going to go investigate this,
rather than just getting paged
because your application is offline,
and you're customers are frustrated.
So it's about collecting data,
taking a look at that data, analyzing it,
and then identifying the things that you need to watch for.
So we have to have monitoring. Great,
but what exactly can CloudWatch do for us?
Well, the first thing it can do
is collect system-level metrics.
System-level metrics just tell us,
is there a problem with the service?
Is there a problem with the operating system?
Am I using too many resources inside of this architecture?
This could be something like my CPU is too high
on my EC2 instances,
or my memory utilization is too much on my RDS database.
Now the more managed the service is,
the more metrics we get out of the box
without doing anything.
For example, with RDS,
since AWS has taken over that operating system
and the application,
it has to give us more details
because we can't get access to those things,
to install an agent to check them for us.
Now, application-level metrics,
these are generally going to require
that you install the CloudWatch agent
inside of your EC2 instance,
to get this information into CloudWatch.
Now an application metric could be,
is my Apache process running on that EC2 instance?
And if it isn't,
well we should probably do something about it.
This gives us insight
into how was the application performing
or what's going on inside of the operating system,
rather than focusing on, maybe, my resource utilization
with those system metrics.
So it's great to have metrics,
but what's the point of collecting them?
Well, to create alarms, of course.
If we collect all of this data
but we don't do anything with it,
what's the point of collecting it.
An alarm is here to tell us when something goes wrong.
So we can catch, for example,
those breaks on the car not working
before we drive over a cliff.
We can solve the problem before it gets a whole lot bigger.
So we have 2 kinds of metrics
that you're going to see in CloudWatch.
We have the default metrics,
like I said these come with every service,
at least at some level,
these are things that AWS can see,
and just tells you about
without any additional configuration.
And then we have those custom metrics.
This is where that CloudWatch agent
has to be installed into that operating system
to generate a data point and feed it up into AWS.
So let's see the difference between the 2 here.
The first one I have is CPU utilization.
What do you think?
Do you get this out of the box, or is that custom?
Well, it's actually a default metric
because AWS can see not what your CPU is doing,
because that would be a huge security problem,
but it can see how much CPU your instance is using,
your resources are using.
So it can tell you that.
It can graph it without you having to do anything.
How about network throughput?
Default or custom?
Well, if you said default, you're correct.
AWS cannot see the networking traffic
that you're sending,
because that'd be--once again--a big problem,
but it can see how much bandwidth are you consuming,
how much data are you sending through those pipes?
And it can give you that metric in CloudWatch
out of the box.
Now what about EC2 memory utilization?
It's kind of surprising,
but the answer is actually custom,
and this tricks a lot of students up
when they're taking that exam.
So it's a good point to remember.
Amazon only gives you that memory to use,
and it says, "Cool, you do whatever you want with it."
Amazon cannot actually see what's going on
inside of that memory,
unless you install the CloudWatch agent
and report that memory utilization back to CloudWatch
as a custom metric.
What about that storage capacity for EBS?
That follows a similar principle
as our EC2 memory utilization.
Amazon hands you a chunk of storage
and says, "Do with this what you want,
"that's it, it's your problem now."
So you have to have that agent installed
to read how much are you using inside of that volume
and then report that data point back to CloudWatch.
Alright, well,
it's great to just talk about this in theory,
but let's see this hands-on inside of the console.
Alright, folks.
So as you can see, I've got my console open.
Now, if you'd like to follow along at home,
the only thing that I've done before this demo
is I have an EC2 instance that I've already created.
And I have an SNS topic that I've built out
to handle notifications when the alarm state changes.
Now we'll talk much more about SNS in an upcoming lecture
but for now, let's go ahead and create our very first alarm.
So I don't have any right now.
So I'm just going to go ahead
and click up here to the Create Alarm button.
Now it's going to ask me to select a metric
or metrics that I want to alarm off of.
Now, in this case here,
I have a lot of resources available to me,
a lot of default metrics,
I could use DynamoDB, S3, EBS.
I want to go in here and select EC2,
and I'm going to get my standard metrics--
the default metrics--that are provided to me.
Notice here, I can use the CPU credit balance
since this is a T class instance,
EBS, write operations, read operations.
There's a whole lot of metrics available to me
just out of the box.
You don't have to memorize all of these for the exam,
but you do have to have a general idea
of is it standard, is it default or is it user-provided?
Namely, do I have to have that agent installed?
So I'd really recommend that you open this up
and read through this a little bit yourself,
because we can't cover all of the possibilities together.
Now I do want to scroll up here to status check failed.
Now remember that status check
that 2/2 check for an EC2 instance.
If something about that underlying architecture
has a problem,
that will then shift into 1/2 or 0/2.
So right now what this metric says
is it currently is at 0,
which just means there are no status checks
that have failed.
If that does change, I'd like to be notified about that.
So I'm going to go ahead and click Select for this metric.
I have this particular instance that I've picked here.
I'm going to go with my average reporting,
and I'm going to change that period,
or that sampling number, of when do I look for data points.
So for custom metrics,
you can go all the way down to 10 seconds,
for the standard default metrics,
you can pick 1 minute, but you can't go lower than that.
So I'm just going to check every single minute
to see if this has changed.
And down here I have my standard greater,
greater than, equal to, lesser,
all those fun things from math class.
I'm going to go ahead and just select this as 0.
So if it's greater than 0,
namely if one of those status checks have failed,
that's a problem.
Next I have to figure out what do I do with this problem?
Well, first off, I probably want to know about this.
So I'm going to send out an email. I'm
going to pick my CloudWatch-Alarm SNS notification topic.
Now, like I said we'll talk more about this
in an upcoming lecture.
So for right now, I'll just leave it at this.
Trust me to say that eventually I would get an email
if I have this further configured with my email address.
Now it might not just be enough to get an email.
Maybe you want to trigger Auto Scaling
to scale out new EC2 instances or terminate EC2 instances.
In this case here, let's actually add an EC2 action.
So if we're in an alarm, what do we want to do?
Well, in my case I'm going to select
to just to stop the instance.
I can't actually recover the instance
because I don't meet this particular criteria
for this EC2 instance that I've spun up.
You don't have to know about exactly why that is.
It is important to know though, for the exam,
that you can create an alarm to stop,
terminate, reboot or recover,
which namely just goes through
that stop and restart process,
preserving the EC2 instance,
but cycling that underlying hardware.
Now you can also kick off calls to Systems Manager,
which we'll talk a lot more about in an upcoming lecture.
For now, let's just give it a name.
Probably could have guessed
I was going to use something along those lines (chuckles).
I get my little summary here of what exactly I'm doing.
And I'm just going to scroll down here, and say Create.
Awesome.
Now it does go into an insufficient data space,
meaning it just hangs out.
We don't have enough data. That 1 minute has not passed.
So I can't make a determination of, is this check okay
or am I in an alarm state?
So I'm going to go ahead and I'll pause the video here,
until we have more information.
Alright.
So it does look like that's now in the okay state.
If I open it up here,
it's because I'm currently reporting 0 as the metric,
the status check has not failed,
and I'm only set to alarm
when I go into that greater than 0 state.
Alright.
This was a very quick walkthrough of CloudWatch.
I would strongly recommend that you go through
and review this a little bit yourself.
Take some time to check out some of these metrics
on your own and get familiar in general
what you get out of the box with EC2, with RDS, with S3,
because CloudWatch can make an appearance on that exam.
Alright, let's go ahead
and flip back and cover some of those exam tips.
Alright.
So my first exam tip here is,
what tool do we use for monitoring?
Well, the answer is CloudWatch.
On the exam we should automatically associate
any sort of monitoring question to CloudWatch.
It's also important to know the basic checks
that you get out of the box.
What do you get by default?
and what do you have to add in yourself?
Now a few more tips here for us,
CloudWatch, CloudWatch, CloudWatch.
Anytime we talk about monitoring, think CloudWatch.
There are no default alarms.
That's important to know.
If you want to hear about something,
you have to create that alarm yourself.
It's important to understand
that AWS cannot see past the hypervisor level.
This means you get those standard checks
such as CPU utilization,
but you don't get those extended checks out of the box
for things like memory utilization,
or how full is that disc?
The more managed the service is,
the more checks that you get by default
because you have less control over the service,
AWS automatically provides you with more checks.
It's also important to know
that the standard reporting interval
for metrics in CloudWatch is every 5 minutes.
If you opt in for the additional detailed metrics,
those times are reduced to 1 minute,
and there is a small additional cost
for those detailed metrics.
And finally, you
can save money by cutting your own hair at home (chuckles).
It looks just as good as the professional cuts,
or at least that's what I keep telling myself.
Alright folks,
thanks for going through CloudWatch with me,
and I can't wait to see you in the next lesson.


Application Monitoring with Cloudwatch Logs
=============================================

Hey, Cloud Gurus. Welcome back.
In this lesson, we're going to take a look
at monitoring our applications using CloudWatch Logs.
So the first question that we have to answer is,
what is CloudWatch Logs,
and why do I need to monitor logs to begin with?
We'll take a look at 3 key terms that you're going to see
any time you use CloudWatch Logs.
We'll take a look at some of the features
that are available to us,
and then we'll get to see this hands-on
inside of my AWS account.
And then, we'll round it all out
with some of those exam tips.
So let's go ahead and dive right on in.
So what do we do with all of our logs?
Well, if we're just monitoring 1 or 2 EC2 instances,
maybe I don't do anything.
Maybe I just leave them on the host,
or maybe I ship them to an S3 bucket.
But it's not too bad until our resources start to scale.
And let's throw in RDS on top of that.
What do I do with my database logs?
How about my Lambda logs? What do I do with those?
What about CloudTrail?
And then, we dump on-prem logs on top of this pile.
You are going to be overwhelmed with logs, with data points.
How do you make sense of all of this?
How do you handle rotation? How do you handle storage?
How do you handle compliance?
Thankfully for us, AWS has provided a tool
to help us deal with all of these logs.
It's CloudWatch Logs.
Now, CloudWatch Logs is actually a part
of the CloudWatch suite.
What it allows us to do
is monitor, store, and access those log files
from that variety of different sources
that we just took a look at.
We can then look for potential problems.
We can look for trends.
We can analyze this information.
Instead of just leaving these valuable data points
on the host, we can collect them
into a single, centralized storage location.
Before we go any farther,
we need to define a few different terms
that you're going to see whenever you look at this service.
And the very first term is called a logging event.
This is just a data point.
It contains the data and a timestamp
of when that data occurred.
So, for our example here,
let's pretend I have an Apache website,
and maybe I'm going to track my error logs.
Every time an error pops up inside of that log,
that's going to be a logging event.
We're then going to take all of those logging events
and combine them into a log stream.
Now, a log stream is a collection of those events
from a single source, specifically that same source.
So this would be from a single EC2 instance
serving up my website.
Now, as we've talked about in all of our other lectures,
high availability is important. You're
not just going to have 1 instance or 1 web server.
You're going to have a collection of them.
So we're going to take all of those logging streams
and group them together in a logging group.
So this would be all of those web server logs
from all of my different web servers
combined in a single location.
Because the idea is,
I don't care about the individual instance,
the individual data point.
I care about the collective.
And we'll get to see this in my console in just a minute.
So we're collecting our logs.
We're not going to be crushed by them.
Now, what do we do?
Well, I have this logging group set up,
and I can create filter patterns
so I can look for specific errors in those logs.
For example, those 400 errors.
Every time I get a 400 error in a log,
boom, data point shows up.
So I've got my Apache log group that I just created.
I can build a filter pattern to look for particular terms
inside of those logs.
So think of it like this,
every time that 404 error pops up, that's a data point.
Another error pops up. That's a data point.
What I want to be able to do is take a look
and watch for trends.
Are my 404 errors spiking?
If they are, kick off that CloudWatch alarm
and notify someone or do something about it.
I can also use a service called CloudWatch Logs Insights.
Now, this isn't going to be featured too heavily on the
exam, so we're only going to talk about it briefly.
What it allows you to do is use SQL-like commands
to query all of your logs and look for general trends.
So instead of exploring a specific application, it's
kind of like you're looking at all of your logs at once.
Across all of your different applications,
all of your different services,
you can be as granular or as open as you'd like.
Now, like I said, you won't have to know this in depth.
Just know SQL, CloudWatch Logs.
You want to use CloudWatch Logs Insights.
Now, once you've identified a trend or that pattern,
it's time to do something.
You're going to have a CloudWatch metric similar
to what we set up in that last video to then alarm off of.
So let's go ahead and see this in practice.
Alright folks, so let's go ahead
and walk through configuring CloudWatch Logs
on my EC2 instance here.
So the very first thing that I'm going to want to do
is install the CloudWatch Agent.
Now, this is the unified agent
for both streaming logs to CloudWatch Logs,
as well as those custom metrics.
So let's go ahead and get this put in here.
And it's always a good day when I remember my password.
So this'll just take a quick second.
And it looks like it's already done.
Now, the next thing that I'm going to want to do
is go ahead and run this wizard command right here.
And what this is going to do
is it's going to kick off the config wizard
and generate a configuration file
for any custom metrics that I'd like to ship somewhere
or, in our case here, those logs.
Let's go ahead and walk through this here together.
So I am on a Linux host.
Now, I can select on-prem or EC2.
And it's good to remember,
CloudWatch is not just for EC2 architecture,
but in this case, it's going to be.
I'm just going to pick it as that root user.
Now, in my case,
I'm not going to select any additional monitoring.
You might want to in your actual application.
However, for now, just for the sake of brevity,
as there are a lot of additional answers you have to give,
I'm just going to go ahead and say no
to all 3 of those options.
Now, in this case, I do not have the CloudWatch Log Agent.
There used to be just a standalone agent.
You can still install that.
If you already have that installed,
you can import your previous configuration file.
But since I don't, I'm just going to go with the standard.
And yes, I would like to monitor a log file.
So let's go ahead and paste right here
my /var/log/httpd/access_log.
So any access to this application, to this website
will then get pushed into CloudWatch Logs.
And we're just going to leave it
with the standard name for now.
going to go with my instance_id.
And I would not like to specify any other logs.
So that's what the configuration
is actually going to look like.
It's just a JSON document.
And in my case here, I'm going to go ahead and say
no, I don't want to store it in the parameter store.
So the configuration just finished
and left me with that config.json file.
And my last step here on the actual host
is going to be to run or load the agent
and pull in that configuration file that I just created.
So let's go ahead and kick this off.
Alright, it's now done.
Let's generate some web traffic
so we can see this in action.
Now, I've got my super advanced webpage up and running.
Let's go ahead and refresh this a few times,
just to generate some logs,
and then we'll go check it out inside of the AWS console.
So I've got my CloudWatch Logs console open.
And as we can see here,
I have my access log available to me.
So let's go ahead and open up this log group.
Here, I only have 1 log stream
because I only set this up for a single EC2 instance.
And I can see those data points.
I can see exactly what that request looked like
coming into my web server.
Now, one quick point to note, this all works
because I have a role attached to that EC2 instance.
You have to give permissions for whatever resource
that needs to write to CloudWatch Logs
to have that permission set.
When we talk about some more of our serverless architecture,
such as Lambda, a little bit later on in this course,
this is going to be really important
to know how to set up and configure
because services like Lambda
will need to stream their logs somewhere,
and the answer is CloudWatch Logs.
Alright, folks, that's going to be it for the console.
Let's go ahead and close this out
and go check out some of those exam tips together.
So my first tip here is, logs go to CloudWatch Logs--
if we need to process them,
if we need to view them--in a sort of real-time situation.
Now, quick note, this is not real-time.
We will talk about real-time log streaming later on,
but it is very quick.
If the question that you're seeing on the exam
focuses only on storage and not processing,
then we can think about shipping them to S3.
But 9 times out of 10, we want to think CloudWatch Logs.
You need to have a high-level understanding,
and we've covered a lot of those pieces
in either other lectures or the start of this lesson,
knowing that it's EC2, Lambda, RDS, CloudTrail, on-prem.
These are common log creating services
that can all write into CloudWatch Logs.
Now, a few more tips here for us.
It's going to be our go-to tool unless the exam
explicitly calls out needing a real-time solution.
And in that case, we'd probably want to use Kinesis.
And we'll cover that--well, outlier--in a future lesson.
You can create an alarm based off of that CloudWatch metric
that is generated with a filter pattern
that you are creating.
So you can actually interpret the content of the logs
and look for issues before they become a bigger deal.
CloudWatch Logs is agent-based.
It can use that same agent
that you can use for your custom metrics,
as we saw in our configuration steps.
It is not automatically there.
It is not automatically configured.
So you do have to take the steps
to get it on your host and set it up properly.
If the exam mentions using CloudWatch Logs
and needing to run SQL queries over that content,
you want to think CloudWatch Logs Insights.
Now, we didn't talk too much about it
in this particular lesson,
as it might pop up as a distracter, or in passing.
The exam won't focus too heavily
on CloudWatch Logs Insights,
so I wouldn't worry about it too much.
And my final tip here, while it's not very tasty,
in a pinch, the inner bark of trees,
such as maple, birch, or pine, is actually edible.
I'd personally suggest boiling it if it's an option
because eating it raw could prove to be rather fibrous.
Alright, folks,
thanks for going through CloudWatch Logs with me,
and I can't wait to see you in the next lesson.


Monitoring Exam Tips
=====================

Hello, Cloud Gurus, and welcome back.
In this lesson, we're going to be taking a look
at some monitoring exam tips.
So let's go ahead and dive right on in.
Now as always, we have 4 questions
that I want you to be asking yourself in the exam
any time you see something remotely relating to monitoring.
First one is, what's the best tool to monitor with?
Now, we should already know the answer.
9 times out of 10 it's going to be CloudWatch,
but occasionally, as we'll see in just a second,
there are limitations to CloudWatch where another tool
might be able to fill in that gap.
Is that metric available by default?
Is it CPU that comes out of the box,
or is it that kind of tricky memory or disk utilization
where you might have to install the engine
if it's an EC2 instance, or you might just get it
if it's a more managed service such as RDS?
But you need to have have that breakdown of,
can Amazon see this, can they provide it for me?
Or is it a data point that I have to bring to the table?
Where can I find those logs?
Where does the service place those log?
How do I configure that service to use CloudWatch Logs?
Or, when it comes to something like CloudTrail,
does it make sense to use CloudWatch Logs?
Now we'll talk about CloudTrail
in much more depth in an upcoming the lesson.
But you're going to have to know,
do we need a streaming or processing service
for these logs, or is simple S3 storage good enough?
Do I need to adjust my alarm threshold?
Now we're going to talk about Auto Scaling
and the wonders of automatically creating
and destroying architecture here in a few lessons,
but for now start thinking through
what happens if I adjust my alarm threshold?
Would my alarm become more aggressive or more conservative
when it comes time to create, destroy, or notify a user?
Because you're going to see questions that will deal with
how do we make this alarm fire more regularly?
Oh, it's firing too much.
How do I reduce that notification window?
So think through what happens if I change that alarm?
So a few things to keep in mind for exam,
CloudWatch is the main tool
for anything alarm metric monitoring related.
It works on-prem, it works with EC2,
it works with RDS, it works with Lambda,
it works with CloudTrail,
and it's very simple to get set up with.
Not everything needs a CloudWatch.
Now I know I just told you we use CloudWatch for monitoring
but there are some times where monitoring AWS best practices
should be handled by AWS Config.
Now we'll discuss Config in a future lesson,
but an example of this would be are my EC2 instances
using an approved AMI.
Now I might be able to somehow craft that in CloudWatch,
but it would be very painful
and very much a custom metric,
where AWS Config can handle that out of the box.
So system-level, application-level...think CloudWatch.
Monitoring AWS standards and best practices,
think Config. Know that interval.
That's really important by default,
your metrics are delivered every 5 minutes.
And if you opt in for detailed metrics,
you'll see those every 1 minute.
The 5-minute metrics are free,
the 1-minute metrics include an additional charge.
It's also important to know
that if your metrics come in every 5 minutes
but your alarm is set to look for data points
every 1 minute, you will be stuck
in the insufficient data purgatory forever
and your alarm will never work correctly.
So knowing those intervals
and how often they come in are going to be key
for creating the correct alarm threshold.
And a few more tips for CloudWatch Logs--
it's the place for logs.
You need to know on-prem, EC2, RDS, Lambda, CloudTrail
all integrate with this service.
You have the ability to stream them into CloudWatch Logs.
It's not always required in the exam
and it won't always be listed as an answer.
But in general, when you think logs, think CloudWatch Logs.
If you see SQL pop up, think CloudWatch Logs insights.
You don't have to know it in depth,
we haven't talked about it too much.
It's mostly just association of the service.
Looking over all of my logs of SQL is something
the CloudWatch Logs insights can get you.
And if you see the exam asking for a real-time solution,
think Kinesis, this is covered in the big data section
but CloudWatch Logs is not real-time.
It's technically listed as near real-time,
I know this sounds like we're just splitting hairs here,
and it feels like we are,
but it's an important sticking point for the exam
as I've seen that trip up quite a few folks.
Alright, everybody.
Thanks for going through these exam tips with me
and I can't wait to see you in the next section.



High Availability and Scaling
=============================


Horizontal vs Vertical Scaling Overview
========================================

Hey Cloud Gurus, welcome back.
In this lesson, we're going to give you an overview
of the 2 types of scaling
that we need to be familiar with:
horizontal and vertical scaling.
We're going to start off with vertical scaling.
This is also a little bit of the older style of scaling
when it comes to applications.
We'll see how does this translate
into our new cloud architecture.
And then we're going to get to dive into horizontal scaling,
aka, the type of scaling
that we probably want to focus on in most situations.
And then I'm going to walk you through 3 important questions
to ask yourself in the exam.
These are going to be the 3 W's of scaling.
So let's jump right on in.
Now, you might be familiar with vertical scaling,
if you've worked in an IT environment before.
Vertical scaling is,
well, it's kind of like that skyscraper
that you see on the screen,
gradually getting taller, and taller, and taller.
And this works up into a point,
at some point, well, we can see,
that building just can't continue to get taller.
We can apply this same mentality to our EC2 instances.
We can start off with that t2.micro
and we can turn it off and resize it,
and maybe now it's an m4.large
and we can turn it off and resize it again,
and now it's an r5.16xl.
But at some point,
we can't just keep resizing that same instance
over and over again,
and expect our application to be able to perform
and be highly available,
and really be cost-effective.
If your answer to scaling is
create that x132.xl instance without 4TB of RAM,
we might want to rethink what we're doing here.
Now, that's not to say that vertical scaling
never has a place,
no matter how hard you try that t2.micro
is going to be a terrible production database.
At some point, you'll probably need to scale vertically.
We'll make a call out to a few spots
as we go through these upcoming lessons
where that could be appropriate.
But what I want you to be focusing on
is not vertical scaling, but horizontal scaling.
Now, instead of having just a single skyscraper,
now I've got, well, my suburbs.
This is a collection of EC2 instances,
just like that neighborhood,
it can sprawl and sprawl and sprawl,
I can keep spinning up EC2 instances,
as long as I have money to pay for them.
I can spin up instances over and over and over again,
and there's no real limit besides that checkbook.
Now horizontal scaling includes a few benefits.
It also increases our high availability
because as long as we're spreading
those across multiple availability zones,
if one AZ fails, who cares?
We've got redundancy, we've got backups.
And we're going to see this in practice,
as we take a look at some of our auto scaling technologies
in some upcoming lessons.
Now I promised you those 3 questions to keep in mind.
So let's go through those now.
The very first thing
that we're always going to want to to ask ourselves
is what, what do we scale?
Are we scaling an EC2 instance?
Are we scaling that database?
How do I spin up that instance appropriately?
Where is it that template coming from?
How do I decide when it's time to scale
what I need to bring online?
Now, once we've answered that question, of what I'm scaling,
then we have to decide, where does it make sense to scale?
Where in the VPC?
How many availability zones am I using?
What load balancer are those resources
going to be registered behind?
Now, just that what and where by themselves,
don't really do us a lot of good.
If I've decided that I want to scale that EC2 instance
and it needs to be in these availability zones
and this network, I need some sort of data,
some sort of trigger,
some sort of alarm to tell me,
when do I need that architecture?
That's why all of these W's are critical to have.
We have to be able to look at our resources and say,
"When does it make sense to spin up more?
"When does it make sense to take something away?"
Now as we are in the AWS space
and studying for that AWS exam,
most of the time that's going to be CloudWatch,
those CloudWatch alarms to tell us
when something needs to happen.
Now we're going to go through all of these W's in detail
in our upcoming lessons.
And we'll answer each one of these questions.
These 3 W's are going to be my exam tip.
Whenever you see a question
that involves some sort of scaling,
whether it's a database, an EC2 instance,
even some of those Lambda functions or serverless resources
that we're going to talk about in some upcoming lessons.
Doesn't matter what it is, ask yourself:
What do we scale?
Where do we scale it?
And when do we need to scale?
These 3 W's should help lead you to the correct solution
for the scenario that you're provided with.
All right Cloud Gurus, thanks for walking in through
the two types of scaling and those 3 W's. Join me in
the next lesson where we're going to start applying
these scaling types and W's to actual AWS architecture.
I'll see you there.


What are Launch Templates and Launch Configurations ?
=======================================================

Hello, Cloud Gurus, and welcome back.
In this lesson, we're going to take our very first steps
on our auto-scaling journey
by taking a look at what are launch templates
and what are launch configurations?
The very first question that we have to answer is the what.
What are we scaling?
That is the first W that we are going to cover.
Once we've hit on that,
we're going to take a look at launch templates
versus launch configurations. To get an idea
of when do we want to use one versus the other.
Once we have that understanding,
of course, we're going to hop into the console
and actually see this in action.
I'll go ahead and walk you through creating
your very first launch template
and then spinning up that EC2 instance.
And after we see it in action,
of course, we have those exam tips.
What do we need to focus on
when it comes time to take the test?
So let's go ahead and jump right on it.
What? What are we scaling with?
This is the very first scaling question
that we have to answer.
And when we're thinking about auto-scaling,
when we're thinking about EC2 architecture,
we solve that what with a launch template.
Now, the launch template
is simply a collection of all of the settings
that go into building and configuring an EC2 instance.
Now, in previous lessons,
you would have walked through building out that EC2 instance
using that EC2 wizard.
And that's great for spinning up your first instance
and your second and your third,
but what about your 5,000th instance?
Do you really want to be walking through that wizard
every single time?
I don't, and I'm going to go ahead and guess
that you probably don't want to either.
So what we're going to do with a launch template
is walk through that wizard
and effectively just save the results, templatize this,
so when we need that architecture again,
we can just press that magic button and the instance pops up
exactly as we've configured it in that template.
If you've worked with auto-scaling before,
you might be thinking, "Alex, this launch template
sounds a lot like a launch configuration."
And you're not wrong.
So let's take a quick look here
at using a launch template versus a launch configuration.
Now, the templates
are the newer, better, faster, stronger option.
It's just the best.
It doesn't mean that if you're using a launch configuration
that you're technically wrong.
It's just AWS has released the templates,
and they give us additional flexibility and tools
that the configurations didn't.
So the launch templates, as we'll see in our upcoming demo,
are for more than just auto-scaling.
What this allows me to do
is use that template any time I want
compared to those launch configurations,
which is only for auto-scaling those EC2 instances.
Launch templates support versioning.
This means that I can easily create
new versions, new revisions,
maybe I've got new user data, a new AMI,
and it's all kept in one single place.
And as we'll see during the demo, I can simply select
which version of that template I'd like to use.
Where the launch configuration, well, it's immutable,
I can copy and paste it, but it's just a lot harder to track
those different versions.
It's a little bit more frustrating.
Launch templates also support versioning.
Versioning just means whenever I need to make a change,
such as a new AMI, a new user data, a new security group,
something along those lines,
it's very simple to make that change
and then still have that history of what came before.
Launch configurations, on the other hand, are immutable.
I can copy and paste them,
but they're much harder to deal with.
They're not as flexible.
Templates support more granularity
in the configuration choices that we have.
I could do things like defining the VPC,
defining the networking space,
filling in everything that I possibly have to have
to spin up that EC2 instance,
where launch configurations don't have
all of these same options that that EC2 wizard provides.
So in short, we want to use launch templates
and not configurations.
Now, on the exam, configurations aren't wrong,
but just in general,
always favor an answer that includes launch templates.
Alright, well, it's great to talk about these,
but let's actually dive in and see what it looks like
to spin up our very first launch template
and an EC2 instance.
Alright, I've got my AWS Console open,
and I would invite everybody to follow along at home
if you're interested,
or just sit back, relax, and enjoy the show.
So let's go ahead and open up the EC2 service.
From here, on the left-hand side,
we can see our Launch Templates.
We'll go ahead and open that up
and then click that Create Launch Template button.
Now, this is going to walk us
through creating our very first launch template.
So just like everything in AWS, I have to give it a name,
this will be my yay-aws-template, and a short description.
We always like learning new things.
As we scroll down, I do want to quickly call out
this Auto-Scaling guidance button.
If we select this, it will assist us in making sure
that we have all of the needed settings
to use this launch template in an auto-scaling group.
Now, don't worry, we haven't covered that just yet.
We'll get to those auto-scaling groups in our next lesson.
For now, though, I'm going to leave this on.
Now, notice it does add
that additional required information here,
saying we have to have that AMI
if we're going to use it in that auto-scaling group.
Well, let's go ahead and fill this in.
I'm just going to select my base Amazon Linux 2 AMI.
Now, scrolling down,
you're going to notice a lot of these are optional,
meaning I don't have to fill them in, but if I do,
this is actually going to save me work in a long run.
So in this case here,
I'm going to say that I would like this to be,
let's just go that t2.micro to save some money,
and I'm going to attach a key pair.
Now, it is important to note
some of these values can be overwritten or provided
at the time of creating an auto-scaling group.
For example, if I didn't specify an instance type,
I could specify that in the auto-scaling group later on.
However, if I don't specify a key pair now,
I could not specify it in my auto-scaling group.
So I would encourage you
to go through and play with this a few times yourself
just to get a feel for all of the different settings
because there's unfortunately more in here
than we have time to talk about in this lesson.
Now, in the Network section,
I am purposefully going to leave that VPC selection blank.
If we specify a VPC in the launch template,
that launch template cannot be used
in an auto-scaling group
because the auto-scaling group has to specify the VPC.
So let's just leave this blank for now.
For the Security group, I will also leave that as optional,
and we'll fill that in here in a minute. Storage,
we're just going to stick with the standards, tagging,
all of that fun stuff you're already familiar with
from that EC2 wizard. Now, I
quickly want to call out the Advanced details section.
As you can see in here,
there's a lot of different values we can fill in, right?
That instance profile, termination protection,
all that good stuff.
But what I really want to call out
is all the way at the bottom.
It's that user data.
Remember if we want to provision or bootstrap the instance
as it comes online,
there's a pretty good chance
that we want to use our user data here to do this.
So now that I've got all my settings ready,
let's go ahead and click that Create Launch Template button.
So now that that's created,
for now, we're going to spin up an instance
directly from this template.
I promise we'll get fancier in that next lesson.
Now, I'm going to fill in some of the basics
that I skipped over when I created that template
since I didn't define
absolutely everything that we have to have.
So here I can pick the version of the template.
Remember these can be versioned out as we make changes.
I'm going to stick with that standard AMI that I specified,
that standard instance type.
But now I actually have to fill in that VPC information.
I'm just going to go ahead and pick
one of my default VPC subnets here.
And I can select a security group, nothing too fancy.
We'll go with the standard values
for all of the other options,
and I'm going to go ahead and click Launch.
Now, if we open up that instance here,
we'll see it's in that pending state.
We'll pause real quick just to make sure it comes online.
Alright, it looks like that instance is up and running.
Now let's go ahead and jump back into the slides
and hit a few more things
that we have to watch out for on the exam
before we call it quits.
Exam tip number one: what makes a template?
It is critical to understand
what goes into a launch template.
As we walk through in the demo,
we need to know that includes the AMI,
generally that EC2 instance size, security groups,
and potentially networking information.
Now, we just talked about this.
Remember that if it includes networking information,
it cannot be used in the auto-scaling group.
It could only be used by itself.
So we need to know what goes into that what.
Now, a few more tips here for us before we wrap this all up.
Launch templates are the best.
Launch templates are the easiest.
Launch templates are the most flexible.
It's not very often
that I get to make a declarative statement
and say always use this versus that,
but in this case I can.
Always use launch templates over launch configurations.
Launch configurations are older, not as great.
It's not wrong, but just don't use them.
Hopefully, I've gotten that point across in this video.
Now, we need to remember for the exam,
configuration of those instances generally happens
with that user data,
and that is included in that launch template.
So if we need to make a change to the user data
because that's what the exam is asking us to do,
it would require us to version out that launch template
and then include the adjusted user data.
Change, change happens all the time.
Templates can be versioned.
As we saw in the demo,
I selected that version that I'd like to deploy.
When you make changes, you'll get that history list.
Launch configurations, while we didn't see it hands-on,
it's just good to know they're immutable,
you can copy and paste them,
but you technically cannot edit them
once they have been created.
And then networking.
Your launch configurations don't include the networking.
Your launch templates could,
but still, we want to focus on using that auto-scaling group
with our launch templates,
and I promise we're going to cover that topic
in our next lesson.
And one last tip here for you.
If you ever find yourself being attacked by a moose,
it's best to play dead by curling up in a ball.
If you're wearing a pack or a backpack,
try to use that to protect your head.
Alright, folks,
thanks for going through this lesson with me.
I can't wait to see you in the next video.


Scaling EC2 Instances with Auto Scaling
==========================================

Hello, Cloud Gurus,
and welcome back.
In this lesson, we're going to be taking
a look at scaling out EC2 instances using Auto Scaling.
We'll start off by taking a look at
the Auto Scaling groups themselves.
What goes into it?
Where do they run?
Why do I care about this?
Then we'll take a look at all of the settings
that go into creating an Auto Scaling group
from beginning to end.
We'll talk about limits.
Now, I know what you're thinking--
scaling? we want to scale forever.
Well, we'll talk about why we want to use min.,
max., and desired capacity to put limits
on how many instances we're going to create or destroy.
Of course, we're going to get to see this hands-on.
We'll open up my console,
and walk through spinning up an Auto Scaling group
before finally taking a look at some of those exam tips,
what do we need to focus on for the test.
So let's go ahead and jump right on in.
Now, you probably knew this was coming--where?
Where are we going to scale?
We've answered the "what" with those launch templates
and launch configurations,
now we're going to answer that "where."
And the short of it is, Auto Scaling groups are the "where."
Auto Scaling groups are simply a collection of instances
that are treated as a single, cohesive unit
as far as scaling and management goes.
The idea is, I have a huge collective
and they all function as one.
And that Auto Scaling group is going to dictate
where those instances come online,
where are they created,
and where are they destroyed when we no longer need them?
So it's going to answer that second W,
that we always want to be focusing on.
Now, what goes into an Auto Scaling group?
Well, the very first thing we have to have is a template.
The template just tells the Auto Scaling group,
all of the settings that it needs to know
to create that EC2 architecture, the user data,
the security group, the size of the instance.
The next thing that we have to define
is that networking space.
Now, this is why we don't want to include
that networking information in those launch templates
that we talked about in the last lesson,
because the Auto Scaling group is going to define that.
How many Availability Zones do we need to use?
And the answer is at least 2,
because remember, we want to have high availability.
If one AZ goes offline, we want to have that backup.
We want to have that secondary architecture.
And of course, cost is important.
We can use the Auto Scaling configuration to define;
Do we want to use Spot Instances,
or do we want to use On-Demand?
We can even use Reserved Instances to cover the bare minimum
of architecture that we have to keep online.
So we always want to be thinking through
how do we optimize those costs.
Your load balancer.
Now, we've already talked about ELBs in this course.
So hopefully we're already familiar with this,
but that load balancer sits
in front of my Auto Scaling group,
and as instances come online,
they are registered behind the load balancer.
As they are taken away, they are de-registered.
This happens automatically if we select
the Auto Scaling group to interact with that load balancer.
It also allows us a critical setting.
The Auto Scaling group can use
the health check of the load balancer.
Now, this is a really important detail to know.
It means if the instance fails at the load balancer level,
the Auto Scaling service will de-register that host,
terminate it, and spin up a replacement.
As we'll see in the demo,
it's just a check box that we have to check,
but if we don't enable this,
it will not use the load balancer health checks,
it will only use those standard system level checks
for those EC2 instances.
And that's going to be a big test concept,
so make sure we understand this
before we move on from this lesson.
Now, your scaling policies.
We're going to talk more about dynamic scaling
in the upcoming lesson,
but for now, we need to think about defining
a minimum, maximum, and desired capacity.
Effectively, these are the guardrails to ensure that
we don't have too few or too many EC2 instances.
And then finally, notifications.
We need to know when something occurs.
If we want to get that text message, that email,
we can get that heads up.
And the answer for this is SNS,
the Simple Notification Service.
Now I know what you're saying,
we haven't talked about SNS just yet,
I promise that we will in a few lessons.
For now, just keep it in your head
that we can get notifications if a scaling event occurs.
Hopefully this gives us a little bit of an idea
of what goes into Auto Scaling.
Before we see it in action, I want to talk about, well,
what I would say are the 3 most important things
that we have to set in an Auto Scaling group.
The very first one is the minimum.
The minimum is the lowest number of EC2 instances
that the Auto Scaling group will ever have online.
So I have a question for you;
if I set my minimum to 0
and I had 1 EC2 instance left,
and I was gaining an alarm that said
there is low utilization on that instance,
would Auto Scaling terminate that last host?
What do you think?
Well, the answer is yes.
Yes, it would, because you said it was okay to have 0.
That's probably not what you want to have here.
Generally, we want to set a minimum of 2 EC2 instances.
Now, why 2?
The answer is high availability.
We want to have that spread
across multiple Availability Zones,
which Auto Scaling will do naturally.
So if one goes offline, not a big deal,
we have that secondary one,
Auto Scaling will then terminate that broken host
and build a new one for us.
Now, of course, that minimum can be higher than 2,
but remember on the exam,
keep in mind 2 should be about as low as we ever go.
Now, the maximum.
The maximum is just the roof.
Well, what do we never want to go above?
If I normally need 50 EC2 instances,
maybe I set this at 60.
I give myself a little bit of wiggle room,
but I don't set it at 5,000 instances
because I would never need that many instances.
Keep this in mind, we don't want to pay too much,
we don't want to accidentally scale out past
what we expect it to scale to.
The minimum is the floor, the maximum is the roof,
and then we have desired capacity.
Now, I don't know about you, but desired capacity,
that's kind of a weird term to use here.
It makes me think of some average number of instances
that I want to keep online, and that's not it at all.
Desired is simply how many do I want at this very second.
So desired is constantly changing.
It will never be lower than the minimum,
it will never be higher than the maximum.
Really AWS should have just called desired capacity,
"how many instances do I want at this very second,"
although that wouldn't fit into the console very well.
Now, I know these terms
are a little confusing at first glance,
so that's why we're going to jump into the console now
and explore min., max., and desired together.
Alright, you can see that I've got my console open.
Now, if you are following along at home,
please note that I'm picking up where we left off
in the last demo,
so I already have my launch template created.
So we're going to open up my EC2 service,
and scroll all the way down to the bottom
to those Auto Scaling groups.
Now, let's go ahead and create our first Auto Scaling group.
Now, I have to give it a name,
I'll go with yay-scaling.
I think that seems appropriate.
Select my yay-aws-template.
Notice here I can pick the version of said template.
We're just going to go with the one
that I made in the last video.
I have that key pair already allocated, that AMI ID,
that t2.micro.
Now here I can overwrite some of these values.
If I wanted to,
I can mix and match on-Demand and Spot capacity.
Now, this is a good testing topic.
It's important to know that you can use Spot Instances
in Auto Scaling groups in order to save some money.
Now, in this case,
we're just going to go with the Standard EC2 architecture.
I'm going to define the VPC that I'd like to create this in.
And I have to pick the subnet or subnets
to spread this out in.
So let's go ahead and pick 2 subnets here
for high availability of course.
Now, once again, I can attach a load balancer.
In this case, I'm not going to,
but it is important to know that that is an option.
Now for Health checks,
we see here that it does use
that EC2 instance health check by default,
and we can enable that load balancer health check
if we are using a load balancer,
but this is something that we have to opt into.
Now, here comes that min., max., and desired capacity.
So let's go ahead and set that Minimum capacity.
We'll just say is 2 to make sure that
we're in both Availability Zones.
And we'll set that Maximum as 10.
Now, just trust me, if I were to click Create right now,
it wouldn't work, it would throw an error.
Desired has to be between the minimum and the maximum.
So let's start off with 2 EC2 instances. Now,
for now, we're not going to include any scaling policies,
I promise we're going to cover those in the next lesson.
I can add in those notifications if I'd like,
and finally, we can create a tag.
I'm going to add one here real fast.
I'll go with aws-rocks.
All my new architecture is going to have that.
And then finally I get my Summaries page.
So let's go ahead and scroll all the way down,
then click Create.
Now, what's happening behind the scenes
is, notice here, it says we have 0 EC2 instances,
and we have a desired capacity of 2 EC2 instances.
That means that Auto Scaling is saying,
I have 0, I desire 2,
I need to create 2 more.
And if I click on my Instance Management tab,
we can see that I have 2
that are in that pending state coming online.
So we're going to pause the video here right now,
and we'll check back in
as soon as these have finished provisioning.
Alright, it now shows that my 2 instances are online.
Now, let's imagine for just a minute
that I had a huge influx of traffic.
Now, I know I don't have scaling set up just yet,
but let's imagine that I do.
And I get this alarm that says,
you know what, we no longer need 2 instances,
we set the desired capacity to 7.
So let's go ahead and make that happen here.
We're going to change the desired to 7.
Now, this is where those scaling policies
will handle this for us.
And like I said, we'll cover that in the next lesson.
So let's go ahead scroll all the way down.
I'll click Update.
Now, what Auto Scaling is going to do
is it's going to say I have 2, I desire 7.
So I need to spin up 5 more.
We can kind of think as Desired capacity
as Auto Scaling is chasing its own tail.
Always trying to catch it,
always trying to have that desired capacity online for us.
So we're going to go ahead and pause
and wait for these instances to come online.
So it looks like the instances are now up.
Now, let's go through this once more, but in reverse,
let's imagine that that load has passed,
those servers are no longer needed.
We're going to go ahead and change that now
from a desired of 7, down to let's say 3.
I'll go ahead and click Update.
We'll go ahead and pause one more time
to wait for it to terminate those 4 EC2 instances
because it has that Desired capacity of 3,
it currently has 7, it needs to terminate 4.
So we'll check back in when that's done.
So it looks like those instances
are now currently terminating.
While that happens,
let's go back to the slides and go through
a few study tips related to Auto Scaling groups.
My first exam tip for everybody
is Auto Scaling and high availability go hand in hand.
We want to ensure that we are creating
that Auto Scaling group across multiple Availability Zones,
so if there's an outage, we have that redundancy.
Anytime you're looking at an exam scenario
and you see an EC2 instance, you should be thinking,
would this benefit from being
inside of an Auto Scaling group?
We want to ensure that we're spreading those resources out.
You will never pick a correct answer
that uses 1 single Availability Zone
when you're talking about high availability.
You want to make sure that you're using multiple AZs,
and you're using those load balancers
to front that Auto Scaling group
that's spread out across those AZs.
high availability is a thread that runs
through every single question
that you will see on this test
and we always need to keep these concepts in mind.
Now, let's go ahead and take a look at
a few additional exam tips.
Remember that networking space is defined
in that Auto Scaling group.
This one can be a little bit confusing for some folks
Because we can set it in those launch templates,
but we're going to want to reserve that
for the Auto Scaling group
to ensure that we're using multiple subnets
that span multiple Availability Zones.
Now, networking configuration is kept
inside of the Auto Scaling group.
As we saw through the walkthrough,
we want to make sure that we're selecting multiple subnets
that allow us to spread out our architecture
across at least 2 Availability Zones.
It is vital to remember the load balancer.
Auto Scaling and load balancing also go hand in hand.
We want to make sure that we're selecting answers
that use an ELB,
but also define and explicitly call out
that the Auto Scaling group will use the ELB health check.
Because remember, the ELB health check
is not enabled by default,
so we want to ensure that we're turning that on.
Know your limits.
You need to make sure that you understand what would happen
if we change that min., max., and desired setting
inside of our environment.
These are the 3 most important settings,
and they're going to govern all of the rules
that your Auto Scaling group is really going to live by.
It's also good to remember
that the Auto Scaling group can notify users
when something happens,
when something comes online,
when something is terminated,
and SNS is the tool that it uses to make this happen.
Now, I promise you will dive a lot deeper into what SNS is
a little bit later,
but for now, just pin that into your brain.
We can use SNS to send out push-based notifications.
Auto Scaling groups will balance your architecture
across those AZs.
Remember, we at least have to have 2,
and you're not going to find yourself in a position
where you have 45 instances in Availability Zone A
and 2 instances in Availability Zone B.
It'll keep it as balanced as the numbers
will possibly allow.
And my last tip for everybody,
if you ever find yourself going down a waterfall,
make sure that you go feet first,
you tense up all of your muscles like a pencil,
and you wrap your arms around your head to protect it.
Alright, folks,
thanks for sticking with me through this lesson.
And I can't wait to see you in the next one.


Diving Deeper into Auto Scaling Policies
=========================================

Hello Cloud Gurus and welcome back.
In this lesson, we're going to take a deeper look
into those auto-scaling policies
and see how we can start automating the creation
and destruction of instances inside
of our auto-scaling groups.
We'll start off by taking a look at step scaling.
Namely, we're going to walk through
what would it look like to scale an application out
and then back in
and then we'll get to see what is a warmup
and cool down period
and how can we use these to prevent our instance count
from going crazy inside of that auto-scaling group.
We'll then take a look at our scaling types
that are supported with auto-scaling.
We'll of course get to have that hands-on demo inside
of our console and I'm going to show you a magic trick.
We'll have to wait a little bit for that though
and we'll come back together
and wrap it all up with some exam tips.
So let's go ahead and jump right on in.
Up into this point, we have answered 2 of the 3 W's.
What are we scaling with those launch templates?
Where are we scaling with those auto-scaling groups?
Now we're going to define when do we scale
using our scaling policies.
So I'm going to lay out a quick application here for you.
On the left-hand side, we see my average memory utilization
for an auto-scaling group
and I have found through much trial and error
that my application performs the best
when it's between 40 to 60% utilization of its total memory.
Now that is across all of my EC2 instances
so we're averaging these numbers out.
When I'm between the 60 to 100% range,
I need to be scaling out.
When I'm between the 40 and 0% range,
it's probably time to be scaling back in.
I have a minimum count of 10, a maximum count of 50,
and I'm currently sitting at about 20 EC2 instances.
Now let's take a look at some scaling policies
that we're going to apply here.
On the left-hand side, I've got my scaling out policies.
Between 60 and 80%, I want to add 10 EC2 instances
and between 80 and 100%, I want to add 15 instances.
I like to call that the my hair's on fire level.
We really need to get some more architecture up
before the application falls over.
On the right-hand side, we can see my scaling in policies.
When we're between 40 and 20% utilization,
we're going to terminate 10 EC2 instances and when
we're between 20 and 0%, we're going to terminate 15.
Let's go ahead and jump right on in.
Now while we've been talking,
my application has seen some increased utilization.
We can see on the left-hand side
that my average memory utilization
is between that 60 and 80% window.
It's time to add in some more architecture.
So how many instances am I going to add?
Well if you remember our scaling policies,
we're going to add 10.
Now those 10 EC2 instances,
are they immediately online and available?
The short answer is no.
They take time to come online.
My user data has to run.
My packages have to install.
My code has to deploy.
It's going to take me a little bit.
This is where the concept
of a warmup period comes into play.
So auto-scaling knows these 10 EC2 instances
I've built them out, but they're not ready just yet.
Because if I were to register them behind the load balancer,
they would immediately fail the health check,
I would terminate them, spin them up again,
they'd fail, terminate them
and this vicious cycle of money spending would start
and nobody wants that.
So the warmup period is where I'm just going to pause.
I'm just going to hang out.
Auto-scaling is smart enough to know
we've got 10 waiting and I'm going to have a set time period
for how long I'm not going to health check them,
I'm just going to set them aside.
Now this is a configurable window
and it will depend on your particular application.
Now in the meantime, while those 10 EC2 instances
are warming up, I've got more load.
We now get another check-in between that 60 to 80% level.
What happens, do we add in 10 more EC2 instances?
The answer is no, we add nothing more.
This is because auto-scaling is smart enough to say yep,
I've got 10 instances, they're still in that warmup period.
They're still coming online.
So we do nothing.
Load continues to rise.
I am now in the my hair is on fire 80 to 100% level.
Oh no, how many instances do I add?
Well if you said 5, you were right.
Now why 5 and not 15?
That's because auto-scaling knows I've got 10
that are still in that warmup period
so I'm only going to add in 5 additional ones
because what we would hate to have happen
is we add a bunch more, they all come online,
the load drops and then we terminate those hosts
and then the load comes back and our instance count
is going up and down and up and down all over the place
and we're just spending money.
We want this to be a very gradual
slow and controlled process.
Now the load has gone down across my applications.
Why is that in check-in number 5?
Well, you can probably guess
it's because those instances finally came online.
That warmup period expired
and now those 15 additional EC2 instances
all have their member utilization considered
in my overall stats.
So now I'm in that perfect window of between 40 and 60%.
Now as the evening goes on,
maybe it's 3 o'clock in the morning now,
all of my users have gone to bed,
I don't need as many instances.
So I'm now ready to terminate my first set of architecture.
I'm in that 40 to 20% range.
Let's terminate 10 hosts.
Now how long does it take to terminate an instance,
an hour, 10 minutes, instantaneous?
Yeah, it doesn't take very long.
It's just that one API call that says boom, you're gone.
This means that it's a lot longer to get instances online
than it is to take them offline.
So we want to scale out aggressively to handle the workload
and then scale in conservatively.
We want to avoid something called thrashing
where you rapidly increase your count, rapidly decrease it.
You're spending money and your instance count
looks like a heart attack, that never needs to happen.
Or in that next window, that 0 to 20% utilization
and let's terminate those 15 hosts taking them offline.
And then this process would continue on
and the idea is my auto-scaling policies
do not require my manual intervention.
All of this takes place without my knowledge
because I've set the rules
and auto-scaling simply abides by them.
We've already talked a little bit about that warmup.
It's that idea that your instances as they come online
kind of get that grace period where they're not counted
against that health check, their stats aren't included
in your overall CloudWatch alarms.
It just hangs out, its sole job is just to come online.
This stops the instance from being placed
behind the load balancer and being prematurely terminated.
Now we also have the process of a cool-down period.
The cool-down period is a little bit
like that warmup period,
but it applies to both creation and termination events.
The default window for a cool-down period is 5 minutes,
which means if we have multiple scaling events
in those 5 minutes, it just ignores the most recent ones.
Every time something happens,
we count 5 minutes before we consider that next step.
This helps to prevent runaway scaling events.
Either crazy scaling events scaling out or scaling back in,
remember we want this process to be gradual and the warmup
and cool down allow us to avoid thrashing
where that instance count looks like a heart attack.
Remember up and down and up and down,
we never want that to happen.
It's slow and deliberate.
That's the goal with auto-scaling.
What types of scaling do we have available to us?
The first type of scaling that we have available to us
in auto-scaling, it's called reactive scaling
and that's actually what we just talked about
where I'm responding to data points.
I don't know when the workload is going to be there
so I just have my policy set up
to respond to whatever happens.
And this is great if I don't have data to tell me
when I need my instances, when I need those resources.
That second type is called scheduled scaling.
Now this is great if you have a predictable workload.
A predictable workload would be my accountants
always crunch their month end numbers
on the third Tuesday at 5:00 PM.
So why would I wait until 5:00 PM
on that third Tuesday to spin up instances?
Get them ready at 4:45 so the users don't have to wait
for that architecture to come online.
If at all possible,
we want to create these scheduled scaling events
to get ahead of the workload.
If we don't have that data,
that's where we would fall back to the reactive scaling.
Now the last type of scaling, this is actually the coolest.
This is the AWS secret sauce scaling.
Now they didn't like my title
so they decided to call it predictive scaling.
We'll go with that for now.
Predictive scaling just means Amazon
is watching your application.
It's using its really cool machine learning algorithms
behind the scenes to effectively take reactive scaling
and scheduled scaling and blend it all together.
It takes a look at your historical data
and builds a model for how many instances
you're going to need over the next 48 hours
and it revisits that model every 24 hours
to make sure that that forecast is appropriate.
Now I'm sure you are all thinking back
to that introduction slide
where I said that I was going to show you a magic trick
and I'm not kidding.
What I'm going to do right now
is I'm going to open up my console
and I'm going to show you the magically migrating instance.
So I've got my AWS console open.
Now for any folks who'd like to follow along from home,
I am picking up where we left off
from the last demonstration.
So we're going to go ahead and open up the EC2 service
and I'm going to open up my instances
and I can see here something very, very special.
This is called the magic instance.
Now you might ask why is this magic?
Well, it can magically migrate
from one availability zone to another,
solving that pesky problem of having a single point
of failure inside of my environment.
So let's make a quick note here right now
that this to availability zone B in Virginia.
Now I'm going to go ahead and scroll back over,
go to my instance state here and terminate this instance.
Let me just quickly throw this away.
Now this will take a few seconds
so I'm going to check right back in
as soon as this instance has magically reappeared.
Well, it looks like that that instance
is now magically back online.
Let's take a quick note.
It's no longer in availability zone B,
it's in availability zone C.
Now just to prove to you that this was not a fluke,
I'm going to terminate this instance once more
and we'll see that magic trick again.
Terminating that host and we'll check back in
as soon as that new host is up and running.
Ta-da, this is not actually a magic trick,
it's not anything that we haven't already talked about.
I see that the instance is now running an availability zone
A and trust me, I could do this all day long
terminating the host, bringing it up again
and you might be saying Alex, why does this matter?
Well, it's not really a magic instance.
This right here is called a steady state auto-scaling group.
And it is a topic that you might see on the exam
so let me quickly explain.
On the left-hand side,
if we scroll all the way down to those auto-scaling groups,
we're going to find my magic instance auto-scaling group.
Notice a really interesting setting right here.
I have a min, max, and desired capacity set to 1.
Now why would I want to do this?
Well, there are some times where you cannot have
multiple copies of an EC2 instance.
This is a legacy code base.
This is a system where you can't have
more than one running at once,
but you still want it to be highly available.
This is your best bet.
Now this might be something that you see on the exam
if it talks about instance needing to be online
but not able to have more than one copy.
Now the reason why it migrates between availability zones
is on failure of that instance, namely termination,
it will be migrated or randomly picked
between the subnets that I have selected here.
As we can see, I'm using availability zones A, B, and C.
So it might come up in the same subnet more than once,
but most likely it will migrate between those AZs
and if AWS detects that there's an outage
in an availability zone, it will not use that downed AZ.
Well hopefully this was helpful to clarify
why we'd want to have that steady state group.
Let's jump back to the slides
and finish up with some exam tips.
My first exam tip is it costs how much?
We need to have a really good understanding
of how auto-scaling handles scaling policies,
that min, max, and desired capacity
because all of these play into cost.
You're going to be presented with scenarios
where you're going to need to know how do we scale,
but how do we keep it cost effective?
For example, you might be given a scenario where
maybe you're asked to pick a scaling solution
to handle an increase in CPU for your application.
One option could be I create an auto-scaling group
with 5,000 instances, and they're all r5 24 xls
and another is I create a very gradual increase
and decrease of my instance count
using reasonably sized EC2 instances.
Now that's a bit of an exaggeration.
It's never going to be that easy,
but you're going to have to be able to look at it
and say how do I solve this problem
while not going completely overboard
and over engineering solutions?
Now a few more tips here for us.
The first one is going to be
we want to scale out aggressively.
Get those instances online to handle that workload,
and then scale in conservatively.
Once they're up, slowly roll those numbers back in.
Because you paid money to get those instances online,
you might as well take advantage
of having them up and ready.
When it comes time to provision your instances,
we want this to be as quick as possible.
The more that we can bake into those AMIs,
the faster the instances come online,
the shorter that warmup period is going to be,
and the better value I'm going to get
out of my architecture.
We've talked a lot about costs in these lessons.
Use reserved instances to cover the minimum count.
If the scenario allows you to do so,
you can use spot instances to scale with
and then fail back to on demand
if the spot market is not available,
but you always want to be thinking
how do we optimize those EC2 instance costs?
CloudWatch is your number one tool for telling you
do you need more or less of something?
This is what's going to tell auto-scaling
that it needs to adjust that desired capacity
based on those scaling policies that you've set.
And finally, the best way to avoid a magpie attack
is to stay away from their nesting zones.
If you can't do that,
I generally recommend wearing a hat
with Googly eyes on the back
because magpies will not swoop you
if they can see your eyes.
Alright, everybody, thanks for going through this lesson
on auto-scaling policies with me.
I can't wait to see you in the next video.


Scaling Relational Databases
============================

Hello, Cloud Gurus, and welcome back.
In this lesson,
we're going to take a look
at how do we scale those relational databases?
We'll start off with the 4 ways to scale
those relational databases.
Whether that's vertical scaling,
all the way up to serverless, and everything in-between.
We have a variety of tools at our disposal.
And then of course, we're going to get to see that hands-on.
We'll jump into the Console and get to explore:
what does it look like to take an RDS instance
and make it just a little bit more capable?
After that, we'll come back to our slides
and wrap it all up with some exam tips.
So let's go ahead and jump right on in.
When we talk about scaling relational databases
inside of AWS, that's really just code for scaling RDS.
The very first way that we can scale RDS
is with vertical scaling.
Now, I know what you're saying.
You're like, "Alex, we just talked about Auto Scaling
"those EC2 instances.
"We just talked about the benefits of horizontal scaling."
And I am completely with you.
However, there are times where vertical scaling
just makes sense.
There is no situation
in which you're going to be able to take
that t2.micro RDS instance
and somehow throw it into production environment
and throw lots and lots of traffic at it
and not have it fall over.
At some point, you just got to give it more juice.
You got to give it more CPU.
You got to give it more RAM. You've got to resize it.
So that can be one of our easiest and potentially
even our first steps is to make sure
that our architecture is appropriately sized.
Scaling storage with RDS is very, very simple.
In fact, if you were using any sort of database engine,
besides Aurora, all you have to do is adjust the settings
and say, check box, yep. I'd like some more disk space.
That's it, AWS gives you some more and you're done.
Now the downside to this is, you can go up,
but you can't go back down.
If you add in more storage than you need,
there's no way for you to somehow take
your Amazon scalpel out and cut out all the unused bits
and shuffle things around.
So just be careful.
Add more as you need it, but don't add too much.
Now the caveat to this is Aurora.
Aurora automatically scales your database storage
in 10 gigabyte increments.
So there's no need for you to go in and resize
because AWS is doing that for you.
Now, you can't technically scale
your RDS architecture horizontally.
You're not going to have a fleet of instances spread over
those Availability Zones
like you would with your EC2 Auto Scaling groups.
However, you can create read replicas
and sort of scale horizontally.
Now by sort of scaling,
what I mean here is if you have a read-heavy workload,
you can create additional read-only copies
of your datasets and spread them out across
your Availability Zones or even cross-Region.
This helps because you don't have to send all
of those reads back to the primary writing database.
You can send your reads to those additional resources
and you've just offloaded a large burden of work
from your architecture.
Now, this is going to be a big exam point.
Anytime you see relational databases
and read-heavy workloads together,
you should automatically be thinking, read replicas.
And we can even build out up to 15 read replicas
with Aurora, that's a lot.
Now the last, and in my opinion,
the coolest way that we can scale
is using Aurora Serverless.
Aurora Serverless shifts all
of that burden of management onto AWS.
Now, unfortunately for us,
we don't have time to dive into the inner workings
of how this is going to function.
As far as the exam is concerned though,
it's not particularly relevant.
All you want to look for is,
if you're seeing a workload inside of RDS
that's unpredictable and it's using Aurora,
we should be thinking Aurora Serverless
when it comes time to pick that answer.
That's about as deep as we really need to dive
into that particular section,
at this point in time.
Now all of these scaling types,
well, it's great to learn about through theory,
but it's even better to get some real hands-on experience.
So let's go ahead and open up the Console
and walk through scaling out one of my RDS databases.
Hey folks.
So as you can tell, I've got my AWS Console open.
Now, for those folks who like to follow along at home,
all I've done here to set up this demo is just spun up
a MySQL Aurora instance
using all the base default settings.
So what we're going to walk through now is:
What does it take to scale
using read replicas and vertical scaling?
Now, this is a pretty simple process,
but it's going to be important to keep in mind on the exam.
So let's go ahead and click that Modify button.
Now scrolling down here a little bit,
I'm going to see that right now I have that t3.small:
2 virtual CPUs, 2 gigs of RAM.
This is great in that dev environment,
but it's not going to be awesome in my production setting.
So let's go ahead and switch to that r-class.
Now here I see something that's a little bit larger.
Now, if you are following along at home,
please don't actually resize it to the r5.24xl.
I'm going to, because it's fun,
but it's not worth the cost for you.
So live vicariously through me.
Now scrolling down,
we see we've got a lot more CPU, a lot more RAM.
Now because this is Aurora,
Aurora does scale that database storage
automatically in 10 gig increments.
So we don't have to worry
about scaling that underlying storage,
but just keep in mind if you're not using Aurora,
that storage is a one-way street.
Now going all the way down here,
going to go ahead and click Continue.
Here, let's actually apply that right now.
Now just a heads up,
if you are doing this in a real environment,
this will cause downtime.
That maintenance window is important,
but since this is just my test database,
doesn't really matter.
Let's go ahead and click Modify.
Now this will take a few minutes to complete.
So I'm going to go ahead and pause the video
while this resizes.
Okay, it looks like that database is now much,
much, much larger.
So I have scaled that RDS architecture vertically.
Now let's take a look at what it would take
to scale as close to horizontal as we can get.
Now that's using read replicas.
So let's go ahead in here and do a couple of things first.
Let's go ahead and add that read replica.
Now, Aurora supports up to 15 read replicas,
which, that's a lot.
We're going to add in
our first database reader.
I can size this.
Now I'm going to make it a little bit smaller this time,
just to save some cash.
Still get to decide where it's going to be placed.
I'm going to go ahead and click Add.
Now that'll take, once again,
a few minutes for it to come online.
So we're going to pause and wait for that to finish.
Okay. It looks like it's now up and running.
Now, I do want to give a quick callout here
in the Actions menu.
We also have the ability
to create a cross-Region read replica.
This can help with high availability because
if a primary Region goes offline,
we can promote that read replica
to be the new primary database in that secondary Region.
Now let's go ahead and open up that reader
that I just created.
Now scrolling down here a little bit,
we are going to see that it will have a dedicated endpoint.
This will be separate than the endpoint for our writer,
right? That primary database.
So you will have to ensure that your code and applications
have been updated so your writes will point
to that primary database and your reads will come
from one of these secondary reader endpoints.
Alright, let's go ahead and jump back into the slides
and take a look at some exam tips.
My first exam tip is
on the exam,
you're going to see situations that talk
about a relational database
that's having performance problems.
Now in the real world,
if I were to say we're going to change
that relational database to a non-relational database,
you might start crying just a little bit.
Because you'd know,
that's a whole lot of work that you have to go through.
That is so much effort to get it done.
But in the magical wonderland of AWS exams,
it's just a button-click away.
Don't shy away from picking answers that ditch
that relational database in favor of a non-relational,
or in favor of DynamoDB.
Because as we'll see a little bit later,
DynamoDB is much easier to scale.
There's less we have to do.
And it's a more managed AWS service.
Now don't discount all of the other types of scaling
that we just talked about
if switching database engines isn't an option on the test.
Let's take a look at a few more exam tips
about those relational databases.
Read replicas are your friend anytime you see a scenario
that's talking about read-heavy workloads.
If you see that phrase, think read replicas.
Now remember you do have to update your code
because those read replicas do get separate endpoints
to send those reads to.
Careful when you scale that storage.
Make sure that you're not selecting answers on the exam
that talk about scaling storage down.
It only goes up, it won't come back down.
And keep in mind that Aurora scales that storage
automatically in 10 gig increments.
Vertical scaling: Well, it can have its place
with our RDS architecture.
If you have that t2.micro database,
at some point you just got to give it
a little bit more juice.
Upsize that architecture, as we saw in that demo.
Multiple Availability Zone, we always want this turned on.
This creates that additional copy of our dataset
in a second Availability Zone
and will automatically failover
if our primary dataset fails.
The only time we don't want to have this on
is in that dev environment
and that is only for cost-saving measures.
Whenever possible, pick Aurora. AWS loves Aurora.
Aurora, as we remember from previous lectures,
is that super-tuned version of MySQL or Postgres
and it's drop-in compatible.
We get all of those cool additional features,
better performance,
and when we're looking at our exam questions,
we should generally favor this database engine over some
of the more legacy types such as SQL Server or Oracle.
And finally just remember,
if you're ever stranded outside and you need some food,
the dandelion is completely edible.
This includes the flower, stem, roots,
and can provide basic caloric content.
All right folks, thanks for going through this with me
and learning just a little bit more
about relational database scaling.
I can't wait to see you in the next video.


Scaling Non-Relational Databases
=================================

Hello Cloud Gurus and welcome back.
In this lesson,
we're going to be taking a look
at how do we scale non-relational databases.
Now everybody knows in AWS
when you say non-relational databases,
we're really talking about DynamoDB.
So we'll jump right on in
to the 2 different options that we have for scaling
or I should say for handling capacity inside of DynamoDB.
We'll get to take a look at Provisioned versus On-demand.
Then of course,
we're going to go ahead and jump into a DynamoDB table
that I have inside of my AWS console
and get to take a look at the 2 different types
as well as how do we switch between them
before finally wrapping it all up
with some of those exam tips.
Let's go ahead and jump right on in.
DynamoDB is a fully managed AWS service.
This means there's not really a lot that we control
inside of DynamoDB besides the data
and the read and write capacity for those tables.
Now, what I just described
is the traditional Provisioned model.
With the provision model,
we're going to set how many reads and writes do I need
and that's really it.
Now, if we want to get fancy
and it's actually not too hard to do,
we can go ahead and set up auto scaling.
We would set a minimum, we would set a maximum
and then instead of a desired capacity,
we're going to set a target utilization.
So we'd say, hey, I want to be using 70%
of this read
or 70% of this write capacity.
And when we hit that or go above it,
we can add in a little bit more.
Now this is great for a generally predictable workload
where you know what that lower and upper bounds should be
and it's not too surprising
when you get additional reads or writes.
You saw that one coming.
The benefit of this
is this is the most effective scaling model.
Now let's imagine for a second
that we don't have a generally predictable workload.
What if it's sporadic?
What if it's all over the board?
What if I go to zero to 100 and back down to zero
and I have no data to tell me that that's going to happen.
That's where the On-demand option comes into play.
This is perfect for when those reads and writes
need to go up and down and everywhere in between
and I don't have a general trend line that I can follow.
What happens here,
is we actually take that read and write capacity setting
and we chuck it right out the window.
We don't need it anymore.
Amazon just asks us to pay
a small fractional amount of money per read and write.
They handle all of the scaling,
they handle all of the management.
Literally all we handle is the data, that's it.
So we're offloading the performance on to AWS
and just changing that billing model.
Now, this is great.
It is simple but it costs a bit more money.
So we have 2 very different models for DynamoDB.
Let's go ahead and actually open up our console
and create a table using Provisioned or On-demand
and then switch between them
and get a feel for how this actually is going to work.
Alright, so I've got my DynamoDB console open.
Now for those folks playing along at home,
I don't have anything set up before this
so you're starting from a blank slate just like I am.
So let's take a look at the 2 different capacity models
that we talked about,
that Provisioned and that On-demand.
So we're going to start off by creating a DynamoDB table.
I'm going to give it a handy little name here.
I think aws-rocks is appropriate.
We'll give it a partition key.
Everyone loves to learn something new
and then down here,
we are going to select that customize step.
Now, here are the read and write capacity settings.
Now we'll explore that provision capacity
in just a second,
for now, I want to go ahead
and click on that On-demand button
and that's it.
This is as simple as it possibly gets.
There's no need to set up read or write capacity
or alarms or anything like that.
Amazon is just handling it all for us.
Now, scrolling all the way down,
we're going to go ahead and click Create Table
and this'll take just a second.
So I'll check back in as soon as it's finished.
Well, that didn't take too long.
Now, notice something real fast.
It does say read and write capacity mode
listed as On-demand.
There's nothing that I have to do capacity wise.
Now we do have the ability
to change back to that provision model.
So let's go ahead and do this real fast.
We're going to select this table,
we're going to open this table up,
and then we're going to go to Actions and Edit the Capacity.
Now notice we are in that On-demand space.
So let's go ahead and switch over to Provisioned.
Now unfortunately or fortunately
depending on how you look at it,
there's a little bit more work that we have to do.
Now, we do have the ability to turn off auto scaling
and then we can just set a flat amount
of read and write capacity and we're good.
However, where's the fun in that.
Let's go ahead and turn that on
and notice, I can set the minimum, the maximum
and then I don't have a desired capacity
but I do have a target utilization.
This is what we're attempting to hit.
I want to be using 70, 75%
and if it gets above that,
then I start to hit 80, 90, 100%,
then it's time to increment that up.
Here we can set the initial amount
and it'll fluctuate between that minimum and that maximum.
Now we can turn auto scaling on and off
independent of each other for reads and writes
but let's go ahead and leave it on for both.
Now, this recommendation down here is a good one.
We want to make sure that we're using CloudWatch to monitor
to make sure that we have the appropriate number
of reads and writes available to us
and we're not over or under Provisioned
with our minimum and maximum capacity.
I go in down here.
One quick thing to note,
if I do want to change from On-demand to Provisioned
that is fine,
but I cannot change back to On-demand mode for 24 hours.
That's just there
so I can't switch back and forth and back and forth,
and I guess try to gain that pricing model.
Now we're going to go ahead and click Save
and we'll pause the video as it does that table update.
Alright, looks like that just finished
with the table update.
Hopefully this has given us an idea
of whether we want to use Provisioned or On-demand
for our workloads.
Let's go ahead and jump back into the slides
and take a look at some of our exam tips.
My first exam tip is to pinch those pennies.
Cost is going be a big factor
when we're talking about scaling inside a DynamoDB
whether we want to use that On-demand
or whether we want to use that Provisioned capacity model.
If we have that predictable workload,
Provisioned is the way to go,
sporadic pick On-demand.
That's what it's really going to come down to on the exam
if you're asked about which model do we use
for our DynamoDB tables.
Now a few more exam tidbits here for us.
We need to know that access pattern,
and this will generally be laid out
in the scenario questions
so it shouldn't be a huge surprise
when you're looking at it.
You should be scanning through the answers
and immediately thinking,
do I want Provisioned?
Do I want On-demand?
Just know the difference in that billing model
and that On-demand is going to be more expensive.
Design for our database matters.
Now in our database overview--
we would've covered this a little bit more--
but remember when you create those DynamoDB tables
you have to define those partition keys.
We want to avoid keys
with similar values to avoid hot partitions.
If I'm using that same value over and over and over again,
it's not going to give me optimal DynamoDB performance.
As we saw in the demo,
we can pretty easily switch from Provisioned to On-demand,
but just note, we can only switch once per 24 hours.
That's just a quick limitation
to make sure that you're not gaming the system.
Switching to On-demand whenever you see a crazy workload
and then switching back
so just a little setting AWS has added here for us
and then keep those dollars in mind.
A lot of your DynamoDB questions will be relating to cost
if it's talking about performance.
So keep these key tips in mind.
Predictable equals Provisioned,
sporadic equals On-demand
and finally, just a quick heads up,
between 1978 and 1995,
37 people were crushed
when they tried to shake their snacks
out of a vending machine
and the moral of the story is don't skip lunch.
Alright folks,
thanks for going through that DynamoDB scaling with me.
I can't wait to see you in the next video.


High Availability and Scaling Exam Tips
==========================================

Hello Cloud Gurus,
and welcome back.
In this lesson,
we're going to take a look at some
of our exam tips
for the high availability and scaling lessons.
So let's go ahead and dive right on in.
We're going to go over 4 important questions
that I want you to ask yourself in the exam
anytime you see a question that seems
to relate to scaling and high availability.
Now the first one here is,
is it highly available?
We always want to pick answers
that include high availability.
Even if the question isn't specifically
asking: Is this solution highly available?
We still want to gear ourselves
towards picking answers
that are highly available,
unless you are presented with a situation
that explicitly calls out
that you do not need high availability,
and it is only focusing on cost,
we always want to make sure
that our resources can withstand failure
and are spread out across those multiple availability zones.
Whenever we're presented with a situation
that involves scaling,
we have to ask ourselves,
should this be horizontal
or vertically scaled?
Now, as we talked about in those previous lessons,
we generally want to focus on horizontal scaling
because we can continue to spin up more resources,
more architecture,
it helps us be more highly available,
where vertical scaling just resizes that instance,
has that natural limit.
Now that doesn't mean
that we never want to vertically scale on the exam.
You might see a situation which says,
we want to increase that network throughput
for that t2.micro instance.
The only real answer there
is you need to scale vertically.
You need to up size that
to be something that's a bit more appropriate.
So we want to generally favor horizontal,
but that doesn't mean
we always want to write off vertical scaling.
Is the scaling solution cost affective?
Even if the question is not specifically asking
for a cost effective solution,
we want to keep cost in mind.
You might be seeing a question that says,
how do we scale out this architecture?
And one of the answers
is we need to spin up that fleet of x1 32 XL instances.
And sure, would that technically solve the problem?
Yeah, but that's going to cost a lot more money
than it should.
So we always want to balance in
how much does the solution cost,
and does it meet the situation
that we're being provided with?
Could we switch the database to fix our problem?
I know what you're thinking.
If you've worked in IT before,
switching databases is a very complex task.
There's no magic button that I can just press
to move from a relational database
to a NoSQL database.
However, on the exam,
in the wonderful world at AWS questions,
we can easily flip between them
with no real work.
So when you're looking at a situation,
if it says, perhaps we have that relational database
that's just falling over
and we're spending too much money,
and we need something
that's a little bit more flexible,
we might want to think,
hey, would that DynamoDB option work for us?
That non-relational database.
We can select an answer that does that migration
even though actually doing that migration
would be a huge burden on our team.
Now that we've got those 4 questions in mind,
let's go through some specific tips
relating to auto-scaling
that can hopefully help us out on the exam.
Our first point to keep in mind is,
Auto Scaling is only for EC2 instances.
So no other service can be added into an Auto Scaling group.
It cannot be used to add in load balancers
or RDS databases or Lambda functions
or anything else.
Now, some of those services that I mentioned
do have the ability to scale,
but that is separate and independent
from those EC2 Auto Scaling groups.
And this is a common pitfall
that I've seen a lot of folks,
well, stumble their way into.
You might be presented
with a scenario that says,
we'll scale the RDS database
by using an Auto Scaling group.
And that's just not possible.
Whenever we can,
we want to select answers
that get ahead of the workload.
If you're given a situation that says, hey,
we have a predictable, consistent workload
at 5:00 PM every single day.
You wouldn't want to pick an answer
that says we're spinning up instances
after that workload has started.
If it makes it seem like there is a consistent pattern
to what's going on,
we need to get ahead of that.
This is where you're going to want
to pick an answer that says,
maybe we're spinning up those instances
at 4:45 to get ahead of that 5 o'clock rush.
It's better to be ahead,
rather than playing catch up.
You're often presented with scenarios
that talk about,
how do we better fine tune
or tweak our Auto Scaling groups?
And one of the problems
that you might run into on the exam
is that your instances are taking just too long
to come online.
You have a really long provisioning time,
and that's causing an issue
when you need those instances
to respond to a workload
that has just hit your architecture.
So whenever we can,
we want to bake everything inside
of that AMI.
By baking those AMIs,
by loading it up with our dependencies
and our code,
they're going to provision a lot quicker
than trying to do it using that user data
or another solution.
We want focus on picking answers that include
that short provisioning time whenever possible.
Let's take a look at a few more important tips here.
We want to ensure that we are spreading out.
You are never going to pick a right answer on the exam
that includes one single availability zone.
We always want to spread out our content
over at least 2 availability zones,
because you can't really make a highly available solution
in a single availability zone.
So ensure that your auto scaling groups,
and in general the rest of your architecture,
is spread out across multiple AZs.
That gives us the ability for that one AZ
to fail, the architecture
to have problems,
and for us to have that backup.
Now, in that previous lesson,
we took a look at that steady state group.
Remember, it's that Auto Scaling group
that has that min, maxed,
and desired capacity of one.
This is great for that situation where maybe
we have that legacy instance,
that NAT instance,
that resource where there can't be more
than one at a time.
We saw this during that demo,
where if that instance fails,
Auto Scaling will automatically recover that architecture.
Keep this in mind if you're asked
about creating a highly available solution
for that legacy resource
where we can't have more than one online at once.
While we just talked about
that your low balancers cannot be included
inside of your Auto Scaling groups
for scaling purposes,
they are best friends
with your Auto Scaling group
when it comes to distributing traffic
to those instances,
and also determining if an instance has failed
that health check
and needs to be replaced.
Now, this doesn't happen by default.
So ensure that you are picking answers
that include a load balancer
to distribute the traffic,
and that you've checked that check box
to make sure that any unhealthy instances
in the load balancer
are terminated by the Auto Scaling group.
Remember that is not default behavior,
and therefore we have to make sure
that it is explicitly called out
in the answers that we are choosing.
Alright, that's a bit about Auto Scaling.
Let's take a quick look at a few points
that we need to remember when it comes time
to scale those databases.
Now, a lot of the scaling questions
that you're going to run into
have to do with RDS,
because RDS has the most amount
of database scaling options available to us.
Now, we talked about those
in our RDS scaling lesson,
but let's do a quick recap here.
Remember we have the ability to scale vertically
which sometimes is needed.
Remember that EC2 instance
that's a t2.micro,
we're probably going to need something
that's a little bit bigger.
We have the ability to scale out
with additional storage
if we're running out of storage in that database,
and we also have the ability
to do horizontal scaling.
Now, I know what you're thinking,
horizontal scaling.
How do we do that with a relational database?
Well we really do that
through the use of those read replicas,
and read replicas are your best friend.
If you see a question
that calls out a read heavy workload,
you automatically want to think read replicas.
I need to spin up read replicas,
adjust my code
to point my reads to that read replica
away from that primary database.
So as you're going through the exam,
think through,
because it's more than just horizontal scaling.
We have a variety of solutions
for RDS architecture.
When it comes to DynamoDB,
it's a lot simpler for us
because AWS handles almost everything.
Now, if we want to think back
to that DynamoDB lesson
that we had just a little bit ago,
we talked about the 2 types
of tables that we could create.
If we have that consistent access pattern
where it's predictable,
where we gradually need to scale up
and then scale those reads and writes back down,
that's where we're going to want
to select the Auto Scaling method.
If we see a scenario given
to us that has an unpredictable workload,
where that access pattern spikes up and down,
and up and down,
and we can't really predict what we need,
then we want to pick that on demand option.
So when it comes to scaling DynamoDB,
it's all about that access pattern.
Alright folks,
thanks for joining me in these exam tips.
Hopefully this was helpful here
for us to summarize all
of that content that we talked about
with high availability and scaling,
and I can't wait to see you
in our next lesson.



Decoupling Workflows
======================

Decoupling Workflows Overviews
================================

Hello, Cloud Gurus, and welcome back.
In this lesson,
we're going to be taking a look at what does it mean
to decouple our workflows
and how can we apply this to our applications.
We'll start off by taking a look at tight coupling
and how this can cause problems and bottlenecks
inside of our applications.
Then we'll compare that to loose coupling
and see how loose coupling can solve some of these problems.
Once we have that under our belt,
we'll see what are we going to cover, service-wise,
in the next upcoming lessons.
And then we'll finally wrap it all up with some exam tips.
So, let's go ahead and jump right on in.
What's wrong with the diagram in front of you?
At first glance, it looks like nothing.
I've got my happy user and they're placing orders
or sending traffic through that web frontend,
which then forwards that information along to the backend
and everything chugs along perfectly, until it doesn't.
We need to keep that famous Werner Vogels quote in mind,
"Everything fails all the time'.
In this situation,
what happens if my web frontend goes offline?
What happens if my web backend goes offline?
Well, the short answer is, my happy user becomes a sad user.
This is an example of a tightly coupled application.
Now, that means that my user cares directly
about the EC2 instance that's acting as that frontend,
and that EC2 instance cares a lot about that single
EC2 instance that's acting as that backend.
Tight coupling just means that we have one instance
talking directly to another EC2 instance.
So, what we want to focus on is ensuring that we are never
tightly coupling applications.
While it is much simpler,
it leads to a lot of problems down the road.
So, how do we solve this issue?
Well, the short answer is,
we need to set up loose coupling.
Now in this situation, my end user gets the same result.
Their request is passed through the load balancer,
which gets distributed out to a fleet of EC2 instances
acting as the frontend,
which then pass traffic to that load balancer in the backend
and distributes it out to those EC2 instances.
Now with a loosely coupled application,
if one instance or multiple instances go offline
from either the frontend or the backend or both,
who really cares? It doesn't matter.
The load balancer handles only sending traffic
to the healthy architecture.
So now, I can have more than one instance.
I can have a fleet of resources,
and that frontend doesn't have to know anything about
that backend, besides send it to the load balancer,
and that's it.
The load balancer then ensures
that it gets to healthy architecture.
So as long as we have one single instance in that frontend
and one single instance in that backend online,
we're doing okay.
This is going to be a really important point
that you want to understand:
that loose coupling is better at just about
every situation than tightly coupling applications.
You never want to have one EC2 instance
talking directly to another EC2 instance.
You always want that scalable, highly available,
managed service in between those resources.
Now, load balancers aren't always the answer.
Sometimes, we don't want to have a
direct line of communication
from that frontend to the backend
through the load balancer.
We might want to have something that maybe could hold on
to that message until that backend resource is ready
to come and retrieve it,
rather than having that backend resource
needing to be online 24/7
and always ready to accept that connection.
So. let's take a look at some upcoming tools.
Let's take a look at three services that we're going to see
in the next few videos.
The very first one is the Simple Queue Service,
also known as SQS.
Now, SQS is a fully-managed, highly-available messaging tool
that we can use to decouple applications.
Effectively, it can sit in between that frontend
and that backend and kind of replace that load balancer.
So, my web frontend is going to dump messages
into that queue,
and then those backend resources can pull that queue
looking for that data
whenever those instances are ready to do so.
Still allows my applications to never have to talk directly
to each other,
but it doesn't require that active live connection
that the load balancer would need.
We'll also talk about the Simple Notification Service
or known as SNS.
SNS is your go-to tool
if you'd like to push out notifications.
If you want to take one notification
and proactively deliver it to an endpoint,
rather than leaving it in a messaging queue,
SNS is the tool for you.
We'll talk about using SNS to send out notifications
to applications as well as sending out text messages
and emails to our end users.
And then we'll take a look at API Gateway.
Now, API Gateway allows us to put a safe, scalable,
highly-available front door to our applications.
So, we can control what users talk to our resources.
Now, I promise we're going to take a much deeper look into
all three of these tools.
But for right now, let's end it out with a few exam tips.
My first exam tip is:
we never want to tightly couple applications.
On the exam, you are never going to pick a correct answer
that includes tightly-coupled resources.
You always want to focus on loose coupling.
Ensure that you never have that EC2 instance
talking to another EC2 instance.
You always have that load balancer
or that messaging queue in between,
because as far as our AWS architecture is concerned
tight coupling does not offer a meaningful benefit
over loose coupling.
In fact, it only introduces problems
that we'd like to avoid.
Let's take a look at a few more tips here.
Always loosely coupled!
You're going to hear me say this over
and over and over again.
But I just want to start getting this into your head.
Loose coupling is always the answer, compared
to tight coupling.
Never tightly couple!
Now, I'm sure you could think of a situation
in maybe your own environment or in the real world
where you would say, "Alex, we need to tightly couple.
This application cannot be loosely coupled."
And you might be right.
That might be the right solution for you.
But on the exam,
we're really focusing on only loosely coupling.
Every level of your application should be loosely coupled.
From those users coming in through Route 53,
through those load balancers,
to the internal portions of your application,
whether that's a load balancer or SQS.
Just because you have loosely coupled
the front part of your application doesn't mean
that you've automatically loosely coupled the entire thing.
You need to ensure we have no direct EC2-to-EC2 calls.
Now, there's no one single solution.
As I kind of hinted at here or specifically called out,
sometimes low balancers are the right option.
Other times, SQS might be a better fit.
And we'll talk a little bit more around the use cases
of those messaging queues in our upcoming videos.
Now, I want to leave you with one last tip here.
If you ever find yourself in a position
where a bull is charging at you,
it's best to simply sidestep, allow it run by,
and then hightail it in the opposite direction.
All right, folks.
Thanks for going through this decoupling overview with me,
and I can't wait to see you in our next lesson.


Messaging with SQS
====================

Hello, Cloud Gurus,
and welcome back.
In this lesson,
we're going to be taking a look at the Simple Queue Service,
or, as people more commonly refer to it as, SQS.
Now, before we talk about the SQS service itself,
we need to understand a really important concept,
and that is: What is poll-based messaging?
Once we have that under our belts,
we'll take a look at SQS
and see: How does this fit into our architecture?
Then, we're going to see all of the different settings
that we have to dive into.
What are the different knobs and dials
that we have to twist and turn to set up SQS correctly?
We'll take an especially deeper dive
into one of those settings, called the visibility timeout,
and how does it govern the interaction
and communication between my architecture
and that messaging queue,
before we finally round it all out
with some great exam tips.
So, let's go ahead and jump right on in.
Our very first question is: What is poll-based messaging?
Now, I like to think of poll-based messaging
like a little scenario we might all be familiar with.
I'd like to send a letter to my best friend,
so I sit down at my desk,
I write them a very nice note,
I put a stamp on that letter,
and I hand it off to the mailman.
Now, the mailman takes my letter,
drives over to their house,
and drops it off in their mailbox.
Whenever my friend is ready, they can head to the mailbox,
grab that message, and read it at their leisure.
That is effectively what poll-based messaging is.
We have a producer of messages, perhaps a web frontend,
that EC2 instance that takes a message in,
writes said message into the SQS queue,
and then my backend consumer can come get that message
from the queue whenever it's ready.
So, we can think of SQS kind of like that postman
delivering the message into the mailbox.
So, that's effectively what SQS is.
It's a messaging queue
that allows asynchronous processing of work.
Now, what does that word asynchronous mean?
Well, it's a really important concept to understand,
and it's going to function a little bit differently
than maybe how we're used to communicating
with our EC2 instances.
In previous lessons, we talked about our EC2 instance
calling a load balancer
and then that request being directed from the ELB
to my backend resource.
Now, what if I don't want a direct connection?
What if I want to write that message,
write that data to that SQS queue,
and then have that backend resource
come and get it whenever it's ready?
That way, it doesn't have to be able to immediately respond
to that message coming in via a load balancer
if it's not ready to accept that content.
It can go to SQS and get that data, retrieve that message
whenever it's ready to do so.
So, that's what we're thinking of when we say asynchronous.
It's not direct communication.
We're using that SQS queue as a buffer.
Now, well, SQS is a very simple service as the name implies.
There are a lot of settings that we have to understand
to ensure that the queue is functioning properly.
The very first one is called the delivery delay.
Now, the default value for this is zero,
but it can be set up to 15 minutes.
Now, the delivery delay does just what you would expect:
I write a message to the queue,
and then, if that delay is set at something other than zero,
the queue just hides the message for a set period of time
that I've specified before it will reveal it
if that backend instance is asking for messages.
So, why would we want to purposefully delay messages?
Well, let's imagine we have that web frontend
that's placing orders.
A user types in their credit card information,
their address,
and I need to pass that information to my backend instance.
Perhaps I'd like to delay
that notification being sent to the user
to say that their order was processed successfully
until I can verify with the credit card company
that, yep, they have money behind that order
to pay for everything.
I can set a delay in the amount of time
that there is between when my message shows up in the queue
and when that backend resource
can actually receive that content
and do something with that order.
The message size --
now, by default, you can post a message
up to 256 kilobytes of text in any format,
and that's a really important number to remember.
It does not matter the format of said text.
It could be JSON. It could be YAML.
It could just be a free-form paragraph
that you're writing to your best friend.
It doesn't matter.
But 256 kilobytes is the largest amount of text
that can fit into a single message.
Now, you can adjust that number down,
but it cannot go higher than 256.
Encryption --
encryption is very important,
as we've talked about in this course,
and we need to know what can we encrypt
versus what's already encrypted.
Well, with S3,
the messages are encrypted in transit by default.
They are not encrypted by default at rest.
However, this is a very simple thing to do.
It's just a check box.
Pick your favorite KMS key, and you're good to go.
We then encrypt the data in transit and at rest.
But keep in mind for that exam,
it's not encrypted at rest by default.
Message retention --
this is a critical concept to understand.
Nothing lives in SQS forever.
In fact, by default, messages will only live for four days.
It can be set up to 14 days
and can be set as low as one minute.
But after that window expires,
the messages are purged from the queue,
which means if we're not actively processing data
out of the queue and the messages are starting to back up,
if we don't get to them eventually, they will be lost.
There are two types of polling that are available for SQS,
and the default is called short polling.
Now, short polling just means my backend instance connects,
says, "Hello, do I have work?
"I don't.
"I disconnect.
"And then I try it again.
"I connect, disconnect, connect, disconnect,
"over and over and over."
And that does a few negative things.
First, it burns CPU cycles on the backend instance.
Second, by making all of those additional API calls,
I'm wasting money, because API calls to SQS are not free.
So, what I can do is, I can specify
by setting the connection time window
that I would like to use long polling.
Long polling just means I'm going to connect
and then wait a little bit.
I say, "Hey, do I have work?
"And then I hang out and I wait
"and I wait and I wait for those messages to come in."
Now, this is not enabled by default,
but we should focus on picking answers on the exam
that use long polling.
Now, there are some very specific situations
where you can't have multi-threaded processes
where short polling does make sense.
But in general, on the exam,
always look for that long polling as 99.999% of the time.
That's what we want to be using in every situation.
And finally, queue depth --
now this isn't particularly a setting, but more of a value.
We just finished talking about autoscaling
in the last set of lessons.
Remember, those EC2 instances
can be triggered by a whole lot of things.
It's not always CPU.
One of them could be the depth of the SQS queue.
As messages back up,
CloudWatch kicks off that alarm to autoscaling
to say we need more instances to process all of that data.
As the queue depth shrinks,
we have less messages in there that need to be processed.
We could spin down those EC2 instances
that are working to churn through that content.
And our final bonus tip here:
This is a very important service for the exam.
We will recap this in our exam tips,
but it is vital that you know all of the defaults out there
and what happens when we change any one of these settings.
Now, there's one really important setting
that we haven't talked about on this page,
and it's called the visibility timeout.
So, let's go ahead and take a look
at what that number means.
Now, this is the basic layout of SQS decoupling.
My happy little user is writing an order to my EC2 instance,
my EC2 instance places that order in my SQS queue,
and my backend EC2 instance polls the queue
saying, "Do I have work?
"Aha, I have that message.
"Let's process it."
Now, let's take a quick look at the interaction
between that backend instance and that queue.
So, my order comes in, and the message is placed
in that SQS queue.
Now, the visibility timeout is here
to ensure proper handling of that message
by my EC2 instances in the backend.
So, what's going to happen is my backend instance
says, "Ah, I have polled for this message.
"I see that I have to process this order
"from my happy little user."
SQS says, "Cool. You've downloaded the message."
And now what's going to happen
is it's going to put a lock on that message,
and this is called the visibility timeout.
Now, this is a setting that you can configure.
By default here, we just have that 30-second window.
This is for 30 seconds.
This message remains in the queue,
but no one else can see it.
So if other instances poll that queue,
the queue is going to say, "I don't have anything for you,"
even though there is this hidden message.
Now, if that backend instance fails to process that message
in 30 seconds and reach back out to the queue
to say, "I'm done, I'm good,"
you can purge that.
The message is going to reappear.
This means if my instance goes offline, if it fails,
I'm not losing that content.
So, the visibility timeout
is a vital setting for your SQS queue.
Now, let's assume everything went well.
I've downloaded that message.
The message is locked.
And 25 seconds through that visibility timeout of 30 seconds
my instance reaches out and says, "Thumbs up.
"We're good to go."
Awesome!
The queue is then going to delete that message.
It's going to say, "We're done.
"We don't need this anymore."
We need to understand this interaction,
because this is going to be a big test concept.
Now, I promise we will have some hands-on activities
with SQS coming up in the next few lessons.
But for now, let's go ahead and recap some of this
with a few exam tips.
So hopefully, I'm starting to drive this point home.
SQS is important.
It will be seen on a lot of different questions.
And we'll go hands-on and build this out
into our own environment in a little bit.
But for now, start thinking through
what settings do we have that we've talked about?
What happens when I make that change?
And why would I want to make that change
to all of those different default values?
It's important to keep these in mind,
because you will see them in many scenarios on the exam.
Let's see a few more exam tips.
Like I just said, know all of the settings.
We didn't cover all of the needed settings for the exam
in this one lesson.
We'll be introducing a few more
and seeing them in action in upcoming lessons.
So, just keep this section in mind
and the future SQS sections as you're studying for the test.
Nothing lasts forever.
Remember that 14-day maximum.
You will see exam questions that talk about,
"I haven't polled my SQS queue in 28 days.
"Where did all of my data go?"
And the answer is: it was purged,
because SQS is not a permanent message storage solution.
You're going to be asked to troubleshoot.
Why did we see lost messages?
Why do I have messages popping back up?
A lot of times, this comes down to misconfigured settings.
Maybe that visibility timeout was set too low.
I set it at 5 seconds,
and it really takes my instances 15 seconds
to process those messages,
so that lock is expiring before it should be.
Or maybe I've got that delivery delay set
when I shouldn't have that.
Know that difference between long and short polling.
Long polling is more efficient with your costs
and those CPU cycles.
And this is what we want to be looking for on the exam.
If you see a test question
talking about burning too much CPU or burning cash,
think through: are we using long polling
instead of short polling?
Size: 256 kilobytes of text in any format.
I'm going to keep repeating that
over and over and over again,
because you might see an exam question
talking about, "I've got 512 kilobytes of text,"
and you're going to think, "Nope, that doesn't work,
"because 256 is the maximum size that we can have."
And once again, remember, there is no restriction
or limit on the format of that text.
And my final tip here for you:
if you're microwaving those leftovers,
spread the food to the outer edge of the plate
and leave a circle or a hole in the middle.
This will ensure even heating of the food
and stop you from having that spoonful of cold food.
All right, folks, thanks for going through SQS with me,
and I can't wait to see you in our next video.


Sidelining Messages with Dead-Leter Queues
===========================================

Hello, Cloud Gurus, and welcome back.
In this lesson, we're going to to take a look
at how would we temporarily sideline messages
into a dead-letter queue.
Now,I know what you're thinking.
What is a dead-letter queue?
Well, we're going to start off this lesson
by answering that very question.
And we'll talk about why that name seems a little strange.
Then, we're going to open up the console and go hands-on
with setting up an SQS queue
and a dead-letter queue to see it in action.
After that, we'll come back together in the slides here
and go through some exam tips to put it all together.
Let's go ahead and dive right on in.
Now, we saw this very architecture in a previous lesson.
We've talked about my happy user
sending orders through that frontend,
storing those orders in an SQS queue,
and then the backend architecture pulling from that queue.
So, let's go ahead and throw a message right in there.
My order has just popped up.
Now, what would happen if there was a problem?
Say, for a second, that my happy user made a mistake.
What if instead of putting an address in that order message,
they accidentally put their phone number
in the address section,
and my frontend application didn't check for that issue?
Well, the message would get passed to the queue,
the backend instance would pull the message,
it would fail to process it correctly
and the message would pop back up.
And then another backend instance
would pull that same message, have a problem with it,
it would not go back and delete it from the queue,
but visibility timeout would expire
and the message would pop back up.
And that would repeat over and over and over again
until we hit that maximum retention window for our queue,
which is at most 14 days
and then the message would be terminated
and we would lose that data forever.
So, what are we to do to solve this problem?
Well you can probably guess
what we're going to do right now.
We're going to set up a dead-letter queue.
Now, really all the dead-letter queue is,
it's just another SQS queue
that we can temporarily sideline messages into.
So instead of having that message languish
in that primary SQS queue
once we hit the number of retries that I've specified,
the message is going to get moved to that dead-letter queue.
The benefit of the dead-letter queue
is that I can sideline those problematic messages
and I don't just have to leave them
in that SQS queue forever.
All right, well, this sounds good on paper,
but of course it's going to be a lot more fun
when we see it in action.
And this is all great to see in diagram form,
but to me, it's not really real until we actually build it.
So, let's go ahead and flip open the console
where we're going to set up this
SQS to dead-letter queue relationship.
All right, so I've got my console open here.
Now, for anybody following along at home,
just note I don't have anything pre-built for this demo,
so I am starting from scratch.
So, let's go ahead and create our first queue.
Now, if we're going to create a dead-letter queue,
we need to have that queue created
before we create our primary queue.
I know that's a little confusing, but I hope
it'll be sorted out with a console here.
So, I always thought that the dead-letter queue
is such an aggressive name.
Let's call it something a little bit more fun.
This will be the second-chance queue.
All right, that's a little bit better.
Now do note, these are all those settings
that we talked about in the last lecture:
that visibility timeout, delivery delay,
maximum message size, retention period.
This is going to be a big one,
For your dead-letter queue,
you might want to bump that up a little bit longer.
Let's hit that 14 day maximum
and then, scrolling down here,
I have that access policy that I can set.
This is kind of like that resource policy
that I set on my S3 buckets.
Now, as this is going to be a dead-letter queue,
I'm not going to explore this option just yet.
We'll come back to it in a quick second.
So, I'm going to go ahead and click Create. All right.
My second-chance queue is now up and running.
So let's make another queue real fast.
I'm going to go back here, click Create,
and then let's give it a fun name,
learning-is-fun,
because I think that's an accurate statement.
Now, all the other settings,
I'm just going to leave as default,
except for I'd say the most important setting for this demo:
that dead-letter queue.
Now here, I'm going to say it's enabled,
and then I have to choose that ARN,
that Amazon Resource Name,
for the queue that we just created.
This is why I said, a minute ago,
I have to make the dead-letter queue first
before it can be subscribed to other queues
to act as that dead-letter queue.
And once again note, the dead-letter queue is not special.
It's just a standard SQS queue
that acts as that reject folder.
Now, this setting here is very important:
maximum receives. What this says
is effectively the maximum retry value.
How many times can I read that message?
And it pops back up in the queue
before it gets sidelined into the dead-letter queue.
Now, I probably want to leave this to be something
a little bit higher than 1,
because 1 just means you get one go at it
before it gets sidelined.
But for this particular demo, I'm going to set it at 1
just so I can quickly get the point across.
Now, scrolling down a little bit further,
I'm going to click Create and that's it.
My queue is now up and running,
and notice that that dead letter queue is set as enabled.
So, let's go ahead and send and receive
some messages from my learning-is-fun queue.
So, let's add in a message, "Hello gurus."
I think that's simple enough.
Now, I'm going to go ahead and click Send.
Great! I have just sent my very first SQS message.
Now, if I scroll down here a little bit,
I can interact with this queue.
Now, I do quickly want to call out,
this is not generally how you would interact with the queue.
Most of the time, this will be done programmatically
using the command line, using your API calls.
It's just a little bit easier to show you this in the GUI.
The GUI would be great for a quick spot check
or some troubleshooting.
Now, notice here I have one message currently available.
Let's go ahead and pull for that message.
And I found it.
We're going to go ahead and open this up
and I have that message, "Hello gurus."
Awesome, I'm going to go ahead and close that out.
Notice, right here, that receive count is 1.
How many times has it been received?
So, I'm going to go ahead and stop that polling,
and I'm going to try again.
Now that that polling is finished, huh.
No messages available.
I didn't get any messages. What happened?
Well, I received the message once.
I hit that receive count limit of 1
that I'd set for my dead-letter queue,
and the message was moved out of this queue
into that secondary queue, into the dead-letter queue.
So let's go ahead and see if we can track that message down.
So, I'm going to go back up here.
So now, I'm back on the main page here.
We've got our two queues, that learning-is-fun
with zero messages available,
and we can see now my second-chance queue
- technically the dead-letter queue --
now has one available message in it.
So, let's go ahead and check that out.
I'm going to open up that queue here,
going to go ahead and send and receive messages.
Scrolling down a little bit longer,
I'm going to go ahead and click Poll.
I do have a message.
We're going to open that up: "Hello gurus."
Once again, I see that content.
But also notice something interesting
down here on the right-hand side.
It says it's been received twice now.
That received count carries over with the messages.
So, I can see how many times these have been retried.
Hopefully, that gives us a good idea
of what it looks like to send and receive messages
and the point of that dead-letter queue
to sideline my content,
so it doesn't just get
processed over and over and over again.
Let's go ahead and pull the slides back up
and go through some more exam tips
before we call it quits here.
My first exam tip for you are the
dead-letter queue are the best sideline.
In fact, they're the only sideline.
They're the only way that we can take
our problematic SQS messages and temporarily set them aside.
Now, the reason why I keep saying temporarily
is because, as we saw in the demo,
dead-letter queues aren't special.
They're just standard SQS queues
with all the same knobs and dials and settings
as our primary queue.
But they're just a location to hold onto the content,
so I can go and deal with that
while my application chugs along and does its thing.
Now, of course, we do want to set up
that CloudWatch alarm to monitor that queue depth,
because if the queue starts to fill up,
you know we have a really big problem.
Now, without this alarm,
we really wouldn't have any knowledge
that the queue was backing up.
So, it is important to select answers
on the exam that include this additional step.
Let's take a look at a few more exam tips here.
Monitor, monitor, monitor! I can't emphasize that enough.
CloudWatch is your friend.
Set up those standard alarms that we've been talking about,
like queue depth for auto-scaling for our primary SQS queue,
but also set up that alarm for the queue depth
of that dead-letter queue to give us a heads up
before things go horribly off the rails.
Dead-letter queues aren't special.
Despite the name -- it's kind of a weird name --
it's just a standard SQS queue,
so don't think of it as some sort of
special setup or anything along those lines.
It conforms to all of the same rules
that we've already talked about with SQS.
It's just set up as that secondary option.
It has that same retention window.
This is important to remember for the exam
because, like I said earlier in the SQS lecture
you might see questions
around keeping messages in this queue past 14 days,
and you should immediately know that that answer is wrong
because it's just that standard queue.
So, 14 days is the longest a message can live
in a dead-letter queue.
Now, I know I keep hinting and talking about the SNS service
and we haven't covered it in-depth.
And I promise that lecture is coming.
We're getting to it very soon.
But for now, just know
that we can set up an SQS dead-letter queue for SNS topics.
So similar to our standard SQS queues,
if a message in SNS fails to deliver,
we can sideline it in that SQS dead-letter queue
for further processing and investigation
to see what went wrong.
And then finally, if you're like me
and maybe a little bit forgetful sometimes,
the night before a big trip,
place all of the things that you don't want to forget
on top of your shoes,
because nobody walks out of the house
and forgets their shoes.
All right, folks.
Thanks for learning about dead-letter queues with me.
I can't wait to see you in the next lesson.


Ordered Messages with SQS FIFO
==============================

Hello, Cloud Gurus,
and welcome back.
In this lesson,
we're going to take a look at a very specific type
of SQS queue called FIFO, or First In, First Out.
We'll start off by taking a look at a standard SQS queue
when it comes to message ordering and message duplication,
before we introduce our potentially better FIFO queue.
After we talk about it, then we have to build it.
We'll jump into the console
and build out an SQS queue and an SQS FIFO queue,
and compare them using the tools themselves.
After that, we'll wrap it all up with some exam tips.
So, let's go ahead and jump right on in.
Now, there are two big -- I want to say -- gotchas
that we have not quite covered with SQS,
so we're going to go ahead and do that right now.
So, let's imagine we are in this scenario:
I have built out a standard SQS queue,
and I would like to feed into it four different messages,
so let's go ahead and pop those in right now.
Now, I've loaded them up into the queue,
and it's time to read them back.
So, I poll SQS and I say, "Hey! Give me those messages."
And they went in 1, 2, 3, 4,
but they might come out 2, 1, 3, 4.
SQS offers best-effort ordering,
meaning we're going to give the old college try
and give it a shot.
Maybe they come out kind of like they went in,
and maybe there's a little bit of jumbling going around.
The other issue here is, I might get a message twice.
Now, it doesn't happen very often.
It's not saying this happens every single time.
But more than once could be a problem for me
depending on my application.
Now, this all does depend
whether this matters to you or not.
Let's imagine for a second
that I was hosting a gaming statistics website,
and a user submitted their high score.
And I was reading from the queue,
but I saw the wrong high score first
and I'll deal with it.
I'll graph it.
It's not the end of the world.
That's not a huge deal.
Standard FIFO would be perfect.
Now, let's imagine that we're not building a gaming website.
Let's imagine that you work at a bank.
Would you care if your bank processed your debits
and credits in the wrong order,
and maybe duplicated one of those?
I would probably care.
That's not going to be a great situation to be in.
So in this case, where ordering matters
and duplication matters, that's where we have FIFO.
First In, First Out.
It is right in the name.
So if I load my messages up 1, 2, 3, 4,
I put them through that FIFO queue,
they come out 1, 2, 3, 4.
FIFO guarantees that ordering,
and FIFO guarantees there will be no duplicate messages.
Now, you might be looking at this and thinking, "Alex,
"why would I ever pick the standard queue?
"It just seems like FIFO is better."
Well, that's where it once again depends.
So, let's compare these just a little bit more.
With our standard SQS queue,
best-effort ordering, duplicate messages might happen.
Like I said, it's not a promise,
but eventually you'll see a duplicate.
And it offers nearly unlimited transactions per second,
meaning I can effectively have as many instances reading,
writing, doing whatever I want to that queue, and I'm good.
Where FIFO -- guaranteed ordering, no duplication,
but it limits you to about 300 messages per second.
It also is going to cost you a bit more money,
because Amazon has to do some computing on its side
to deduplicate your messages and reorder them correctly.
So, there's less work that you have to do,
but it potentially has that increased cost
and that hit on throughput.
Now,let's go ahead and jump into the console
and see what this looks like hands-on.
All right.
I've got my console open here.
And for those folks following along at home,
I am picking up where we left off from the last lesson
with one small tweak.
I have changed that maximum receive value
before it gets kicked into the second chance queue to 1,000,
because I'd like my messages to stay in the queue
for this demo.
So, we're going to go ahead
and open up my learning-is-fun queue.
Now, do note, this is a standard SQS queue.
And let's go over here and send and receive some messages.
Just to make things simple,
I'm going to add in a 1, 2, and so forth.
Let's just do, oh, 10 or so.
All right.
We've got 17 messages in here.
Let's go ahead and go down here and poll for some messages.
All right.
So in polling, I do have this set to only grab 10.
Let's go ahead and grab the first one.
17, 16.
Okay.
Best-effort ordering here.
15, 12.
We tried.
10, 7.
You probably get what I'm trying to point out.
I'm not going to get all of the messages
in the exact order that I put them in.
This might be a problem if I care about the order
in which I receive this content.
So, let's go ahead and go in and create a FIFO queue
and see how this responds.
So, I'm going to go back up to my Queues section,
and I'm going to create a new queue.
I'm going to add in FIFO.
I'm going to say yay-ordering.
Now, one quick note, if it is a FIFO queue,
it has to end in .fifo.
Now, there are some additional settings
that you can dive into around throughput limit
or on duplication scope.
That's not going to be particularly relevant for the exam,
so I'm just going to focus on the basics here,
because that's what we need to be familiar with.
So, I'm going to click Create.
And now let's go ahead and send and receive some messages.
So, let's go through that same process again.
Message 1.
But now there's a new field here.
What this is going to do
is it's going to say my message group ID.
This is a tag that all of those messages
belong to a specific group.
So when I pull that group,
none of the messages are repeated.
All of the messages are in order.
So, there are some additional fields
that I'm going to have to fill in.
Now, the last option is that message deduplication ID.
This is a unique value
that ensures that all of your messages are unique.
The deduplication interval is a five-minute window,
where if we see the same ID multiple times,
we will receive the message successfully,
but we will not pass it on to the consumer.
So, let's give this a shot.
I'm going to have my message of 1,
my message group ID of 1,
and we'll put my deduplication ID of 1.
And we'll hit Send.
Awesome!
Now, let's do it again. And again, and again, and again.
Yeah, this is fun.
All right.
Now, if we scroll down here, and we poll for messages,
how many messages do we have?
1.
And what is the message?
1.
We get the same message sent multiple times.
It's received successfully,
but the queue is cutting that out.
Let's go ahead and try this again.
All right.
Now, we've got some more messages in here.
Let's try that once more.
I'm going to click on Poll.
And let's take a look at this content.
7, 6, 5, 4.
You get what I'm doing.
There's no duplications and the messages are ordered,
where on the other side, with SQS standard,
we have the potential for duplications
and it's best-effort ordering.
Hopefully, this clears up
a little bit around when to use standard
versus when to use FIFO.
So, let's go ahead and jump back into the slides
and take a look at some of our exam tips.
My first exam tip is that FIFO is the only option.
Well, there's SQS standard, like we just talked about.
But it's the only option when it comes to message ordering.
If you're in a scenario
and it's talking about that message ordering
or duplication of the messages being a problem,
you should be looking for FIFO in the answers.
Now, there are other ways to handle message ordering.
You could include that message ID in the message itself,
and you could do it on your side of the application
using SQS standard.
But in general, FIFO is going to be the only way
that it will come up on the exam.
All right.
A few more tips here for everybody.
First off, FIFO does not have the same level of performance.
If the exam is talking about a situation
where you need to have thousands and thousands
of transactions per second,
FIFO is not going to be the answer for you,
because that max is out at about 300-ish
transactions per second.
Now, it's not the only way to order your messages.
Like I said, you could include that message order ID
in the SQS messages themselves,
and use the standard queue option,
and then just have your application poll for those messages
and reassemble it on its side.
But it is a little bit more work,
and that is a custom setup that you have to build,
where FIFO, like we saw, is just that one-click button.
Now, as we saw in that console demo,
remember that message group ID
ensures that all of my messages with that same ID
are bundled into one group that I'm then going to download
and process in the correct order.
However, there is no guarantee
that those groups of messages will be processed in order.
So, the groups might come out of order,
but the messages inside of those groups
will always be in order.
Always keep cost in mind.
Just remember on the exam:
FIFO does cost you a little bit more,
because AWS is spending that compute power
to deduplicate those messages and reorder that content.
And then one last tip here for you:
let your jar of peanut butter
sit upside-down in your cupboard.
Then when it comes time to stir it,
all of the oil is already at the bottom
when you flip it over.
And trust me when I tell you,
it makes it much easier to mix.
All right, folks.
Thanks for going through these FIFO queues with me,
and I can't wait to see you in the next video.


Delivering Messages with SNS
==============================

Hello, Cloud Gurus, and welcome back.
In this lesson, we'll be taking a look
at how do we deliver push-based messaging
using the Simple Notification Service
or, as it's more commonly known, SNS.
Now, the first question that we're going to have to answer
is: What is push-based messaging?
And really, how does this compare to that previous type
of messaging that we've been talking about, poll-based?
From there, we'll jump into the service itself.
What is SNS and why would I want to use it?
And then what are all of the important settings
that I need to be aware of in order to set it up correctly?
Of course, we're going to have that hands-on activity
where I'm going to open up my console and walk
through sending out emails, sending out SQS messages,
using SNS, before we come back together
and wrap it all up with some exam tips.
So, let's go ahead and jump right on in.
Now, that first question: what is push-based messaging?
If you recall back to our previous lectures,
that poll-based messaging
is where the mailman nicely takes your letter
and delivers it to your mailbox
and you get to come to the mailbox anytime you're ready
to get your mail.
Push-based messaging is similar but also very different.
Instead of the mailman nicely delivering that letter
to your mailbox, the mailman takes the letter,
drives to your house and kicks open your door,
runs into your room and hands the letter to you,
and you have to be ready at any point in time
for that mailman to show up with that letter.
So instead of you getting to decide when you want to go
and get that letter, it's kind of decided for you.
Now, where does this fit into the AWS architecture?
Well, the answer is we can use SNS to do a whole lot
of cool stuff for us.
So, SNS can go deliver information on our behalf.
Like I said, we don't have to wait
for the end users to go get that content.
Now, this can be useful in a couple of different situations.
Maybe you would like to go ahead
and send out an email anytime something happens.
Anytime that CloudWatch alarm goes off, send that email,
send that text message.
You get to decide, "Hey, X event happened
and Y individuals or endpoints need to know about it."
So, that key word that we want to associate with SNS
is push-based message.
Now, what do we need to be aware of
with this push-based messaging?
Well, to me, it is a little bit simpler than that poll-based
because, as the message comes in, it immediately goes out,
so we don't have to worry
about anything like visibility timeouts
or message locking or anything along those lines.
But there are still some settings
that we have to configure.
The very first one is: what is subscribed to your SNS topic?
So as we'll see in a few minutes, the general process is
I create an SNS topic, and from there, I decide
what is going to receive data from that topic.
So when a message is pushed to that topic, who or where
or what does it go out to?
Well, we have a variety of different options here.
The first is Kinesis Data Firehose.
Now, I promise we'll talk more about Kinesis
in our upcoming big data lectures,
but for now, it's just good to know that this is an option.
You have Lambda. You have email, HTTP, HTTPS, text message.
You also have SQS.
Now, you might be thinking,
"Why would we want to chain together SNS and SQS?"
That seems a little counterintuitive.
Well, as we'll see in the demo in a few minutes,
we can do a really cool thing called a fan-out,
where one message comes into SNS, and then it is duplicated
and sent to multiple SQS queues because, perhaps,
we have different groups of backend workers that all pull
from separate queues that need to be alerted
that that information has come in
and they have to process it.
Now, that message size, we already know this one
from our SQS lectures: 256 kilobytes of text in any format,
just the same as those SQS messages.
We have support for that dead-letter queue.
As I mentioned in our previous lesson
around those dead-letter queues,
this is not a special queue. This is not another SNS topic.
This is an SQS queue that is subscribed to SNS,
because SNS will not retry those deliveries
if one should fail, with the exception of HTTP.
Everything else, we would just sideline
into this queue and then go and deal with it later.
Now, we have support for FIFO or standard SNS.
And FIFO isn't quite as useful as you might think
in this situation, because the only thing
that can be subscribed to a FIFO SNS topic are SQS queues.
So really, this is only for when you're doing that fan-out
into multiple SQS queues. Otherwise, 99.999% of the time,
you're just using a standard SNS topic.
AWS offers encryption, just like SQS.
It's encrypted in transit by default
but you can add that at-rest encryption
by simply checking that checkbox and picking that KMS key.
We have that same access policy that we can apply
to our S3 buckets or SQS queues or SNS.
We can control who or what can publish information
into these SNS topics.
And our bonus tip here,
while this is an important service,
it will not, in general, be as featured heavily on the exam.
That's because there's not quite as much
that we have to tweak and set up.
And when we're on the test, we can generally
just associate SNS with any sort of type of alert
or push-based notification that needs to be sent out.
So it'll be there, but SQS is the one
that we really want to deep dive on.
Now, that's not to say that we can't have some fun with SNS.
So, let's go ahead and jump into the console,
and we'll send out that email
and fan out some messages into some SQS queues.
All right, so I've got my AWS console open
and I'm on the SNS page.
Now for those folks who'd like to follow along at home,
just know I have pre-created a few SQS queues,
just to save some time,
but otherwise we're walking through this from scratch.
So, let's go ahead and on the left-hand side here,
click on Topics, and we're going to create a new topic.
Now, let's go ahead and select that Standard,
because I'd like to send it
to something besides just SQS.
Notice, FIFO only supports SQS.
Standard supports SQS, Lambda, all of the other options.
We're going to give it a name here.
That's it. That's appropriate. A quick display name.
Now, notice down here we have encryption.
We have access policies.
We also have the ability for retry,
but it is HTTP and HTTPS only.
Now, those are going to be a little bit lighter on the exam.
That'll focus mainly on how do we use these services.
So, let's go ahead, scrolling all the way down,
and click Create.
Awesome! So, my topic is now ready
but you're going to notice I have no subscriptions.
So if I sent a message, it wouldn't go anywhere.
So, let's go ahead and fix that.
I'm going to create a subscription here.
I have to pick the protocol that I'd like.
Let's go ahead and use my SQS queues that I've specified.
I'll have the first queue here.
I'm going to click Create. We're going to go ahead
and create one more SQS subscription,
just so we can really hit on that concept
of fanning out, where the message comes
into SNS and is duplicated to multiple SQS queues,
so different backend worker groups can pull off
of those queues independently but get the same message.
So, let's click Create again.
I'm going to add in that second queue.
Awesome.
Now, let's send to more than just SQS.
So, let's go ahead and click Create one more time here.
Now, I'm going to go ahead and select Email.
Click Create. Now, one thing to point out,
this goes into Pending Confirmation.
With email, you have to opt in,
so I do have to open up my email account
and say, "Yep, thumbs up,
"I would like to confirm this subscription,"
before any messages will end up in my inbox.
So, I'm going to go ahead and open that up now.
What I have open here is actually the confirmation email.
So, I have to give it that thumbs up
and click on Confirm Subscription to make sure
that I can receive those messages.
All right, now that I've confirmed my email
by clicking on that link, I am all ready to go.
Now, we're going to go up here,
go ahead and click Publish Message.
I'm going to give it a title.
I think that's an appropriate subject.
Down here, I'm going to say, "Best cloud ever."
I think that works.
And I'm going to go ahead and click Publish.
All right, message was sent and that's really it
as far as SNS goes. It is a pretty simple service,
but let's actually make sure that it worked.
So what we're going to go ahead and do
is go over to Services, open up SQS.
Ah, I have two messages I have successfully completed
that fan out.
Let's go ahead and open up that queue,
Send and Receive Messages
and let's view my super secret message.
There we go. "AWS is the best cloud ever."
I think that makes sense.
Now, one quick call-out.
Now, I didn't show you this during the demo
because it is just a lot of copying and pasting and editing
and text work, but you do have to have
that access policy appropriately set up.
Otherwise, the message will not make it to the SQS queue.
So if we look at my policy here,
I'm just allowing sqs:*
to this queue from my sns-is-fun topic.
This allows SNS to push the message into SQS.
So if you are doing this demo from home,
please make sure that you add in these permissions
to your queue using the appropriate ARNs.
Otherwise, the message will not make it to your SQS queues.
Now, that's great.
Let's check out that email.
Here we have my email. I received it. It worked.
The message comes in. It gets duplicated
to the relevant endpoints that I've subscribed,
and I'm good to go.
All right, let's go ahead and hop back into the slides
and take a look at some of our exam tips.
My first exam tip here:
Anytime you see an alert or notification on the exam,
think SNS. We can use it to alert a variety
of different endpoints, whether that's email, text message
or even that Slack bot that you just set up,
because it can post to those HTTP or HTTPS endpoints.
So, pushing, alerting -- think the
Simple Notification Service.
So just remember, anytime we see that user needing to know
that an event or an alarm happened, SNS is your best friend.
A few more tips here for us: push. Easy word association:
push-based notifications = SNS.
That's what we want to use.
CloudWatch and SNS are best buds.
Now, I know that I've said CloudWatch is best buds
with a lot of other services and that's true,
but when it comes time to deliver
that CloudWatch notification, that CloudWatch alarm,
it's going to be using SNS.
We need to know all of the potential subscribers
that SNS can push data into:
email, text, HTTP, Kinesis, SQS.
Know the use cases behind all of them.
Now, one quick gotcha on the exam.
I've seen this a few different times.
AWS will try and trick you into picking SES,
the Simple Email Service.
SES is based around marketing emails,
so if your company needs to distribute out email lists,
to the end users such as your customers or advertisers,
SES would be a better fit.
On the test, they will try and trick you
to pick SES instead of using SNS
to send out those email notifications,
for example, from that CloudWatch alarm.
So in general, now 9 times out of 10,
if you're seeing an email notification, think SNS.
SES is just the distractor.
And finally here, there are no do-overs with SNS.
There is no retry for anything besides HTTP and HTTPS.
Everything else you would just sideline
into that SQS dead-letter queue
if we don't want to lose that information.
So, those endpoints have to be online
and have to be ready to accept that message.
And my last tip here for you:
whenever you see a child wearing a helmet,
tell them it looks cool.
This encourages kids to wear helmets in the future
and protects those precious noggins.
All right, folks, thanks for going through SNS with me.
And I can't wait to see you in the next video.


Fronting Application with API GAteway
====================================

Hello, Cloud Gurus, and welcome back.
In this lesson, we're going to take a look
at what does it take to build out our own
application programming interface using API Gateway.
The first question that we have to answer is:
What is API Gateway?
And why is it important to use this tool
to front our applications?
We'll then take a look at a few notable features,
some of the big selling points
of why we want to consider API Gateway,
and then we'll get to see it in practice.
Now, I'm going to review with you some of the magic
of how A Cloud Guru actually works
and how we use this tool every single day.
And then of course we'll have that console demo
to take a look at what's going on inside of my account.
And we'll come back together to wrap it all up
with some exam tips.
So that first question, what is API Gateway?
Well, our Amazon definition is
that it's a fully-managed service that allows you
to easily publish, maintain, monitor, and secure your API.
And that's great, but what it really is
is that safe front door for our application.
It allows the outside world to talk to my resources
in a way that is controlled, secured, monitored,
and helps prevent abuse.
This way, I'm not just saying,
"Hey, anybody in the entire world can use my application."
I can put restrictions around who, what, when, and how
my resources can be consumed.
Now, the things that we need to take away from API Gateway:
Security is a big one.
One of the major reasons that we want to consider
using API Gateway in front of our applications
is because we can protect our architecture using a WAF,
which is a Web Application Firewall.
Now, we're going to do a much deeper dive into a WAF
later on in our security lessons,
but for just a quick overview right now,
that WAF allows me to do things like rate-limiting,
blocking specific countries,
looking for those SQL injection attacks,
taking malicious behavior and filtering it out.
It kind of acts like a security group on steroids,
and it allows me to bake in security at the very edge
where my customers interact with my application.
It helps prevent abuse.
I can set up rate-limiting to ensure that users
aren't abusing my API.
They're not abusing my application.
I can help stop those distributed denial of service attacks.
And if you've ever been through one of those,
you can use all the help that you can get.
And we'll see in a minute it's really easy to set up.
I would never claim to be a developer.
That's just not my background,
but even I was able to build out an API endpoint
to kick off a Lambda function.
And we'll get to take a look at that in just a minute.
Now, you as a customer have been using API Gateway
to load this video, to set up your account on our website,
to do just about everything that you have been doing
on our platform.
So, how does this work?
Well, I'm going to give you a little peek
behind how the A Cloud Guru magic actually happens.
So as we've talked about in previous videos,
S3 is a fantastic place to store all of your static content.
This video that you're watching right now is stored in S3.
So, my user makes a request, they come in CloudFront,
and they land on my video that they're trying to download.
Awesome! What about that dynamic content?
That content that, well, S3 can't handle for me.
Maybe you want to change your address
or your credit card number
or set your new profile pic
or add a comment to a forum.
I can't really do that in S3, because S3 doesn't have
any sort of dynamic compute capacity.
Well, that's where API Gateway comes into play.
When you make that request, to change something
about your profile, to post that comment,
you are making an API call through API Gateway.
This call is then sent to a Lambda function
that processes that information,
returns the request back to you,
and potentially even stores that information in a database.
Now, Lambda is a serverless compute option,
and we'll talk more about Lambda
in our next upcoming lessons.
But for now, just know it's a bunch of code that we wrote
that is executed whenever it's called by API Gateway.
So effectively, what we're doing --
static content comes from S3.
API Gateway handles everything else.
Now, that's great to see in diagram form.
Well, let's hop over to my console
and see my API Gateway that I just built out for us.
I've got my API Gateway already pulled up.
Now, this demo is going to be a little bit more
of an overview of the service, because the exam itself
doesn't dive too deeply into this tool.
Really, you just need to know the use cases,
so that's what I'm going to cover here.
Then, we will get to see it in action.
So, let's go ahead and open up this API.
Now, this API is relatively simple.
It's just your basic "hello world."
Now, when I created it here,
I have it set to accept any sort of request.
So whether that's a get, put, delete,
it doesn't matter what it is.
It's just going to kick off this Lambda function.
So, the request comes in and I don't currently have any sort
of authorization turned on,
but that is an important option
that you can enable to help protect these endpoints.
And API Gateway has native integration with Lambda.
And like I said earlier, we will cover Lambda
in much greater depth here a little bit later.
The request gets sent to my Lambda function right here.
And then the response gets sent back as JSON
to the client who requested it.
That's really just kind of the bare minimum
to kicking off this function.
Now, why would I even go through the trouble
of setting this up?
Well, the whole point
is so I don't have to have AWS credentials,
so I don't have to have that access key secret key role
to kick off that Lambda function.
It's so really I can distribute to this endpoint out
to my end customers and allow them to consume
the application that I've built.
Now, API Gateway offers different stages.
Effectively, these are different versions.
So, I just have my test stage created here already.
If I wanted to, I could create a production
or a beta or a V3 or V12 stage to give it a shot,
so I can create different versions of these APIs.
Now here, that hello world call -- it's responding to that,
anything, GET, DELETE, HEAD, OPTIONS, PATCH,
POST, PUT, all of the options.
If I open it up real fast, let's take a look at GET.
I can see here the actual endpoint.
This is the URL that I'm going to hit to make my request.
So, let's go ahead and actually kick off this API.
So, I'm going to grab that URL,
open it up in a brand new tab,
hit Paste.
There we go.
Now, this Lambda function is pretty simple.
All it's really doing is just returning the basics
of my information as the requester.
So, I just get a JSON return.
I sent in a query string, right,
of "my name is Alex and my city is Seattle."
And that was passed through API Gateway
to my Lambda function to actually run this.
So, this is the entire point of what we need to understand.
It's useful to front applications that I'm hosting in AWS,
and that's about all we need to know for the exam.
So speaking of that test, let's go ahead and jump back in
and take a look at some of those exam tips.
My first exam tip is that APIs
are the front door to your application.
An API Gateway is the tool that you want to use,
rather than trying to build and manage this yourself.
Any time you were looking on the exam
and it is talking about an API, you should be thinking,
if it is a custom-built application,
we need to use API Gateway to front it.
It allows us to avoid hard-coding
those access key secret keys into my application.
So if I'm distributing, say, a mobile application,
I don't have to generate an IAM user
to make calls to my backend.
I can have API Gateway sit in the middle.
So, a few more tips here for us:
If the exam is talking about an API that you are building
or managing yourself, think API Gateway.
Sometimes, it does talk about an AWS API that is separate
that's making a call to the AWS service itself.
API Gateway can help in a DDoS attack,
the Distributed Denial of Service,
because it allows us to front our application
with that web application firewall.
And as I said earlier,
I promise we'll go through those WAFs
in a lot more depth in an upcoming lesson.
As we saw, I can create different versions of my API,
so I can have my beta, my staging, my test environment.
I could have different versions of all of those,
and then I can decide what I want to use.
I can even set credentials and open up access
for one version of my API, but not another,
and it allows me to stop baking those credentials
into my applications,
so I don't have to use the access key secret key.
This is how I get traffic from the public environment
into my AWS account when it's not specifically web traffic.
And one final tip for everybody: Brushing your teeth
while you shower in the morning
can save you up to 15 seconds a day.
So, you can take those 15 seconds and, well, they're yours.
Do whatever you want with them.
All right, folks.
Thanks for going through API Gateway with me,
and I can't wait to see you in the next lesson.


Decoupling Workflow Exam Tips
===============================

Hello, Cloud Gurus, and welcome back.
In this lesson,
we're going to be taking a look at some exam tips
for all of those decoupling services
that we just finished covering.
So, let's go ahead and dive right on in.
Now, I know what you're all wondering.
Alex, what are the answers on the test?
So, I'm prepared to tell you.
They are A, C, B.
Okay, I'm just kidding.
Unfortunately, I can't make it that easy,
but hopefully we can give you some questions
to ask yourself to help simplify that exam-taking process.
Now, the very first question --
is it synchronous or asynchronous?
The question will generally lay out
which kind of workload we're using in this example scenario.
If it's talking about that situation
where we want to process those orders,
where we want to be able to scale that backend architecture,
based on the amount of load coming in,
and we don't need that active connection,
we might want to be thinking SQS queues.
If it's talking about that frontend of our application,
where we need those direct connections to our EC2 instances,
that's where we might want to think about those ELBs.
So while we do want to decouple everywhere,
there is no no one perfect solution that I can just say,
"Oh, use this every time."
We really have to think synchronous or asynchronous.
What type of decoupling makes sense?
Is it those load balancers?
Is it those SQS queues?
Put on your logical thinking hat here.
If we're at a position where I don't want to process
all of that content at the exact same time,
and it's okay if there's a little bit of delay,
use those messaging queues.
If I need to take one message coming in and fan it out,
like we saw in that previous SNS demo,
I might want to set up SNS to SQS to duplicate that workload
so each backend can pull in the message whenever it's ready.
This is where that whiteboard,
if you're taking the exam digitally
or that scratch paper can come in handy.
I like to draw myself a nice little diagram
of the existing architecture that's laid out in the scenario
with just a few quick boxes
and then think, "How can I decouple this
"and what tool makes sense?"
Having that visual overview can help lead me
to the right answer.
Does the order of that message or that work matter?
If it does, how are we ensuring that order?
If it doesn't, am I focusing on costs?
Is there another constraint?
Do I need to care about deduplication,
or is it just a free for all as those messages go through?
And that really drives home SQS Standard versus SQS FIFO.
What type of application load will you see?
This is going to play into decoupling
as well as scaling that architecture out.
Is part of decoupling increasing or decreasing
the amount of instances or resources I have available
to handle that workload?
What would be that scaling notification?
If you recall back to that previous SQS lesson,
we talked about scaling our instance counts
based on the depth of those messaging cues.
A few more things to keep in mind here
for SQS as we go through that test:
SQS can duplicate messages.
It does happen.
However, on the exam you might see troubleshooting questions
that say they're consistently duplicated messages.
Like every single one.
In that case, it's probably not SQS doing it.
It's most likely a misconfigured visibility timeout
where I'm not finishing processing the content
before it pops back up in the queue
and another instance can download it.
Another situation I've seen
is when that developer
or the application is failing to make that final call back
to delete that message from the queue.
So while that message duplication happens once in a while,
just naturally by using the SQS service,
it is not a consistent problem.
And if it shows that it is in the scenario,
that's where you got to put your troubleshooting hat on.
SQS queues are not bidirectional,
meaning your web servers
will not be able to take return information
through that queue from the backend instances.
If you need to have that bidirectional communication,
you're going to have to have two SQS queues set up,
one queue going each direction.
That's a stumbling block that I've seen show up
on the exam a few times.
Know the defaults.
You need to know all of those SQS settings
that we talked about.
What is the standard value?
And then what would happen
if we changed that standard value?
For example, we talked about that message duplication
with the default visibility time app.
If you set the maximum message size to 128 kilobytes,
you won't be able to send those 256 kilobyte messages.
Nothing lasts forever.
Now in the SQS world,
that means data can only live in those queues up to 14 days.
After that, it's deleted.
Remember that four days is the default value as well.
I have seen scenarios where they talk about,
oh, a developer made a code change
and went on vacation for 15 days.
And then when they got back,
they noticed that none of their messages
were being processed.
And you should immediately start thinking,
"Okay, any messages past that 14 day window have been lost."
Does order matter?
If it does, think FIFO.
Now, you might run into a situation where
FIFO is not an answer,
even though it could solve the problem.
In that case,
you're going to want to think about including
those ordering numbers in the messages themselves,
and then the consumer of those messages
will have to reassemble the order on its end.
That's a little bit more work.
It allows to use that cheaper SQS Standard queue.
It's more likely, though,
that FIFO will be the right answer.
And when comparing head to head.
we generally want to prefer the managed AWS tools
as opposed to the DIY option,
so FIFO should be our primary choice
and that DIY option -- that's kind of the backup
if FIFO was not presented as an answer.
We have a few more points on SNS and that API Gateway.
If it talks about any sort of proactive delivery,
proactive notification -- email, text, push --
think SNS.
We talked about those use cases with SNS,
those Cloud Watch alarms,
and I also want to keep iterating this.
Anytime we see proactive notification
- push, email, text -- think SNS.
We talked about those CloudWatch alarms.
That's a big one.
We talked about how we can use SNS to alert us
when our architecture auto-scales.
We also want to keep in mind using SNS
to fan out those messages, like we saw in that demo,
to our SQS queues.
CloudWatch loves SNS.
In the scenarios that you're going to see,
think about using SNS to act as that notification tool
whenever a CloudWatch alarm kicks off.
In fact, if you need to know about anything happening
inside of your AWS account,
SNS is the tool that's going to deliver that message.
API Gateway -- it doesn't really require
an in-depth understanding,
but you do need to know
a few important points about it.
It acts as a secure front door to handle
that external communication coming into your environment.
That's the biggest takeaway.
If you see a scenario talking about creating
or managing an API,
look for answers that include API Gateway.
The other thing that you want to keep in mind
is that it supports the use of the Web Application Firewall
to help prevent DDoS attacks.
Now, if you're not familiar with that terminology,
no worries! We will be defining that
and talking about that Web Application Firewall
in one of our upcoming security lessons.
So, I promise we will get to that.
All right, folks. Thanks for going through
these decoupling lessons with me
and reviewing those exam tips.
I can't wait to see you in the next section.



Big Data
==========

Exploring Large Redshift Databases
==================================

Hello, Cloud Gurus,
and welcome back.
In this lesson, we're going to be taking a look
at one very large database engine known as Redshift.
Now, before we take a look at Redshift itself,
we need to understand the 3 V's of big data
to hopefully give us some context
around what kind of information
are we going to be storing in this database.
Then we'll answer that very important question.
What is Redshift?
We'll take a look at some of the use cases behind it.
When would I want to use this in an application
that I'm building?
And then of course,
we're going to jump into the console
and build out our own Redshift database ourselves.
We'll then circle back to the slides
and finish it off with some of those exam tips.
So, let's go ahead and dive right on in.
You might've heard of big data before.
Maybe this is that BI engineer at your company,
or maybe you've just been on that AWS
certifications page and seen
that they have a big data certification.
So, what exactly is big data?
Well, we have the 3 V's of big data
to help explain what's going on here.
The first is volume.
We don't call it big data for nothing.
We're talking about terabytes to petabytes of data.
Now, at a personal level, we think,
"Wow, that is a ton of information."
However, for most businesses,
you start to amass more and more information,
and that just continues to snowball as you go along.
So, you're going to have a lot of info
that you need to get insights from.
You're going to have a lot of variety.
Data comes from a lot of different sources,
and it comes in a lot of different formats.
Maybe this could be customer information
provided directly to you.
Maybe this could be stats
from an application that you published.
Maybe this could be all of the information
from those thousands of IOT devices
that you have all over the world.
Really, there's no limit to the amount
that we're going to get
or the different formats that we're going to see.
And then there's velocity. Businesses requires speed.
If I'm able to collect petabytes of data,
but it takes me 18 months to get any meaningful information
out of that data,
what's the real point of having that info?
It doesn't really get me much.
So, I need to be able to collect that data,
store it somewhere, process it, analyze it,
and then do something meaningful with that information.
Otherwise, there's no reason to collect all of that content.
So, hopefully this gives us an idea
of what we're going to be talking about.
With this huge volume, variety, and velocity
of information coming in, where do we put that content?
Well, the short answer is,
we're going to use Redshift to store this information.
Redshift is a fully managed, petabyte-scale data warehouse.
Now, for those folks who've never worked with one of before,
at its core, it's a relational database.
It's kind of like RDS,
but really just a whole lot bigger.
So, why do I want to think about using Redshift
if it's just a relational database at its core?
Well, Redshift has a few features
that a traditional RDS database wouldn't have.
So, what do we need to know about Redshift?
Well, the first thing is it's big. It's really big.
In fact, it can hold up to 16 petabytes of data.
That is a whole lot of information
- in fact, probably more
than you are ever going to have in a database --
but if you do get there, it can handle it.
This is really nice
because it means we don't have to split up our datasets
into multiple large databases.
We can keep it as one single, cohesive unit.
Redshift is relational.
Now, we talked about relational databases
in some previous lectures here,
but you're going to use your standard SQL
and business intelligence tools to interact with it.
We can really just think of Redshift
as a very big relational database.
We also need to know that Redshift and BI go hand-in-hand.
Redshift is not actually a replacement for RDS.
It's really good at answering queries
over very large datasets.
It would actually horribly fall apart
if you were trying to use Redshift
as the backend for, say, your web application.
It's not a great tool for that particular use case,
so just keep that in mind.
Redshift equals BI, and RDS handles the rest of it.
Now, it's all great to talk about this in theory,
but let's actually dive in and go hands-on
in spinning up our very first Redshift database.
All right, I've got my console open.
And just a quick note to anybody
who's following along at home,
I would recommend that you use one of these Sandbox accounts
or one of the labs to play around with Redshift,
as if you choose to create a Redshift cluster
in your own personal account,
they can get pretty spendy very quickly,
as we'll see in just a second.
So, let's go ahead and create our very first cluster.
Now, here I'm going to give it a name,
just like everything else.
Oh, I think that works.
Now, AWS does offer a free trial,
but let's just go ahead and leave that Production box
checked for right now.
Now down here,
we have to decide how many nodes
are going to be in my cluster.
Now, we can think of nodes as,
well, individual compute units that come together
to form that cluster.
When I create a cluster, I'm going to have a primary node
that's going to receive those queries,
and then effectively divvy them up
and hand them off on to the other nodes to do the work.
So the more nodes I have,
well, the more performance I'm going to get.
So, let's go ahead and select this right here.
We're going to go with the ra3.16xl.
And, well, we can see that I have to have a minimum
of two nodes, but in this case I can have up to 128.
So, scrolling down here a bit more.
Now, that is only $1.2 million a month.
Very reasonably priced.
All right, that's actually a lot of money,
but you do get all a whole lot of storage,
and a whole lot of performance for that money.
Now, we're going to leave that name
and the database port as just standard.
We have to give it a username and a password.
Okay. So now I can get into the database.
So, if we scroll down here a little bit,
we can see that it still spins up inside of a VPC.
I get to pick the particular subnet.
One thing to watch out for, though,
this is a single availability zone.
Redshift is not a highly available service,
meaning it only comes online in one AZ,
and if you want it in multiple AZs,
you're going to have to create multiple copies.
I give it a security group. That automatically backs up.
I have a maintenance window,
a little bit like RDS that we've seen previously,
and I'm going to go down here and click Create.
All right, I'm not actually going to create this cluster.
That's a million dollars a month.
That is way more money than I have to spend.
So, let's actually just shrink this down real fast.
And I'm going to go with something much,
much more reasonable here.
Let's go with one of the more cost-effective nodes.
There we go.
360 bucks a month.
Notice in this case, because of that node type,
I can cut it down to just one individual node.
So, it does vary upon the type that you're picking.
So, 180 bucks. That's not too bad.
We're going to stick with this one.
So, I'm going to go ahead and click Create.
Now, this will take a few minutes,
so I'm going to check back in as soon as it's done.
Well, it looks like it just finished here.
So if I go ahead and open this up,
it's going to land me on a page that, in my opinion,
looks a little similar to that RDS status page.
We can take a look down here at those metrics:
CPU utilization, disk utilization, number of connections,
all that good stuff.
If I take a look up here, I can go to my Properties tabs.
I can change the password. I can rotate that encryption key.
If I scroll down here,
I can take a look at which VPC it's in.
I can deal with public accessibility.
Really, there's a lot of settings in here.
One of the important ones we do need to know:
we have the available endpoints
depending on the type of connection
that you're going to make to Redshift.
And we also have the ability to directly query
this cluster from the console.
So, we don't have to use our standard database tools
or that BI application.
Although most of the time,
that's what we are going to be using.
Now, this is really all we need to know
for this particular exam,
so we're going to go ahead and jump back to those exam tips.
All right, so let's take a look
at our very first exam tip here.
We need to remember Redshift.
It's big. It's 16-petabytes-of-data big.
We don't have to know too much about Redshift on the exam
besides the fact that it's great for BI applications
and it can hold a lot more data
than the standard RDS database can.
So, just a few more tips here for us:
it's important to know Redshift is not standard,
meaning it's not meant to take the place of RDS,
and RDS is not meant to be used in place of Redshift.
They each have their own unique situations.
They each have their own unique use cases.
It's big. It's very big.
I've said that a few times, so just keep it in mind.
And it's relational.
Those are all things
that we want to keep in mind with Redshift.
You might see it as a distractor on the exam,
not trying to trick you,
but trying to get you to pick it over the use of RDS,
and we just want to make sure that we're only using Redshift
when it comes to BI applications.
And finally, folks, just a quick tip.
Driving your car at 55 miles per hour
is the most efficient speed for gas mileage,
according to the U.S. Department of Energy.
All right, folks, thanks for going through Redshift with me,
and I can't wait to see you in the next lesson.


Processing Data with EMR (Elastic MapReduce)
=============================================

Hello Cloud Gurus, and welcome back.
In this lesson, we're going to be taking a look
at Elastic MapReduce,
or as it's more commonly known as, EMR.
Before we dive into the EMR service itself,
we're going to have to define what is ETL,
or extract transform load.
Then we'll take a look at EMR,
hopefully put a definition to that acronym.
We'll see what a basic architecture pattern would look like
inside of one of our VPCs,
and then we'll hop into the console and make it happen.
And then we'll sync back up after that
for some of those exam tips.
So let's go ahead and dive right on in.
So at a high level, we need to understand what ETL is
before we can really go through
and see why would I want to use EMR.
Now for those of you who don't know,
in a previous life, I worked as a gold miner.
That's actually a photo of me right there on the screen.
So day after day, I was toiling mining gold.
Now what I would mine wouldn't actually be usable
by anybody, but the first step
is to just get it out of the ground.
After it's out of the ground,
I'd load it onto that conveyor belt
and send it off to the gold factory.
Now that gold factory would take that raw resource
and then transform it or convert it
into something that's usable.
Before we would then load up
all of those processed gold bars into our storage truck
for safekeeping and eventual delivery to the customer.
Now, while we might not be mining gold ourselves,
we are doing this same process with our data.
We have to take our large amounts of data
and extract it from that source, pull it in.
We then have to transform it into something that's usable,
something that's meaningful.
And then load it into that data source
for further analysis later on down the road.
So I'm taking all of that raw data, that raw information,
and I'm putting it into a form that I can then use.
That's the goal of an ETL workload.
Now thankfully for us,
we have EMR to help us with this ETL process.
So it's actually a managed big data platform
that allows us to process
as much information as we really want to throw at it.
And it's another one of those AWS tools
where it's not proprietary to Amazon.
In fact, it's an open-source managed version
of Spark, Hive, Hbase, Flink, Presto, there's a bunch.
You don't need to know all of those,
but what we want to focus on here
is that it's allowing us to quickly take open source tools
and get them up and running inside of my environment.
Now you don't need to know all of those.
What you want to focus on
is that EMR allows us to take those open source tools
and get them up and running quickly.
Since we have AWS managing this architecture for us,
what is it actually going to look like?
Well, when we spin up our EMR cluster,
it's going to live inside of our VPC.
Now EMR for the purposes of this exam
will generally focus on using EC2 instances.
However, it is good to know that it can also run
on EKS or Outpost, but that's really outside of the scope
of what we're going to be covering today.
So EMR will spin up the instances for us,
keep them online, manage them for us,
but they are going to live inside of our VPC.
It will then take in that data,
take in that information, process it,
put it to the form that we want,
and then store it in our S3 bucket.
Now it's always great to see these diagrams,
but I think it's more important to hop into the console
and spin up some architecture using the service itself.
So I've got the console open here
and our very first step is going to be,
well, let's click that Create Cluster button.
Now we do have to go ahead and fill in a few values here,
so let's give it a more fun name than this.
I think it's a little long, but very descriptive.
Now we do get to pick our particular versions
of the application, what tools we're going to have deployed.
I'm going to go ahead and just leave this as is for right
now. Now we do get to pick the kind of instances
that we're bringing online.
So let's go ahead and, well, we're
just going to go with something a little bit smaller here
just to make it a bit more cost-effective.
I can pick the number of instances, I can set up scaling,
all of those good tools to allow me to dynamically provision
and destroy instances when they're no longer needed.
I can pick a key pair if I need to sign into the host.
I'm just going to go ahead without one for now.
And let's click Create.
Now this will take a few minutes here
to fully get up and running.
So let's go ahead and check back in
as soon as this has finished.
So it does look like it just finished.
Now it takes about 10 plus minutes
for it to come online.
Now, once it's actually up, we're going to hop over
to our application user interface.
And here, well, I can open up those UIs
corresponding to the cluster that I've created.
So let's go ahead and click this.
And now I can start loading in my ETL jobs.
Now everything that you see here is far and above
what you need to know about for the exam.
All of this would be featured heavily on the big data test,
so we're going to go ahead and skip over that for now.
What you will need to pay attention to
is over here in my EC2 instance console.
We can see that I have three m4.large instances
up and running.
Now this is an important thing to know.
On the exam, you might see a situation about optimizing cost
or dealing with these EC2 instances.
At the end of the day, they are just EC2 instances.
Amazon will bring them online, deploy the software,
get them running, but you still might need to think about
where do they live inside of your VPC,
what security groups do they have,
and how do we pay for them.
Alright, that's about all we need to see
from the console here.
So let's go ahead and flip back to those slides
and we'll end it out with some exam tips.
Tip number 1, this is an open-source cluster.
Namely, it's a fleet of EC2 instances
running tools that have not been developed by AWS.
So it's good to know, and we can kind of treat this like
it's that ElastiCache service,
but for ETL workloads where AWS didn't build it,
but they'll help you get it online quickly.
Now a few more tips here for us.
EC2 rules apply.
You can use reserved instances and spot instances
to help reduce your overall costs.
That's a good thing to remember for the exam.
At a high level, you generally need to know
it's going to transform that data.
That's really about as deep
as this particular exam is going to get.
It'll focus more on the architecture side
of how would we manage those EC2 instances,
and namely, how do you pay for them?
And it's always good to remember
that because they are standard EC2 instances,
they live in your standard VPC
and all of the normal VPC rules apply.
And lastly, putting a scoop of baking soda
in your washing machine is a great way to remove stains
without that harmful bleach smell.
It can also help clean the washing machine drum,
keeping it nice and sparkly.
All right folks, thanks for going through EMR with me,
and I can't wait to see you in the next lesson.


Streaming Data with Kinesis
==============================

Hello, Cloud Gurus
and welcome back.
In this lesson, we're going to be learning
about all things Kinesis related.
So as you probably could have guessed
we're going to start off with, what is Kinesis?
Why do I want to use it?
And then I hope you're sitting
down because we've got Kinesis Data Streams,
Kinesis Data Firehose, Kinesis Data Analytics,
Kinesis versus SQS, Kinesis and the Turkey Sandwich.
Okay. One of those might not be real,
but it's going to be a lot of Kinesis
wrapped up with some of those handy exam tips.
And so let's go ahead and dive right on in.
So to answer that first question, what is Kinesis?
Well, Kinesis, you can kind of think of it like a highway,
a big road to transport lots of stuff.
You throw as much stuff as you want
on that highway and, well, it can move it for you.
It can take data from point A
and get it to point B in real time,
or nearly real time for some variants of Kinesis.
So Kinesis is all about moving that data along.
So right off the bat
we have 2 forms of Kinesis that you're going
to need to be generally familiar with
for this particular exam.
The first is called Kinesis Data Streams
and Kinesis Data Streams is real-time.
Now it's as real-time as we can ever possibly get.
That's always a little bit of a gotcha,
nothing's ever real-time in IT,
but this is as fast as it possibly could happen.
Getting data from one point to another.
And we're going to compare that to Kinesis Data Firehose.
Now Data Firehose, it's not quite as real-time.
It will get the data there in nearly real-time,
within about 60ish seconds,
and Kinesis Data Firehose has limited endpoints
that you can send that data to,
where Data Streams really has an unlimited possibility
of where you can send your information to.
So on first glance you might be thinking, wow Alex,
Data Streams just sounds better.
It sounds like Firehose has a lot of limitations,
and I know that's tempting, but let's take a look first
at what it would actually take
to set up Kinesis Data Streams.
So we're going to view a quick architecture pattern here.
Now we have the producers.
Producers are just something that's making data.
Maybe this is an EC2 instance,
maybe this is a web client,
maybe this is a mobile phone, a server,
it doesn't really matter what it is,
but it is something that is creating information.
Could be a log, it could be stats on those IOT sensors,
the type of data isn't particularly relevant,
but you want to move it from one endpoint to another.
So what you would do
is you would provision Kinesis Data Streams,
and you're going to have to decide how many shards
you're going to create.
Now, shards can only handle a certain amount of data.
You're going to have to scale how many shards
do you need to have.
And you're also going to have to create something
called a consumer.
Now, a consumer is just something
that's going to take that data in,
process that content, and put it
in that endpoint that you have selected.
Now that endpoint really could be anything,
because these consumers, well,
this is something that you're going to have to write.
You're going to have to use that Kinesis SDK
to build this application.
And you're going to have to handle scaling this application
by defining how many shards that you're going to have.
So the upside is, it's in real-time.
The downside is, it's honestly a lot of work
to put together.
This is why AWS started with Kinesis Data Stream,
and then released a second service
called Kinesis Data Firehose.
So let's take a look at that really fast.
So Data Firehose is much, much simpler.
Data Firehose handles the scaling for you.
Data Firehose handles building out that consumer,
namely you don't have to write that code.
This is kind of the managed version
of an already managed service.
Amazon has really shaved off all
of those square corners that you might stub your toe on.
So with Kinesis Data Firehose, you send data
to the service and it has a few supported endpoints
where you can then put that data.
It could be S3, could be Redshift,
could be Elasticsearch, right?
It could be something like Splunk as well.
There is support for limited third party endpoints.
So effectively, it does nearly the same thing
as Data Stream, it's just a lot simpler.
Now Kinesis Data Stream or Firehose,
it's not really going to process that information
as it goes through,
but maybe you'd like to.
We can use Kinesis Data Analytics
paired with Firehose or Data Stream,
to process our information using standard SQL.
Now, what this means is it's really simple
to set up a situation where data comes in
in one format and gets changed.
Maybe we will want to sanitize those logs.
Maybe our IOT devices, well, it's not
in a format that we can easily load into our database.
So we can run our SQL on this data, effectively in real time
as it's being pushed through.
There's no servers to manage.
You don't have to worry about scaling.
It's going to handle all of that behind the scenes.
So you're never stuck trying to provision shards,
or resources, or instances, or anything along those lines.
And just like the rest of AWS, you only pay for the amount
of data that you are passing through the service.
You're only paying for how much you are using.
Now for this particular exam, you are going to
have to be aware of a scenario that might pop up for you.
So if you're looking for a message broker,
that messaging queue, which service do we pick?
We've already talked a lot in this course about SQS.
That's Simple Queue Service.
It's easy, it doesn't require a lot of configuration.
The downside is it doesn't offer real-time message delivery.
So if your application requires
real-time message delivering,
namely, you cannot possibly have delay in those messages,
that's when you're going to want to look towards Kinesis.
Now, Kinesis, it's going to be a bit more complicated
to configure.
There's more work that you're going to have to do.
And generally it's focusing on big data style applications,
but it does provide that real-time communication.
So if you're on the exam and you're asked to choose
between Kinesis versus SQS, remember SQS is simpler,
but it doesn't offer real-time.
Kinesis, bit more complicated,
but it does hit that real-time checkbox.
So for my first exam tip, if you see anything
real-time, think Kinesis.
Now that real-time has a little bit
of an asterisk next to it.
If the scenario says near real-time,
think Kinesis Data Firehose.
If it says real-time, then you want to use Data Streams.
That's really what it comes down to.
A lot of the scenarios are going to be based
around speed of that data moving through.
And if it talks about streaming any sort of data,
you automatically want to be looking
for answers that include some form of Kinesis.
Now, a few more tips here for us.
Remember with Kinesis versus SQS, both can handle messages,
both can handle data, but Kinesis is the only option
that's real-time.
If you're asked in a situation to transform
that streaming data, think Kinesis Data Analytics,
because we can apply that SQL to our data
as it's moving through.
Now on the exam, if you're asked for a solution
that automatically scales your streaming service,
Data Firehose is the only option that does this for you.
For Data Streams, you're going to be responsible
for determining how many shards you need to have.
Now, if you're looking at our big data exam,
you'll have to know a bit more about this
and how do we actually pick how many shards
we're going to have to have, but for this particular exam
it's fine just to know it at a high level.
And my last tip here for everybody, if you're ever cooking
with green onions, don't throw those stems away.
If you put it in a glass of water,
they'll actually start to regrow.
And within a few days to maybe a couple of weeks,
you might have some new tasty onions for that next meal.
Alright, folks, thanks for going through Kinesis with me.
And I can't wait to see you in the next lesson.


Amazon Athena with AWS Glue
===========================

Hello Cloud Gurus, and welcome back.
In this lesson, we're going to be taking a look
at 2 AWS services, namely Athena and Glue.
So we're going to kick it off by taking a look at Athena.
What is it, why do I want to use it,
and what do I need to know about it?
And then we'll see, what is Glue?
Now, why are we talking about 2 services?
Well, they commonly go hand in hand.
So we'll take a quick look at how we can use them together
to get some pretty powerful results.
After that, we're going to wrap it all up
with some of those handy dandy exam tips.
So let's go ahead and dive right on in.
So our first question we're going to answer is
what is Athena?
Well, the official definition
is that Athena is an interactive query service
that makes it easy to analyze data in S3 using SQL.
Now think about that for a second.
Up until now, we've gone through this
what can be considered a potentially painful process
of having to transform our data and load it into a database.
What if we could just skip that?
What if we could just query the data
as it sits in that data lake, as it sits in that S3 bucket?
That's what Athena allows us to do.
Basically, we can talk directly to our data
and we don't have to go through that process
of loading it up into a traditional database.
Now that's pretty cool if you ask me.
So if Athena is a serverless query language for S3,
then what is Glue?
Well, Glue is actually a pretty handy tool.
What it allows us to do is stick things together.
You might've most recently used this in school
where you had two pieces of paper that you needed...
Oh, I'm sorry.
You'll have to excuse me,
that's the wrong kind of glue.
Now the AWS Glue is a serverless data integration service.
Effectively, what it allows us to do
is perform those ETL workloads
without managing or running traditional EC2 instances.
So what does that mean?
Well effectively, it kind of replaces EMR.
What we can do now is simply use Glue
to do that extract transform load of our data
without having to spin up a huge fleet of EC2 instances
and use third-party open-source tooling
to transform that data.
So we're talking about some serverless architecture.
So how would we put Glue and this Athena service together?
Well the actual architecture diagram
for this is pretty simple
and that's a good thing because there's very few times
in life when you're building that big data application
and you could use that word simple.
So it all starts off with your data
that's being stored in S3.
Now you have all of this unstructured data,
it's coming from all of these different sources
and effectively you need to put it
in a format that you can then easily query.
If there's no guardrails around what the data looks like,
we wouldn't be able to run those SQL queries
to get us that insight that we need to have.
So you're going to point Glue at this data
and what it's going to effectively do
is build you a catalog, build you a structure for that data.
Now once that's built,
you actually have a few different options.
Now it is good to know
about the Amazon service called Redshift Spectrum.
It's actually a flavor of Redshift
and you don't really need to know too much about it
for the exam that you're going to be taking shortly,
but at a high level,
it allows you to use Redshift without having to load
all of that data into the Redshift database itself.
Now that's great if you're already using Redshift,
but our other option here is to use Athena.
What Athena can do is take that data
that Glue has so nicely structured and run queries on it
without having to load it into that database.
It would then be very easy to use something like QuickSight,
which at a really high level
is effectively Amazon's version of Tableau.
It can help you visualize this data.
So you can simply combine nine storing data in S3,
structuring it with Glue,
querying it with Athena and QuickSight
to give you a dashboard with all of your needed insights.
Now I know, that was a very quick tour
without a lot of depth in these services.
And that's because on the exam,
they're only going to be covered at a very high level.
We have a lot of other big data content available for you
if you'd like to go deeper
on each one of these tools, but for now,
let's go ahead and wrap it up with some exam tips.
So if you're faced with a scenario
on the exam where it's asking
for a serverless SQL option,
namely where you need to query some data
out of your S3 bucket, you want to think Athena.
This could be data from a BI application,
this could also be logging data.
Keep that in mind on the exam.
You can use Athena to query logs stored in an S3 bucket.
So let's take a look at a few more exam tips here for us.
Our first one, both Athena and Glue
are going to be serverless options.
They are fully managed by AWS
and there's actually very little configuration
that you're going to be responsible for.
This makes it a pretty simple rollout on your end.
We want to keep these tools at another high level overview.
So it's good to know generally what they are.
Athena is serverless SQL, Glue is serverless ETL.
That's about as much as we're going to have to know
for this particular test.
And they do work together very well.
So while Athena can work by itself,
maybe if you already have that data
that has a structure to it,
Glue can help you design a schema for that data.
So keep both of these in mind.
It's good to know what they do
and it's also good to know when they are used
as distractors on the exam
because I've seen test questions that referenced using Glue
as a database or using Athena to query RDS.
And you need to know those are not supported functions.
And one last tip here for us.
If you're ever cleaning windows,
try using old newspapers instead of rags.
This will leave your windows with a streak free shine.
All right folks, thanks for going
through Athena and Glue with me
and I can't wait to see you in the next video.


Visualizing Data with QuickSight
==================================

Hello Cloud Gurus and welcome back.
In this lesson we're going to be taking a look
at how do we visualize data and create dashboards
for that data using QuickSight.
We'll jump right into what is QuickSight
and how does it fit in with the rest
of our big data architecture?
We'll see how we can visualize some data that we have, well
transformed and massaged in previous lessons
and then we'll get a quick console walkthrough
just to see what this looks like
to get a visual representation before we loop back
to some of those handy exam tips.
So let's go ahead and dive right on in.
So what is QuickSight?
Well, we have our official definition here
that it's a fully managed business intelligence,
short for BI, data visualization service.
Basically what it does is it takes your data
and it makes it look pretty.
If you've ever worked with a tool such as Tableau
or really anything that can consume data
and make a graph out of it,
that's what QuickSight is doing for us.
It allows us to move away from
I've got these spreadsheet kind of looking collections
of data to I can now look at something that makes sense
to my very graphically focused human brain.
That's what we're doing here.
We're interpreting the results of all of that data
that we've been collecting.
So where does QuickSight fit into the AWS platform?
Well, we actually saw this graph in a previous lecture
about Glue and Glue Data Catalog, right?
We're collecting that information from our S3 bucket,
we're processing it with Athena,
we're running those queries and we're using QuickSight
to front this architecture.
So Glue and Athena do the heavy lifting
and then QuickSight sits in front of that data
and gives us something to look at.
And that's really all we need to know for this exam.
It's going to be a data visualization tool.
So speaking of visualizing data,
let's hop into the console and see this in action.
All right, as we can see I've got
the QuickSight service open inside of my AWS account.
Now I'm not actually running the HR team at A Cloud Guru,
but if I was I might have a dashboard
that looks something like this.
Hiring in different regions, hiring by business functions,
gender diversity inside of the organization,
this is actually just some sample data
that was given to me by QuickSight.
Now in your actual situations, in your actual teams
and companies, this would be your own information.
But hopefully this gives you a quick overview
of the visual interpretation of this data
that you are pulling in and analyzing
with the other big data tools
and then displaying using QuickSight.
Now almost everything on here is not going to be
particularly relevant for the exam.
What I'm just trying to get across
is that it visualizes your data.
Well this was a really quick overview
of what QuickSight can do, let's go ahead
and jump back into those exam tips.
So when we're taking the test
what do we need to look out for?
Well, QuickSight, as we can tell,
is associated with creating dashboards.
So if you see a question talking about interpreting the data
that you're collecting or reviewing
inside of an application,
QuickSight is going to be the answer
to create that dashboard for the business intelligence team.
So if you see BI, think QuickSight.
Just a few more tips here.
So do you need to look at your data?
Well, you're going to use QuickSight to do it,
at least on this exam.
I know there's a ton of other products out there
that have similar functionality, but on the test
we need to associate visualization of information
to QuickSight.
Now like we saw, this was a very, very quick overview.
Mostly, I just wanted you to be generally familiar
with the service.
There's not going to be a question talking about
specific filters or queries or anything like that.
It's going to be a lot more on those big data exams
that AWS offers.
For this particular CSA test,
we just need to have the very high-level overview.
And some great word association here for us,
business intelligence loves to look at dashboards,
loves to look at graphs, so we're going to use QuickSight.
And finally, are you looking to add some protein
to your morning breakfast cereal?
Add a scoop of peanut butter
for a delicious and healthy treat.
I know that's what I had right before I filmed this content.
All right folks, thanks for quickly going through QuickSight
with me and I can't wait to see you at the next lesson.


Analysing Data with Elasticsearch
==================================

Hello, Cloud Gurus and welcome back.
In this lesson, we're going to take a look
at the Elasticsearch service
and we're going to see what it brings
to our big data architecture.
So the first question we're going to have to answer
is what is Elasticsearch?
Why do we need it, where does it go
and then we're going to answer that important question
of what can it do?
Now, this lesson's going to be relatively short
because as we're going to see,
we only need a very high level overview
of this service for the exam
but of course, we're going to end this out
with those exam tips.
So let's go ahead and dive right on in.
So what is Amazon Elasticsearch?
Well, the official definition
has it as a fully managed version
of the open-source application Elasticsearch.
This is where, I'm not going to lie,
it might get a little bit confusing.
Normally, AWS will rename their product
to make it a little bit more clear
for when it's just managing an open-source service.
However, in this time, they just chose
to reuse the name.
So what Elasticsearch is, is it's not an AWS product.
It is an analyzing tool.
It's basically a search engine,
as we'll see in just a second.
Amazon Elasticsearch is a full managed version
of that open-source tool.
Hopefully you're not confused just yet.
Basically what it does is that same heavy lifting
that we would normally do
to have to spin up the architecture,
it's just AWS doing it for us.
And what Elasticsearch does
is it allows us to easily search over our data,
allows us to analyze that freeform information
that it's collecting.
And as far as the exam goes,
it's going to be primarily used in an ELK stack,
Elasticsearch, Logstash, and Kibana.
This is a tool that's commonly built
to analyze logs.
Well, with that confusing naming out of the way,
let's see what we can do with this.
We are collecting information.
Logs, messages, metrics,
all of this freeform, free-structured data.
If you think about that log file,
there's not always a standard
across all of our different logs.
But we still have to hold on to it.
So we're bringing that data in
and what Elasticsearch does
is it allows us to search over all of that content,
analyze that information.
We can think of it kind of like the Google for our data.
Now, using this tool, we can look for problems.
Most commonly in the exam,
you might hear about it with those logs.
The idea is if you want to build
that third-party log analyzing service,
namely not using CloudWatch Logs,
you'd feed your logs into the Elasticsearch service
and that'll allow you to search over that content,
build trends, and look for real-time problems as they
happen. At a high level,
very similar to what CloudWatch Logs allows us to do.
Now, I know, that was a very, very quick
and high-level overview of this tool
but I want to keep it easy to understand in this case
because it is very unlikely to pop up on the exam.
I have seen the tool used as a distractor before.
So what I want you to associate this with
is Elasticsearch equals ELK equals logs.
We are just looking for a way
to analyze and visualize our logs,
using not CloudWatch Logs.
That's what we want to take away from this.
If the scenario is looking
for a third-party logging solution,
look for that Elasticsearch, Logstash, and Kibana
or ELK stack.
So just a few more quick tips here for us.
Why do we want to do it ourselves?
We don't.
Let AWS do that heavy lifting.
I know they named their service the same
as the open source service.
I get it, it's confusing.
If you see it on the exam,
it is definitely talking about the fully managed version,
that Amazon Elasticsearch,
because that's where the focus is going to lie.
So this is one of those other services
that we just need to have that really high-level overview.
We don't have any labs on it.
I would not suggest going through
and building out that ELK stack yourself.
It could be good practice for a future job.
Your boss might ask you to do it
but I wouldn't expect to see this on the exam.
In fact, I'd most likely expect
to see Elasticsearch as a distractor,
trying to trick you.
In general, we would probably want to focus
on using CloudWatch Logs to process our information
unless there is a specific call-out
that they don't want to use CloudWatch Logs
or that they want to use that third-party solution.
And finally here, if you're ever on a phone call
that you want to end
but you don't just want to hand up,
put your phone on airplane model.
The caller will see call failed
instead of call ended
and you can blame it on network troubles.
Alright, folks, thanks for quickly going
through Elasticsearch with me,
and I can't wait to see you in the next lesson.


Big Data Exam Tips
====================

Hello, Cloud Gurus, and welcome back.
In this lesson, we're going to review some exam tips
for our big data services.
So let's go ahead and jump right on in.
So there are 4 questions
that I'm going to want you to keep in mind
on the exam anytime we see anything related to big data.
Our first question is what kind of database works for us?
Are we talking about a BI application
with petabytes of data?
Are we talking about a standard application
that just needs a relational database,
and maybe that would work with RDS?
Or are we thinking about that NoSQL option
and DynamoDB would be the right answer?
Or could we store that data in an S3 bucket
and use something like Athena to just directly query it?
How much data do we have?
Are we talking gigs where that would be great in RDS?
Are we talking more than that, where we would need
to consider something like Redshift or S3 for storage?
Is serverless a requirement?
That will be laid out in the scenario that you're given,
and that can lead you to picking the right answer
versus the wrong answer.
If we're talking serverless for ETL,
well, we'd want to pick Glue over EMR
because EMR is generally focusing on those EC2 instances.
So even though it's not particularly big data,
we always want to think, is the service managed
and does it need to be serverless?
And how do we optimize cost?
Now, that is important to remember.
Cost plays a big factor in this exam
and with our big data questions,
because with a lot of data,
we could end up with a lot of cost.
So we have to think,
do we store this in S3 and access it occasionally,
does it need to be stored in a data warehouse 24/7,
or do we need to store all of that data at all?
Maybe we could throw it away or set up a lifecycle policy
to purge some of that out if it's no longer needed.
Let's take a look at specifically Redshift and EMR.
So it's good to remember that Redshift
is a relational database, but it is not
a replacement for RDS.
I have worked on teams that have tried to do this
and let me tell you, it is a terrible replacement for RDS.
Don't even bother.
It's a great data warehouse,
but we're not using it in standard applications.
Redshift is technically only
a single availability zone service,
so it's not highly available,
and that's good to keep in mind.
You can create multiple clusters
in different availability zones,
but there's no one-click button
that just says make it highly available.
Technically you'd be duplicating your data
over and over again.
So don't fall for the trap of is Redshift highly available?
The answer is: it isn't and there's no real easy way
to make it highly available.
Now, EMR is made up of standard EC2 instances.
They are built and managed by AWS,
but we can use cost-saving measures,
such as the spot market or reserved instances.
If we can time our workloads, we can save a lot of cash.
And that's good to keep in mind.
Now for our last few services here, Kinesis,
Athena, and Glue.
Kinesis is the only service
that offers anything real-time related.
If you see that word real-time, think Kinesis.
Now, real-time being a requirement should point you
to Kinesis datastreams while near real-time
and a focus on automatic scaling
and ease of use would point you to Kinesis Data Firehose.
It's important to know that difference.
Both SQS and Kinesis can act as queues.
Each service has their own reason of
why you'd want to pick it and why you wouldn't.
SQS is easier, it's simpler, but it's not in real-time,
and remember, it can only store data up to 14 days.
Where Kinesis is faster, namely, it is real-time,
but it's a bit more complicated.
Another upside for Kinesis is that it can store data
up to a year if it is properly configured.
That's a lot longer than those 2 weeks,
but it's important to note neither service
offers message storage forever.
They all have a time window.
If you see the words serverless SQL, think Athena.
If you're trying to query anything
inside of S3, think Athena.
Now don't worry, you're not going to be asked
to read or write SQL on this particular exam,
but you might be asked,
how can we query logs in an S3 bucket?
And the answer for that scenario is going to include Athena.
If you're presented with a scenario
looking for a serverless ETL tool, Glue is your jam.
Glue can create that schema for your data
that's stored in S3, and then Athena can query it.
Now I've never seen too in-depth of questions on Glue.
Mostly I've seen it used as a distractor
on this particular exam, so it's good to know what it can do
and, namely, what it's not meant for.
And finally, let's go ahead and end this out
with some quick tips for QuickSight and ElasticSearch.
If you have an exam question
that's talking about visualizing data, namely,
how do we take all of this big data,
all of this information, and put it in a graph,
something that a human can look at?
QuickSight is going to be your answer.
If you find yourself looking at a question
that talks about a third party logging solution,
something that allows you to analyze logs,
analyze unstructured data,
but we're not looking for a proprietary AWS tool,
then we'd want to consider ElasticSearch.
ElasticSearch allows you,
when combined with Logstash and Kibana,
to create an ELK stack.
And this is a very common third party way
to look over those logs that are coming
from your EC2 instances or even on-prem servers.
Alright, folks, thanks for going
through the big data section with me
and I can't wait to see you in the next one.



Serverless Architecture
=======================

Serverless Overview
====================

Hello, Cloud Gurus and welcome back.
In this lesson, we're going to be answering the question
what is serverless computing?
Now in order to answer this question,
I'm going to need everybody to hop into my time machine
and we're going to take a trip back to the past
and explore the history of computing.
So I hope you have your snacks, your change of clothes,
your sleeping bags, because we're going to be here
for a while.
Okay, I'm just kidding.
It'll be a really quick trip.
But we have to understand what are all the building
blocks that have gotten us to this point
where now we can consider computing
without having to manage those underlying servers.
We'll see the benefits of why do I want to go serverless?
Why would I choose this
over my traditional compute environment?
Then we'll take a look at what's coming up.
What are the really big tools that we're going to be
covering in this section before we wrap it all up
with that exam tip.
Let's go ahead and jump right on it.
Now I know what you're thinking, you're saying, Alex,
how do we run code without computers?
What does it mean to go serverless?
Well, we haven't magically figured out how
to get our applications running without computers.
I will just crush that dream right there.
But let's go through our different layers
of compute to understand how serverless builds
on all of the things that we're already familiar with.
Now, a long, long time ago, we built giant warehouses
and we filled them up with computers
and we called them a data center.
And if you worked at a business, they would buy space
in a data center or open up a data center themselves,
and put all their computing needs in there.
Now that worked, and it worked for a while
until virtualization came into play.
Now I can buy a physical computer
and have it run more than one computer inside of it.
We can start virtualizing those operating systems.
And that was a big advancement,
but we'll all probably agree that we don't want to be
in a physical data centers.
I don't want to be dealing with hardware problems.
So along comes the cloud.
The cloud just offloads all of that physical management
onto AWS and we just get to work
with those operating systems.
Those databases deploying our application
in a way where I don't care about the physical hardware.
So serverless is really just the natural extension
of the cloud.
What I'm really doing is I'm just offloading my
operating system onto AWS,
just like I offloaded that physical hardware.
We simply write the code, we bring the code, and that's it.
Everything else is handled by AWS.
So AWS still has physical data centers.
They still have computers.
They still virtualize that, they manage all of this for us.
Serverless is really just about eliminating all
of the things that we don't care about,
namely, the things that aren't the code,
that aren't the application that I've just written.
Now, why do we want to go serverless?
What's the big point to all of this?
Well, it's easy.
As we're going to see in some upcoming lessons,
it is very simple to do.
Now unfortunately, I'm not going to be able
to help you write all of your code.
That's still something that you have to do.
But outside of writing that code,
there is very little work that you actually have to do.
And we'll get to see this in action,
here in just a little bit.
It's event based.
That just means that that serverless compute
does not come online, it doesn't get kicked off
until something happens.
So it's not stuck there running 24/7.
What we can have happen is, oh, somebody made an API call,
somebody clicked a button, somebody interacted
with my application, and now that code comes online,
it runs for that short period of time
and then it disappears.
Now not wasting compute power also brings us
to that billing model.
It is really pay as you go in the purest form.
Rather than having to pay to provision EC2 instances
to keep them online, to deal with scaling,
to deal with management, to pay your admin
to keep everything up and running,
we've offloaded that to AWS.
If my code runs for 5 minutes and 32 seconds,
that's exactly what I pay for,
is just that time that the code was running,
that I was getting value out of that code.
Now, these all sound great,
but how do I embrace this concept of serverless compute?
Well, that's really where our upcoming services
come into play.
So over the next couple of lessons,
we're going to explore various concepts related
to the 2 biggest serverless compute options in AWS:
Lambda and Fargate.
And we'll go through and talk about the differences,
and pros and cons and when to use one versus the other,
but at a really high-level,
Lambda allows us to write our code,
set what's going to trigger that code, and that's it.
And Fargate allows us to take our containers,
bring them to AWS, and forget about that traditional,
underlying operating system.
If you're not familiar with Lambda, Fargate, or containers,
not a problem. We'll be going through all of this content
in the next upcoming sections.
Just wanted to get those terms
wiggling around in your brain for now.
So my exam tip from this:
it's embrace serverless architecture.
On the test, we want to focus on using more managed tools.
We want to get away from that EC2 architecture
whenever possible.
The more that we can offload to AWS,
the better it's going to be.
So generally you want to favor applications,
favor answers that use Lambda, that use containers,
that use Fargate over the traditional EC2 architecture.
And like I said, we'll be exploring all of that
in just a bit.
For now though, I think this is a great stopping point.
So thanks for going through this overview.
Hopefully I've provided a little bit more clarity
on what does that word serverless mean?
And I can't wait to see you in our next lecture.


Computing with Lambda 
=======================

Hello, Cloud Gurus, and welcome back.
In this lesson, we're going to be taking a look
at everything Lambda related.
Now, that means our very first question
that we have to answer is: what is Lambda?
Now, we've talked about it. I've even used it in a demo.
We've effectively danced all around this concept,
but I haven't actually gone through and defined it
or showed you a Lambda function directly,
so we need to fix that.
Once we have an understanding of what Lambda is,
we'll talk about the basics of building a Lambda function.
What are the first four or five steps
that I need to be aware of when I'm creating that function?
After that, of course, we're going to go ahead
and build a function.
I'm going to walk you through creating one from scratch
and then executing that code.
And then finally, we're going to go ahead
and wrap it all up with some exam tips.
So, let's go ahead and dive right on it.
So, what is Lambda?
Well, Lambda is a serverless compute service
that lets us run code
without managing that underlying architecture.
So, it really excels when we can write our code,
and then that's it.
We simply write what we want Lambda to run,
and AWS handles provisioning the compute resource
that spins up, executes my code,
and then shuts it down when it's no longer needed.
So, it's kind of like we're running code without computers.
The computers are still there.
They're just not our problem anymore.
So, I know what you're thinking.
You're thinking,
"Great, I want to run code without managing those computers.
"How do we actually do this?
Because this sounds kind of strange."
Don't worry. I'm right there with you.
So, we're going to walk through the quick steps here
of how would we build out a Lambda function
and what are the things that we need to pay attention to.
Now, step number 1: Lambda, well, it requires
we pick from a list of supported runtimes.
So, this is: what did I write my code in?
Is it Java, Node.js, Python, or a variety of other options?
We also have the ability to create our own runtime
and even run containers inside of Lambda.
Now, creating our own runtimes
is going to be a little bit outside of the scope
of this particular exam,
but it is good to know that we do have to pick
from one of the supported runtimes or bring our own.
Permissions.
If your Lambda function makes an API call,
you're going to need to attach a rule.
This is a perfect exam topic.
The AWS test loves to talk about attaching roles
to Lambda functions
because one of the most important things
that we can do with Lambda
is making API calls with it.
We can basically build in our own features,
our own remediation for problems
or troubleshooting issues that pop up
using Lambda functions.
Now, we're going to take a look at this in a few minutes
to see it in action during that demo walkthrough.
Networking space is a really important Lambda topic.
While you can run your Lambda functions
inside of a particular VPC or subnets,
you don't actually have to.
It can kind of run in the network in either
if it doesn't need to talk to any private endpoints.
But if you wanted to sign in to your RDS database,
or use your VPN, or talk to the rest of your architecture,
you're going to have to define a VPC, a subnet,
and a security group.
You can kind of think of it as a really small EC2 instance.
Resources: How much oomph does that Lambda function get?
You get to define the amount of memory
that your Lambda function receives.
That's a single slider
that we're going to see in just a minute.
And what you're going to want to do
is pick the least amount of resources that it takes
to get your code done as fast as possible
because with Lambda, you are billed on two things:
the amount of resources that you have assigned
and the length that your Lambda function runs.
So ,the quicker you can get your code done
with the least amount of resources possible,
well, that's going to equal
the cheapest bill for that function.
And then finally, and maybe the most importantly,
what's going to trigger your Lambda function?
Because Lambda's kind of a lazy service.
You build a Lambda function, and it just sits there,
and it will never run until something or someone comes by
and gives it a kick in the pants and says,
"Hey, it's time to start up, execute that code,
"and then go do something."
Now, this is all great to see in slides,
but it's not actually real, at least to me,
until we build it out.
So, let's hop over to the console
and create our very first Lambda function.
All right, so as you can see,
I've got my Lambda console open.
Now, we're going to go ahead
and create our very first function.
Now, for those folks following along at home, don't worry.
I will be posting this code that I painstakingly wrote
in the Resource section of this video,
so you are welcome to download it, make a change, edit it,
and try to do this yourself if you'd like.
Or just sit back, relax,
and watch our very first function come online.
So, I'm going to go ahead and
click that Create function button,
and in this case I'm just going to go ahead
and author from scratch.
Now, there is so much to know about Lambda.
I'm going to focus on what we need to know
for just the exam,
but I would encourage you to play around with this.
On this page here,
I'm just going to leave the Author From Scratch selected.
It will give us a fun little Hello World,
but we're going to do something a bit more interesting
than just that.
I'm going to give my function a name,
and we're going to call this the instance-stopper.
Now, you can probably guess what that is going to do
just by the name.
Now, for the runtime, we have a lot that are supported here:
.NET, Go, Java, Node.js, Python, Ruby,
and some older variations of those languages.
Now, I'm going to go ahead and select Python 2.7.
Don't yell at me.
I know this is out of date, but to be honest,
this is the last language that I really dove into
as I'm not much of a developer,
so that's what we're going to use here for today.
Now, permissions-wise, I do want to assign a role.
Now, I'd like to use an existing role,
and I did do a tiny bit of work before this demo started.
All I did, just to save time here,
was I created an existing role,
I called it the instance-stopper,
I gave it the basic Lambda execution policy,
and I gave it full EC2 instance permissions.
That was it.
So if you'd like to do this yourself,
just go ahead and select those policies.
Now, down here, I'm going to go ahead and click Create.
Now, that gives me really nothing.
It just jumps me on this page.
And if I scroll down here, I can see that, yeah,
that's just a pretty basic Hello-World,
which is a great place to start,
but I think we can do a little bit better than this.
So, I'm going to take that code
that I managed to cobble together,
and I'm going to erase that Hello-World
and paste that right in.
Oop, there we go.
Now, we're not going to run
through what this is doing specifically,
so I'll just give you a high-level overview.
I approached Lambda
from the idea that I'm a system administrator
because before I got into the training,
that's what I did every single day.
And I had a problem where users were creating EC2 instances,
but they were not using name tags on their architecture.
So, I quickly put this together,
and every time an EC2 instance comes online,
we're going to check: Does that instance have a tag of name,
and is it in a running state?
And if it is, it's good to go.
If it isn't, well, you can probably guess
what we're going to do.
We're going to turn that instance off.
Now, that is one of the 10 billion possible uses for Lambda.
So, first, I need to go up here and just click Deploy,
basically the Save button,
and we're going to take a look
at those oh-so-important triggers.
Let's add a trigger in.
Now, there are 10 billion ways to kick off Lambda,
or a lot,
as we saw in that previous lecture:
API Gateway, Alexa, Load Balancers,
CloudWatch Logs, CodeCommit, DynamoDB,
the list goes on and on and on.
So, we can use Lambda to do a variety of things for us.
Maybe it's that mobile backend
like A Cloud Guru was using it for.
Maybe it's an image resizer.
Any time I upload an image to an S3 bucket, it resizes it
to be appropriately sized for my mobile application.
Maybe, in my case, it's doing my admin work for me.
So, Lambda can do anything that you can put into code
and run within the given resource
and time constraints that it has.
And we'll take a look at that in just a minute.
So in this case here,
I'd like to create a new rule for when this is going to run.
Now, we will talk about EventBridge.
We will talk about CloudWatch events in another lecture.
Just for quick review, all this is going to do
is it's going to kick off my Lambda function
any time an EC2 instance goes into a running state.
So, let's give it a quick name.
And down here, I'm going to look for a particular event.
So, any time an EC2 instance changes state,
I'm going to go ahead and kick this function off.
Now, I could get more specific if I wanted to.
We're just going to keep this pretty generic.
So, it is going to run a lot.
Any time an instance comes online, turns off, shuts down,
it will kick this function off.
So, I'm going to go ahead and click Enable,
add that in, and now we should be ready to go.
So, let's go ahead and go back to my EC2 console.
All right, I've got my EC2 console open.
And just to save some time,
I went ahead and spun up an instance already
and put it in a stopped state.
Now, this instance has the appropriate name tag.
Let's go ahead and clear this out real fast.
Let's imagine that I created an instance,
and I didn't have a value for that name tag.
Now, let's go ahead and turn this instance on
and see Lambda work its magic.
So, this will take a quick second here.
Wow! Did you blink?
Because if you blinked, you missed that.
That instance went very quickly
from pending into the starting state,
and then immediately Lambda came in
with that shutdown hammer and said,
"No, you do not have the correct tagging structure."
So, I built an auto-stopping resource
from my architecture. Right?
I added in an additional feature to AWS.
Now, there's one quick thing
that I want to show you with Lambda
before we call it quits on this demo.
So, I'm back in my Lambda function here,
and I've clicked on that Monitoring tab.
Now, scrolling down, it has built-in CloudWatch monitoring.
So, I can see that it's run, for how long that it's run,
how many times has it been run.
I've actually run it a few times here.
Always good to test things out to make sure that they work.
So, we have built-in logging and built-in monitoring
using CloudWatch and CloudWatch Logs,
and this comes out of the box.
Now, the last thing that I want to call out
is on this Configuration page,
and this is going to be a good stat to know for the exam.
On the left-hand side, if we click General Configuration,
here we can see how many resources
I have assigned to this function.
So, let's go ahead and click Edit.
And now I can set between 128 megs and 10 gigs of memory,
and CPU scales proportionate to that.
I can also set a timeout
for how long this function has to run.
Now, one quick note:
Your function cannot run longer
than 15 minutes.
That is the hard limit.
That is the maximum that your function can run.
Remember you want your function to run
with the least amount of resources allocated
in the least amount of time
to ensure that you are getting the best possible cost
for running that function.
All right, I know this has been a whirlwind tour of Lambda
and just one of the many, many things
that we can do with it.
But let's go ahead
and flip back for some of those exam tips.
So, exam tip number 1: Lambda is the answer
if the question is, how do we add features
or enforce things inside of AWS?
Just like we saw on that demo,
if I want to make sure that an instance
is automatically conforming to the rules that I've set,
Lambda can be the action that kicks that off.
So, if I automatically need to remove entries
from a security group,
start and stop an instance, or really do anything else,
I can do it with Lambda because I can write it in code
and then set a trigger to kick Lambda off.
It's also great for those microservices, right,
those smaller applications that
we're going to migrate into AWS
that don't make sense to run on an EC2 instance
because they're only kicked off maybe every once in a while,
or they only run for a short period of time.
Now, a few more tips here for us.
It's good to know those limitations.
Remember, 10 gigs of RAM and 15 minutes
is the longest the function can run.
So if it talks about needing a function or an application
that runs for hours on end
or needs tons and tons of architecture behind it,
maybe don't use Lambda.
But if it is talking
about lightweight, easy-to-use, temporary code,
Lambda is your best pick.
How does it start?
Now, I know we went through them very quickly,
but it's worth knowing those triggers,
knowing that we can kick off from S3,
from CloudWatch Events, or also known as EventBridge,
from API Gateway, from ALBs.
Those are all common triggers
on the exam you're going to see
and need to know that in general
they support kicking off Lambda functions.
Microservices.
Now, microservices work well inside of Lambda.
They're small, they're lightweight, they run quickly.
This is what we want to be using inside of Lambda.
Lambda plays a major role in the AWS environment.
It's easy to integrate, as we saw with those triggers,
and it adds in those custom functions or applications
that just don't make sense to run
in those long-running EC2 instances.
So, it's easy to build your own tools,
build your own features,
as well as building your own applications.
And then remember, keep in mind,
that Lambda functions can run in or out of a VPC.
Unlike EC2 instances,
they don't have to run inside of this networking space.
For example, my Lambda function didn't
because it didn't need to make any private API calls
or talk to a database.
But if yours does,
or your scenario that you're reading about on the exam,
requires access to private architecture,
then, yeah, it is an option.
We can place it in a subnet and give it a security group.
And finally, just remember,
never, ever, under any circumstances cut peppers
and then touch your eyes.
If you do end up making this mistake,
you can wash your eyes out with milk
to help reduce the burning.
Please don't ask me how I know this.
All right, folks,
thanks for going through this whirlwind tour
of Lambda functions with me,
and I can't wait to see you in the next video.


Container Overview
====================

Hello Cloud Gurus, and welcome back.
In this lesson we're going to be diving into Containers.
Now of course, the first question that we have to answer
is what exactly is a container
and why would I want to use one of these
inside of my environment?
And then we'll go through some general terminology
that we need to be familiar with
before we open up my terminal,
and we actually build our very first container together.
And then of course, we're going to look back together
to have some of those fun exam tips at the end.
So let's go ahead and dive right on in.
So our first question is what is a container?
Now a container in the IT world is kind of
like a normal container that you would have dealt with
in day to day life.
It just holds things.
In fact, I've got a lot of containers
in my refrigerator now holding my leftovers.
The same idea applies, a container is just something
that I can put everything into.
What I can do is I can bundle up my code,
my application, my packages,
my dependencies, my configuration files into a container.
And then I can pick this container up
and I can move it anywhere I want.
I can run it in dev, I can run it in production,
I can run it in staging,
and I get the exact same environment every single time.
And that's because this container, well,
the contents don't change between environments.
So it's really kind of a portability device.
So really, a container is just an easy way
to standardize everything that I need to run my application.
Now, when I first learned about containers,
I heard a definition pretty similar to that
and I thought great, that just sounds exactly
like a virtual machine, right.
I've got my hardware, I've got my hypervisor,
I install my Linux on my Windows,
my application, the configurations and I'm good to go.
Why do I need to containerize everything?
Well the reason why we want to consider containers
really comes down to that guest operating system
that duplication of resources between every single VM.
Because with the container,
I don't have to duplicate everything.
So instead of having to create multiple operating systems
over and over and over,
and duplicating all of the stuff that doesn't really matter
to my application, because let's face it,
we never use everything in that operating system
for every single deployment, right
there's a lot of extra fluff.
With this container, I can cut that down.
I'm only including that application
and the needed libraries, packages, configuration files.
Effectively, it's like I'm creating
my own little micro environment
that only has everything that I need to have.
So I can stuff more containers,
more copies of my application into that underlying hardware
rather than putting things in there that don't matter.
So effectively I'm just getting a better bang for my buck.
Now look, I get it.
Just talking about containers
in this abstract form,
it's really difficult to understand.
But before we actually build our very first container,
there are a few terms that we have to understand.
The very first one is a Docker file.
We can consider our Docker file
kind of like our set of instructions.
Okay, I want to build one of these quote containers,
what goes into that?
Well, using a Docker file we're going to use
a Linux like commands to define
what does our container include.
Where do the files go, what applications need to run,
all that good stuff.
Now technically the Docker file builds
what is called an image.
We can kind of think of an image
like it's my source of truth.
That image it's immutable,
it contains everything that I need to have.
And I'm going to take this image,
this kind of template if you will,
and I'm going to store it in a registry.
Now registries can be both public or private.
You can think of it like GitHub but for your images.
Now technically up until this point
we don't have a container yet,
we just have that template when we download that image,
and run it inside of our EC2 instances
or on my laptop or on-premise in my data center
then I actually have the container.
So it's easy to call everything a container
and people understand what you're talking about.
But the general flow is I write a Docker file
from that Docker file I build an image,
from that image I upload it to a registry
and then from that registry I download the image
and then run it as a container.
Now I know that was a lot, but let's go ahead
and hop into my terminal
and walk through actually putting one of these together.
Alright, now I've got my terminal open
instead of my AWS account.
I've already SSHed in into one of my EC2 instances
that I have inside of my AWS environment.
So let's see what I've got going on in here.
Well, I've got my code that I've written
and we'll take a look at that in a minute.
And then that important Docker file,
that set of instructions.
Let's go ahead and open up my Docker file.
Now in this file, if you've ever worked with Linux before,
these commands should be pretty familiar.
Here I'm going to go in and just say from centos:7.
That's just my base image that I'm pulling in.
And then every step that I do
on top of that base image,
well those are all going to be custom
for this particular application.
I'm going to go ahead and run my updates, install Apache
and then remove the default Apache installation website,
add my code for my local directory up here right,
where I've written that website,
and I'm going to expand that into /var/www/html
inside of this new container.
Then I'm just setting up some logging
so I can troubleshoot in the future if something goes wrong.
I'm exposing port 80 from inside of my container,
and then I'm just running Apache.
So the end goal is I will have a website
when this container starts up.
Apache listens on port 80,
traffic will be sent into the container,
and it's going to serve out my code.
So let's go ahead and put this all together,
because this text file by itself,
doesn't really get me a whole lot.
I'm going to run my sudo docker build
and now I'm actually using a cached copy
of each one of these steps because I've done this before.
The cache just allows it to speed up.
If you're doing this as a fresh run
it would take an extra few minutes or two,
but for the sake of brevity here
we're going to go ahead and just use that cache.
So lets run through each one of these steps
and now I have a successfully built image.
Now remember that image doesn't do much by itself.
In fact, it just sits there.
I need to run this image as a container
on my host to see my website.
Now before I do that,
I just want to show you something real fast.
I'm going to hit my localhost.
Basically I'm just trying to connect
from my instance to my own instance.
And it just shows that nope
I have no website currently running.
Just want to show you
there is no funny business going on here.
Now I'm going to go ahead and run that Docker image
as a container.
So here I've got my sudo docker run
and I'm going to bind port 80 on my host
to port 80 on that container.
So let's go ahead and kick this off
and well it's running.
How do I check that it's running though?
Let's run that curl again.
Oh, I get a website this time.
Now apparently it's showing out
some fun dog photos, alright.
Now this is where we're going to leave it for this lesson.
I promise these dog pictures are a little bit more exciting
in the actual browser.
But we're going to hold off on that for right now.
So this was a pretty basic walkthrough
of creating that image and running
as a container on my host.
So let's go ahead and hop back into our slides
and take a look at some of those exam tips.
Now my first exam tip, is that containers are generally seen
as more flexible on the exam.
We have full control over what's running inside of them
via that Docker file.
Now it's really easy to create your container
and then move it around to different environments
and you get that same container everywhere.
It means it's really easy to standardize
what you're about to push out.
You don't have to run into that issue of
but it worked on my machine, it worked in dev.
So if you're looking at a situation that talks about
oh, I need flexibility, I need an immutable environment
that I can move anywhere.
You want to generally favor answers
that include containers.
Now a few more tips here for us. The
very first one is, look, you've got to keep it high-level.
This is not a Docker container image registry exam,
you just generally need to know the use case
of why do I containerize applications?
It's for portability, it's for ease of use,
it's for flexibility, it can run on-premise
it can run inside of AWS.
Those are all things that you're going to have to know.
The exam is not going to grill you on what is line 12
of this Docker file do?
Because that's not really the point of this AWS test.
Now you need to know that it's easy.
It's easy to pick up and move your environments
from on-premise to AWS and back.
That's going to to be an important concept
that you want to keep in mind.
It's also important to know that dev is prod
and prod is dev, but not of the usual panic incense.
Usually if somebody tells me that I freak out
because that's a terrible position to be in.
But in this case, it's actually a good thing
because it means that all of the testing that
I'm doing in dev, is going to carry through to production.
I'm not in that situation where I'm missing dependencies
where things disappear between those different environments.
Now my last tip here for you,
before you go shopping at that grocery store,
take a picture inside of your refrigerator.
That way when you get there you won't forget
what you already have.
I know that saved me a whole lot of time.
Alright folks, thanks for joining me in this dive
into the world of containerization.
I can't wait to see you in the next video.


Running Container in ECS or EKS
===============================

Hello, Cloud Gurus, and welcome back.
In this lesson, the container train keeps on rolling
and we're going to take a look at 2 very important
services: ECS and EKS.
We'll start off with some potential problems
that we might run into in our containerized environment.
We'll explore the Elastic Container Service
or, as it's most commonly referred to, ECS.
And then we'll take a look
at its open-source cousin, Kubernetes.
We'll do a quick breakdown of when do we use ECS versus EKS
and then jump into the console
to spin up our very first container using that ECS service.
We'll come back together
and wrap it all up with some exam tips
before we call it a day.
So let's go ahead and dive right on in.
Now, in the previous lecture,
we saw a diagram that looked, well, exactly like this.
I've got my operating system
and I load it up with as many containers
as I can possibly fit.
And that's great and that's easy to manage
when I have 1 container, 2 containers.
Well, what about when I have 4 containers, 10 containers,
50 containers, 1,000 containers,
and I need a fleet of instances to put those containers in?
How do I manage all of these?
Because I really don't want to do this myself.
Well, the short answer is, to handle all of this scale,
that's where we're going to turn to ECS,
the Elastic Container Service.
Now, ECS is a proprietary managed service by AWS
that can help us as our applications grow,
as we need more and more containers
and more and more resources to run those containers.
So it allows us to manage 1, 10, hundreds,
millions of containers
and appropriately spin-up the underlying architecture
that we're going to need,
and then place those containers inside of that architecture,
and then place those containers on those resources.
It integrates natively with our load balancers.
So as new instances come online
and we load containers up inside of them, they're
automatically going to sit behind those load balancers
that we've built out.
We have easy integration with roles,
meaning, when my containers need to talk to something else
inside of AWS, it's really simple,
I can just attach a role to that particular container
and I'm good to go.
It has all of the needed permissions
that it would have to have.
Now, one of the big benefits,
and we're going to see this in a few minutes,
it is easy. It is simple.
Click a few buttons and it's all built out for you,
and it all has that potential
to talk to the rest of your AWS environment
because this is a tool built by AWS for AWS.
Now, ECS can lead to a potential problem, namely,
it only works inside of AWS.
So what if you're looking for a solution
that's a little bit more, well, cross-platform appropriate?
That's where Kubernetes comes into play.
Kubernetes is an open-source container management
and orchestration platform.
It was originally built by Google
but it's been open-sourced for a while now.
And what this allows us to do is,
well, not use proprietary AWS tools.
Kubernetes can be run on just about anything.
I can run it on my laptop, my data center,
inside of this cloud provider or another one,
I could even run it on my Raspberry Pi's if I want to.
So the big selling point here
is that it works on-premise and it works inside of AWS.
Now, if we want to spin Kubernetes up ourself,
trust me when I tell you,
it's not always the easiest platform to get going with.
There's a lot of lifting that we have to get done
before we can start running our very first container.
So that's where AWS has a managed version of Kubernetes
called the Elastic Container Service or EKS.
Now, this might leave you
kind of scratching your head thinking,
ell, what do I use?
Do I use ECS or do I use EKS?
Well, there's no one silver bullet
that's going to solve all of our container management needs.
Each one of these tools has a pros and cons list.
ECS focuses on that ease of use.
It's simple, it easily integrates
into all of our architecture,
and there's not a lot of work that I have to do.
Downside to ECS, it doesn't really work on-prem.
Where EKS allows us that open source comfort of Kubernetes,
we might be familiar with it already,
but it's not all in on AWS,
meaning, there are still some additional kind of hacks
or work arounds that we have to do
to make EKS perform like ECS
as far as compatibility goes inside of the cloud.
Now, for this exam,
that's a lot deeper than you're going to have to go.
Nobody is going to ask you to do a deep dive on Kubernetes.
We want to focus on ECS
when the architecture is all in on the cloud,
and we focus on EKS when we need a little bit outside
of just that AWS environment.
Now, that's all great to talk about,
but we need to see this in action.
So let's go ahead and flip over to the console
and we'll spin up our very first container
using the ECS service.
Alright, so I've got the ECS service up and ready to go.
Now, the only setup that I've done before this demo
was I created a repo inside of ECR,
which is Amazon's Elastic Container Registry,
meaning I have a location where I'm storing that image
that we're then going to deploy.
Now, my first step inside of ECS
is going to be creating something called a task definition.
Effectively, what this is going to do
is it's just going to be all of the settings
that I'm going to need to have
for my container to properly run.
So in this case, I'm going to select the EC2 option,
I'm going to give it a name.
I think that's appropriate.
Now, my container doesn't need a role,
so I'm going to leave that blank.
Down here.
Alright, I have to decide how much
oomph,
how much CPU and how much RAM I'm going to need.
Let's give it 512 megs and just a 1 on the CPU.--
Oh, excuse me--
A 128.
There we go.
Now next, I'm going to go ahead and add my container in.
So I'm going to call this my dog-site to show that dog
photo. And now I'm going to go ahead and paste the location
of the container that I had already uploaded.
Now down here,
I do need to define that host to container mapping,
so I'm just going to say, port 80 on the host
is going to redirect to port 80 on the container.
And that's all I need to do here.
And that's it for this task definition,
just giving it the basic layout.
I'm going to come down here and click Create.
Now that all my settings are set, that's great,
but I have to have a place to run it.
So to do that, let's go to the Cluster section,
and we're going to go ahead and create a brand new cluster.
Now, in this case, I'm going to go ahead and select Linux
because that's what I'm expecting to run this in.
It does have support for Windows
as well as another option called Fargate.
And we'll get to Fargate on our next lesson.
Now I have to give it a name.
There we go.
And I have to define:
do I want on demand?
Do I want spot?
It's important to know these are standard EC2 instances.
Nothing too special going on here,
so we can use our normal cost savings techniques
to make sure we're getting the best possible price.
Speaking of price,
I'm going to go with something very small here.
Let's just do that t2.micro,
since this container doesn't really need too much.
Now, notice these are a lot of the same settings
you would normally put into EC2 architecture.
Notice it lives in a VPC.
I'm just going to use the standard option.
It has a security group, right?
It's just placing those containers on these instances
that it's going to create for you.
Now, everything looks good.
I'm going to go ahead and click Create.
And this will take a few minutes.
So we'll check back in as soon as this is done.
It looks like that's done,
so let's go ahead and open that up.
So I've got my cluster here, and,
well, nothing's running on it,
so let's go ahead and fix that.
We're going to click Run New Task.
Now, down here, I just want to ensure
that I'm running on that EC2 instance.
I'm going to go down
and select that containers-are-fun task definition
that we built a few minutes ago.
I just want one copy.
That's really about it.
And I'm going to click Deploy. Now, what
this is going to do is it's going to pull in that image,
run it as a container on that host,
and then return back to me an end point
that I can then access my content from.
So this will take a minute to come online.
Notice it's still in that pending state,
so we'll check back in as soon as this is done.
Looks like that container just finished coming online.
Now in order to access this, it's pretty simple.
Let's go up here to Services, go to EC2,
and then we're going to find my ECS instance.
Notice here, it's labeled as ECS Instance.
This is what ECS brought online
and then deployed that task inside of.
So let's go ahead and grab this IP address
and pop it up to a new tab here.
And without further ado, my dog website.
Now, this is my dog, Chalupa.
She is adorable
and very excited to be featured in this video.
Alright, folks, that's going to end it out
for our quick demo here of ECS.
Let's go ahead and jump back into the slides
to take a look at some exam tips.
My first exam tip is that, in general,
ECS is going to be the preferred method.
If you're talking about containers,
you should be thinking about ECS on the exam.
Now the real exception to that is,
if the question or scenario specifically calls out
that it needs an on-premise or open source solution,
then we want to think Kubernetes and EKS.
Otherwise, ECS should basically be the default option.
Now, a few more tips here for us.
Generally favor those AWS-managed services
over third party tools.
This is no exception.
So we're going to want to think ECS
unless that open source terminology comes up
or that particular request for Kubernetes.
If it does come up, then we do think EKS
because EKS is going to be simpler
than trying to build out
and manage that Kubernetes environment on your own.
Now, as you can tell with this walk-through,
there is so much more that we have not talked about,
but it's not particularly relevant for the exam.
So we're going to keep it pretty high-level here.
Generally know that ECS and EKS
handle the management, placement, running of our containers,
and that's about it.
You're not going to be asked to do a deep dive
into one particular feature or setting
for either one of these tools.
Now these containers that we're going to run
inside of these tools, they're great for either one-off
or long running applications.
So we don't want to shy away from containers.
They're really going to be one of our go-to options
anytime we're talking about services
that we are building or porting into AWS.
And finally, no matter how cool an eclipse is,
don't stare at the sun.
It's very tempting, I know I've been there,
but staring at the sun can cause long-term damage
to your vision and nobody wants that.
Alright, folks,
thanks for taking a look at ECS and EKS with me,
and I can't wait to see you in the next video.


Removing Servers with Fargate
===============================

Hello, Cloud Gurus and welcome back.
In this lesson, We're going to take a look
at how we can run containers
without servers using the Fargate service.
The first thing that we'll take a look at
is why do we want to do this?
What's the inherent problem that we solve
by removing those servers from the equation?
Then of course, we'll define what is this Fargate service
and why do I want to use it?
We'll compare it against those traditional EC2 instances.
And then we'll also compare it
against those Lambda functions to try to get a feel of
when we should use it and maybe when we should avoid it.
And then of course,
we're going to have that hands-on console demo
and we'll come back together
and wrap it all up with some exam tips.
So let's go ahead and jump right on in.
Now we effectively looked at this diagram
in our last lesson where we explored using ECS
and EKS to place those containers
on the underlying EC2 instances.
And that's great because now I don't have to go through
and place those containers myself,
but it still leaves me with this problem.
That underlying resource, that instance
is still mine to deal with.
And as my architecture grows and grows and grows
and I get more and more resources,
I don't want to spend my time patching
and updating and dealing with those resources.
So what can we do to solve this server problem?
Well, unfortunately for us, there's nothing we can do.
We always have to have those--
alright, I'm just kidding.
Of course we can get rid of those instances.
What we can use is that AWS service called Fargate.
And what Fargate effectively allows us to do
is chuck those EC2 instances right out the window
and run our containers effectively in the Aether.
Now, when I say that our containers run in the Aether
I am being a little bit facetious.
Of course, they are still servers.
They're just owned by AWS.
Similar to those Lambda functions,
we don't control that underlying operating system.
We decide what's going to run
and then AWS handles it for us.
So it allows us to still use ECS
or EKS for our scheduling, placement, management needs
without those pesky servers.
Amazon holds on to all of it
which means we have to do very little here.
In fact all we do is say: yup
I'd like to use Fargate and that's it.
Now Fargate does require either the use of ECS or EKS.
So it's not really a service by itself
but more of a feature of the ECS or EKS service.
And I'm sorry, Windows folks,
there is no support for you at this point in time.
This is a Linux-only service.
So how do we know, when do I pick EC2
versus when do I pick Fargate?
I wish I could tell you that Fargate
was just the silver bullet that was going to solve
every one of your IT problems.
And you'd never need a server again
but unfortunately that's just not the case.
With those EC2 instances, we have that pain.
We have to manage the operating system
but it allows us to build in that EC2 pricing model.
So dollar for dollar, EC2 is a better deal.
Because I can use reserved instances.
I can use those cost saving measures
that we've talked about in previous lessons.
EC2 excels for long running containers.
If you have an application that's online for 24 hours a day,
365 days a year, you're going to get a better deal
running that on EC2 architecture.
Serverless doesn't really handle
that particular workload very well.
So we want to focus on putting long running containers
on EC2 instances.
Now I can cram as many containers on to those EC2 instances
as I have room for.
So I can fill it up to the brim.
So I'm not wasting CPU or RAM.
Now at the Fargate, we lose that operating system access.
Which is actually a good thing
because I don't want to have to sign onto the host
and run updates or do any of that kind of stuff.
I only pay for the exact amount of compute time
and resource allocation that I have decided
that my Fargate task requires.
So similar to that Lambda billing cycle.
I simply say, I'm going to need X amount of CPU and RAM
and it's going to run for X amount of time.
And that's how I'm going to calculate
what my underlying cost is going to be.
So Fargate really excels at those short running tasks.
Those tasks that only appear for a little bit of time.
Maybe I'm doing some batch processing,
maybe I'm responding to an API call.
Maybe I'm simply going through and processing
and all of the data that was uploaded to an S3 bucket.
It starts up, it runs.
And then it disappears when it's done.
Everything is isolated as well.
This means we don't have to worry
about any container doing something that shouldn't
on that host and effecting another container.
Everything runs completely by itself.
So hopefully this gives you an idea of
when do we use EC2 versus when do we use Fargate?
Now I know you're thinking Alex,
great EC2 versus Fargate.
Understand that now, but what about Lambda?
Well, we have to introduce Lambda here.
Because that's kind of a conversation
that does naturally come up.
In this Fargate versus Lambda fight,
how do I know which one I'm supposed to use?
If I've that Serverless is the right tool for the job?
Well, with Fargate it allows me to standardize
with containerization.
If I create a container here in AWS,
I can run it in another cloud provider.
I can run it on-premise.
I can run it anywhere I want.
It allows me to have more consistent workloads
versus Lambda, it's a little bit more difficult
to standardize on Lambda across an entire organization.
Because Lambda is great for just AWS.
Now Lambda does excel when I just have a small
single function, something that might not be as complex
as an entire container that I need to quickly respond
to an event and shut down.
Where Fargate can take that containerized architecture
and run it for maybe a little bit longer.
It doesn't have that time window limit that Lambda has.
There's no way I can say only use Lambda
or only use Fargate.
It really comes down to, do I need to have containers
or do I just want to deal with only my code?
And maybe it's a little bit more simple
and a little bit more lightweight.
Now that's a lot of talk about Fargate.
Let's go ahead and hop into the console
and spin up an application using this tool.
I've got ECS open, and if you're following along from home
all I've done before this demo is I created a repo
inside of ECR and pushed a pretty basic image into it.
So let's go ahead and get started.
Now, the first thing we're going to want to do
is create a new task definition.
I'm going to go ahead and click the Create New
Definition button.
And we're going to leave Fargate selected.
Now scrolling down, let's go to the next page
and I have to give it a name.
I think that's appropriate.
Now a little bit more, we need to know about this.
When I'm using Fargate, I have to define the amount of CPU
and memory that my Fargate task is going to receive.
Now, this image is pretty bare bones
so I'm going to give it the smallest amount possible.
And just remember the more that you consume
and the longer that that consumption takes,
the higher your Fargate bill is going to be.
So you want your task to finish in the least amount of time
with the least amount of resources.
Next, I'm going to go ahead and add in a container.
We'll just call this my ecs-is-fun.
I'll go ahead and paste that image right in there.
Scrolling down here a little bit more,
we just want to allocate that port 80 for my container
and that's really all that it's going to need.
So we'll leave the rest as just the default.
So I click Add and not too much going into this task
definition since it's just going to be that single container
that I need to run.
So I'm going to skip all of these other options
and go ahead and click Create.
We've got my new fargate-rocks task.
So let's go ahead and kick this off.
Now first, let's go ahead and create a cluster here.
I'm going to leave it in the networking only mode.
Notice that powered by Fargate.
There will be no EC2 instances here.
Whoops, got to give it a quick name.
That would help.
Awesome.
Now that the cluster is up,
let's go ahead and kick off our very first Fargate task.
So I'm going to open up the task section,
going to go ahead and click Run New Task.
And then I'm going to grab that specific task
that I just made fargate-rocks.
Let's go with the latest revision.
And I'm going to click Deploy.
And that was it.
This is going to take about a minute or two.
So I'm going to go ahead and pause the video
while this finishes and we'll check back in
as soon as this is done.
It looks like it just finished.
So let's go ahead and open up that task
and to interact with this.
I'm going to need to grab that public IP address.
So let's go ahead and grab this here, copy it in.
And I'm going to paste it into my browser
and well we have our new website.
And I think that self congratulations here is appropriate.
We just walked through, spinning up our very first container
with no EC2 instances in sight.
Now, obviously my website skills are probably not the best
and you'll hopefully use it for something
that's a little bit more useful
but let's go ahead and shift back to our slides
and take a look at some of those exam tips.
My first exam tip is it's really important to know:
When do we use Lambda versus Fargate versus EC2?
And I'll break it down here for you very quickly.
Lambda is for light weight functions.
They can run very quickly and they generally
need to be easily integrated into our AWS architecture.
This could be for processing content out of S3,
responding to an event from config
or API Gateway or something along those lines.
Fargate is for when I have containers
that don't need to run all the time
and EC2 is for when I have containers that need to run 24/7
and I'm really concerned about cost.
So keep all of those use cases in mind
as you're going through the exam.
Now a few more tips before we call it quits,
lose the servers.
Fargate is a serverless tool.
Fargate allows us to run containers without hosts.
We need to keep this in mind.
Fargate doesn't work by itself.
However, the exam might refer to Fargate
without specifying ECS or EKS.
I've personally seen that happen on the test.
It's not a trick question.
It just means we want to think about that use case
of Fargate of running containers without servers
and the ECS or EKS portion is not applicable
to what the question is getting at.
Know that we want to use EC2
when we think about costs and long running content
and we want to think about Fargate
when we just want to run that container
for a little bit of time
and ease of use is the most important piece.
When it comes to Fargate versus Lambda,
Fargate excels for containers that need to run
for a bit longer but we still don't want servers
and Lambda excels at that short and simple code.
And my last tip here, if you're ever on the long hike,
bring a few extra pair of socks.
You'll never know when your feet are going to get wet.
And if you leave your feet in wet socks
for an extended period of time, it can cause trench foot.
And nobody wants that.
Alright folks, thanks for going through Fargate with me
and I can't wait to see you in the next lesson.


Amazon EventBridge (CloudWatch Events)
=======================================

Hello Cloud Gurus, and welcome back.
In this lesson,
we're going to take a look at Amazon EventBridge,
also known as CloudWatch Events.
We first have to start off by, of course,
defining what is EventBridge,
and how does it fit into
our serverless architecture platform.
Then I'll walk you through
what steps are required to create a rule.
How do I know what I'm going to alert?
Basically, what do I need to know
to use EventBridge.
We'll then open up my AWS console
and kick off a Lambda function based
on an API call,
and then circle back for those exam tips.
So let's go ahead and dive in right on in.
So that first question is, what is EventBridge?
Well, right out of the gate,
EventBridge basically CloudWatch Events 2.0.
Amazon has redesigned the console,
split the service out
from its previous banner under CloudWatch.
So if you're on the exam,
mentally you can exchange CloudWatch Events
for EventBridge, back and forth.
EventBridge has some additional features,
but what we are going to focus on here
is common between both of them.
Alright, with that out of the way,
what is this?
Well, it's a serverless event bus.
Now, that's kind of a strange set of words to use.
Basically, it's the glue
that holds your serverless application together.
All it does is it says,
something happened over here,
and I'm going to tell that thing over there
that something happened.
Basically it allows me to say an API call happened,
something kicked off,
I'm going to go alert the next step of the process.
So how do we decide,
well, it happened over here,
I got to go tell that thing over there?
Well, we do this by creating a rule.
So the first thing that we have to do
is we have to decide what kind of pattern
are we going to use?
Now there's a few different ways
that we can kick off that Lambda function,
that email,
that whatever you're alerting,
that something happened.
It could be based on an API call being kicked off,
or it could be scheduled.
Maybe every 6 days,
you want to run a Lambda function
to audit part of your architecture.
You can create that,
but you have to define,
is it going to be scheduled
or is it going to be based on an event happening?
Now EventBridge does include support
for custom events,
for partner events.
In this particular lecture
and on this exam,
we're going to focus on the AWS based events.
So basically those AWS API calls.
You need to then select your target.
Okay.
When my EC2 instance comes online,
that API call has happened,
what do I need to do?
Do I trigger a Lambda function?
Send out an SQS message?
Send an email?
Basically there's no limit to what can happen,
because if we can write it in code,
we can put it in Lambda.
As we've discussed,
Lambda can do just about anything for us.
Then of course we have to tag it.
We tag everything.
I can't emphasize that enough,
including these rules.
And then from there, you're done.
Just sit back, hang out,
wait for it to happen.
Or in our case,
we're going to test it out
and actually make sure that it works.
Alright, well, that's great to see in theory,
but let's walk through these steps in practice.
Okay, so I've got my EventBridge console open here.
Now, just a quick note
for those folks following along at home,
this is going to be building
off that previous lecture,
using that Lambda function.
However, I have made a few tweaks
that I will talk about in just a second.
So let's go ahead
and click that Create Rule button.
From here, we'll give it a name,
call this the instance-starter,
and we'll get to that in just a second.
Now, scrolling down.
This is where I can define,
do I want a scheduled kickoff,
or in our case,
we're going to build a pattern.
So I'm going to select just a predefined pattern.
Now, once again, I can customize this
for any API call inside of AWS.
I'm going to select AWS as that source,
and then let's go through the services here.
So scrolling down a little bit farther,
notice there's a lot of options.
Just about everything is in here.
I'm looking for, there it is,
the EC2 service.
Now I'm going to select that,
well, in this case,
when we get a state change notification,
if you remember in that previous demo,
we would turn instances off that were no longer tagged.
In this case here,
I'm going to flip it and reverse it.
This time we're going to go for,
if the instance was stopped,
we're just going to turn it back on.
Now notice here, I do get that event pattern.
That JSON, just saying,
that's what we're looking for.
If the instance goes into a stop state
then it's going to get kicked off.
Now scrolling down a little bit farther,
I can pick any instance,
specific instance IDs,
I can test my pattern if I want.
Now in this case,
we're just sticking to that AWS default.
And this is really about the extent
of what you're going to need to know
for the exam.
We do have to select a target.
In this case here,
I'd like to kick off a Lambda function.
There are a ton of options though.
Now I did make a quick change
to the instance-stopper before this lecture started.
It's now the instance-starter.
I really just changed the stop to start API call.
So I'm going to go ahead.
I'll give it a tag,
because that's always best practice.
Just so I know I created this.
And then I'll go ahead and click Create.
Now,
that's it.
It's now up and running.
Let's go ahead and open up
the EC2 instance service here
and see if it works.
So I have an EC2 instance here labeled as start,
and I'm going to go ahead and turn it off.
So I'm going to go into the stopped state,
and this'll just take a quick second.
Ooh, did you catch that?
Real quick, it went from this stopped state
for just a quick second,
back into pending.
Now if we keep refreshing just a few more times,
we should see it go into that running state.
The point of this is I can kick off
that Lambda function
off of any API call that I want.
And that is really important to know for the exam,
because we can use this
to basically build our own features,
our own tools into AWS.
So let's go ahead and switch back
for some of those exam tips.
So EventBridge is the glue.
It's what kicks off that serverless architecture.
It's what says, hey, this API call happened,
so this thing needs to know about it.
On the exam,
this is important to keep in mind,
and I keep reiterating this point,
but you need to know it.
Any API call that happens
can a kick off EventBridge/CloudWatch Events.
Now a few more tips here for us.
Once again, API calls can kick off Lambda functions.
I keep saying that over and over again,
because it is that important.
Do you want to dynamically adjust network ACL rules
based on GuardDuty findings?
Of course you do.
You can use EventBridge to kick off Lambda functions.
Now, as far as the exam goes,
keep it high level.
While there's a lot more to EventBridge,
this particular exam is really only going to focus
on those AWS API calls.
New versus old.
EventBridge is the new shiny name,
but you might see CloudWatch Events on the exam.
Don't panic.
They're interchangeable,
as far as this exam is concerned.
Speed is important.
This is the fastest way to respond to an API call.
You technically could feed your CloudTrail logs
through CloudWatch logs
and set up an event pattern
based on a particular API call,
but that will be very, very slow.
EventBridge/CloudWatch Events is what you want to use.
And finally, over-watering kills more houseplants
than under watering,
and their symptoms are exactly the same.
So try watering your plants just a little bit less
if they keep dying.
Alright, folks,
thanks for going through EventBridge with me,
and I can't wait to see you in the next lesson.


Serverless Architecture Exam Tips
===================================

Hello Cloud Gurus,
and welcome back.
In this lesson,
we're going to wrap up our conversations
around serverless architecture
with some final exam tips for you.
So let's go ahead and dive right on it.
Now, of course,
I've got the four questions
that I want you to keep in mind
for every single scenario
that's given to you on the exam.
The first question is,
is the application being described to you
right for containers?
Remember, the containers are those immutable resources.
I fill the container up with all of my code,
all of my dependencies,
all of my packages,
and then I can pick that container up
and move it anywhere that I want.
So I need to really focus on using containers
with microservices,
with small, flexible applications.
If the scenario is talking about,
well, that microservice,
we should be thinking,
how do I containerize it?
If it's talking about that monolithic architecture,
where we just need one giant server
to run everything,
then you probably want to avoid containers
for that application.
Do you need those servers?
Do we have to have them?
If possible, it's best to use a managed architecture.
It's best to use Fargate, ECS,
EKS, Lambda even.
Some situations will require that host level access.
And if it does,
then we want to avoid serverless resources.
But if it doesn't,
or it's not specifically called out,
favor situations that give you the ability
to ditch those EC2 instances.
Is the application AWS specific?
Are we looking for a resource
that has to run inside of AWS,
or do we need that portability
that a container can give us,
so we can run inside of the cloud,
and also on-premise?
How long does the code have to run?
Is it running for a minute,
for an hour, for a year?
We're going to need to be able to look at that
and decide which serverless resource do we want to use,
or do we want to use those standard EC2 instances?
That's going to be a big theme here,
knowing when do we use serverless
versus when do we use standard resources.
So a few more takeaway points here for Lambda,
Lambda loves roles.
Anytime you see a situation
in which you're creating a Lambda function,
ensure that you are selecting
the appropriate role to attach to it.
Sometimes the exam might lay out a situation
where it talks about hard coding credentials
inside of Lambda,
that access key and secret key.
If you ever see that,
you want to avoid that answer.
We always, always, always want
to attach a role to Lambda
to make our permissions
just a little bit easier to manage.
Know what can kick off those Lambda functions.
We talked about a few of these in that Lambda lesson:
as S3, Kinesis, CloudWatch Events,
also known as EventBridge.
These are common tools that can kick Lambda off.
API gateway is another big one.
You don't have to memorize every single thing,
but be able to place Lambda inside
of a general architecture pattern,
and understand the use cases of why each one
of these services might want
to kick off a Lambda function.
Know those limits.
Functions are designed to be short.
We can have them up to those 10 gigs of RAM,
and 15 minutes of runtime,
but it is not possible
to exceed either one of those numbers.
Also keep in mind that that CPU scales out
proportionate the amount of RAM that you've allocated.
It's important to remember that any AWS API call
can be a trigger to kick off an EventBridge rule.
This means any event,
anything that happens inside of your account
can notify a Lambda function.
This is a really powerful tool.
That means you can basically build
in your own features,
your own AWS tweaks,
as we'll call it.
You want to make sure
that all of your buckets are shut down immediately
if they become publicly available?
You can use a Lambda function
and EventBridge to make that happen.
You want to turn off all
of your EC2 instances at midnight
for whatever reason?
You can use event bridge and Lambda
to make this happen.
For the exam, it's important to remember
that using EventBridge/CloudWatch events
is going to be faster than trying
to scrape through CloudTrail
and find an API call in there,
because CloudTrail primarily focuses on auditing.
Now, a few tips for those container questions
that you might run into.
If you see a question that's talking
about anything open source,
and it mentions containers,
automatically think Kubernetes.
Now, Kubernetes and AWS should also lead you
to an answer that contains EKS.
In general, we want to focus on managed solutions.
So if given a scenario,
you should pick using EKS
over trying to spin up Kubernetes by yourself.
We also want to keep in mind
that Kubernetes works on-premise
and inside of AWS.
Sometimes you might see a scenario
that talks about a cross-platform rollout,
and know the ECS doesn't really support
on premise and the cloud.
It's really just for AWS.
Fargate doesn't work alone.
It needs ECS.
It needs EKS.
Like I mentioned earlier though,
some situations we'll talk about Fargate
like it's a service by itself.
In that situation, don't panic.
(chuckles)
And don't be drawn in trying
to think about ECS or EKS.
Focus more on that serverless nature of that container.
The question's probably gonna
be asking you something along the lines of,
would this be a good use case for our application?
Would serverless work for the container
that we've described?
Containers are flexible.
We can put anything we want
or just about anything that we want
into a container.
Think of a container like those microservices.
They're small, they're quick,
they're fast, they're portable,
they're immutable.
Those are all things that we want to remember
when we're looking at containers.
Generally we should favor using a container
over just deploying our application straight
to EC2 instance,
for all the reasons that I just mentioned.
Now, the exam itself isn't going to dive
too much into what's a Dockerfile,
how do we build an image,
appropriate tagging,
repo management
or anything like that.
It's going to focus more on the application
of containers inside of AWS,
with ECS, EKS, and Fargate.
So keep it pretty high level.
It's good to have practice spinning that content up,
but don't get bogged down trying
to understand all the nuances of a Dockerfile,
because that's just not going to be on the exam.
It's better to focus on the application of containers
and the benefits of migrating
to ECS,
EKS,
or even getting rid of those servers with Fargate
All right folks,
thanks for going through these exam tips with me,
and I can't wait to see you
in the next lesson.



Security
========

DDoS:
=====

Okay, hello Cloud Gurus,
and welcome to this lecture.
In this lecture we're going to look at what DDoS is.
So we're going to learn what a DDoS attack is.
We'll then look at Layer 4 attacks.
We'll look at amplification attacks.
We'll look at Layer 7 attacks,
and then we'll move on to my exam tips.
Now I'm sure you've all heard of what a DDoS attack
is before, but just in case you haven't,
basically DDoS stands for
Distributed Denial of Service attack,
and it's an attack that attempts to make your website
or application unavailable to your end users.
And this can be achieved by a multiple mechanisms
such as large packet floods using a combination
of reflection and amplification techniques
or by using large botnets.
So what is a Layer 4 DDoS attack?
Well, a Layer 4 DDoS attack
is often referred to as a SYN flood, and it's called that
because it works at the transport layer, the TCP layer.
So to establish a TCP connection,
a 3-way handshake takes place and the client sends
a SYN packet to a server, the server replies with a SYN-ACK,
and then the client then responds to that
with an ACK or an acknowledgement.
So what should happen?
Well, basically after the 3-way handshake is complete,
the TCP connection is established.
And after this applications begin sending data
using Layer 7, so the application layer protocol,
such as HTTP etc.
So in terms of SYN floods, a SYN flood uses
the built in patient of the TCP stack to overwhelm a server
by sending a large number of SYN packets
and then ignoring the SYN-ACKs returned by the server.
So it's basically manipulating the 3-way handshake.
And this causes the server to use up resources
waiting for a set amount of time for the anticipated
acknowledgment that should come from a legitimate client.
So there are only so many concurrent TCP connections
that a web or application server can have open,
so if an attacker sends enough SYN packets to a server,
it can easily eat through
the allowed number of TCP connections.
And this then prevents legitimate requests
from being answered by the server.
So that's a SYN flood attack.
Moving on to what an amplification attack is.
An amplification attack, or reflection attacks,
can include things such as NTP, SSDP, DNS, CharGEN,
SNMP attacks, etc.
And this is basically where an attacker may send
a third party server, such as an NTP server,
a request using a spoofed IP address.
If you don't know what an NTP server is,
it just stands for network time protocol;
it's the way the internet or the way computers sync up
using atomic clocks so that they all have the same time.
So essentially you get an attacker sending
a third party server an NTP request
using a spoofed IP address.
And so what's going to happen?
Well, that server will then respond to the request
with a greater payload than the initial request.
And usually this would be in the region
of about 28 to 54 times larger than the request
to the spoofed IP address.
So it's responding to the spoofed IP address.
And this means that the attacker can send a packet
with a spoofed IP address of about 64 bytes.
The NTP server would then respond
with up to 3,456 bytes of traffic.
And attackers can coordinate this
and use multiple NTP servers a second
to send legitimate NTP traffic to the target.
And that's why basically, as you can imagine,
it's called an amplification attack.
So let's have a look at it in action.
We've got the hackers real IP address,
it's 190.1.2.3 and we've got our victim's IP address
which is 245.1.2.3.
And then we've got our NTP server over here.
So what happens is our hacker sends a spoofed source address
to the destination, so sending the spoofed source address
to the NTP server, and that is 64 bytes in size.
The NTP server then replies back
to the spoofed source IP address, so 245.1.2.3,
and it's sending 3,456 bytes of traffic.
So you can see that's how an amplification attack works.
Moving on to a Layer 7 attack.
And this is pretty basic, but Layer 7 attack occurs
when a web server receives a flood of GET or POST requests,
usually from a botnet or a large number
of compromised computers.
So a typical Layer 7 attack will be something like this.
We've got our attacker, we've got a web server,
and we're going to send them a flood of GET requests.
So we're just going to say, hey, get this page,
get this page, get this page, over and over again.
And that would be done using let's say a botnet.
We then have our end user, they're trying to access
the website but they can't because the web server
is sitting there responding to all of these GET requests
from the attacker or from the botnet.
Okay, so onto my exam tips.
So just going into the exam, remember what a DDoS attack is.
It stands for Distributed Denial of Service attack,
and it attempts to make your website or application
unavailable to your end users.
And common DDoS attacks include Layer 4 attacks
such as SYN floods or NTP amplification attacks.
And then common Layer 7 attacks
include floods of GET and POST requests.
And you do really need to know the difference
between a Layer 4 and a Layer 7 attack
because there's different ways of using AWS
to prevent a Layer 4 attack versus a Layer 7 attack.
And we're gonna cover that off
in the next couple of lectures.
So if you've got the time,
please join me in the next lecture.
Thank you.


Logging API calls with Cloudtrails
==================================

Okay, hello, Cloud Gurus
and welcome to this lecture.
In this lecture we're going to look
at how we can log API calls using CloudTrail.
So first we explore what CloudTrail is.
We'll then look at a network diagram as to how it works.
We'll then look at how CloudTrail does its logging
and then we will move on to our exam tips.
So what is CloudTrail? Well, basically it increases
visibility into your user and resource activity
by recording AWS MConsole actions and API calls.
And you can identify which users and accounts called AWS,
the source IP address from which the calls were made,
and even when the calls occurred.
So I want you to think of CloudTrail
as CCTV monitoring for your AWS account.
All it is doing is tracking API calls,
who made them, and when they made them.
And so it works like this, basically we've got our end user
we've got our control plane, which is our public API,
which is essentially just our interface into AWS.
And then that goes in and makes API calls to the AWS cloud.
So we might be calling DynamoDB.
We might be setting up a DynamoDB table.
We might be storing files in S3 or deleting them.
We might be provisioning an EC2 instance, etc.
So when our user goes and does this every single API call
is logged in an S3 bucket using CloudTrail.
Things that aren't logged though is RDP or SSH traffic.
So it's not like CloudTrail can go in there
and log what command somebody entering in using SSH to EC2.
It's only going to basically log the API calls made to AWS,
and this includes anything that you do in the console.
So what is logged when it comes to CloudTrail?
Well, metadata around API calls, the identity
of the API caller, the time of the API call,
the source IP address of the API call,
the request parameters, and the response
elements returned by the service.
So what CloudTrail allows is after the fact
incident investigations, so if something has happened
and you need to investigate what that was,
you can basically do it through your CloudTrail logs.
It also gives you near real-time intrusion detection.
So you can basically integrate CloudTrail
with Lambda functions, for example
and it will give you an intrusion detection
system that you can customize.
And it also gives you industry and regulatory compliance.
So just remember that CloudTrail
is basically just CCTV for your AWS account.
It logs all API calls made to your AWS account
and it stores these logs in S3.
Now in the exam, you're going to get a whole bunch
of different scenario questions
and they'll always try and confuse you between let's say
CloudTrail and CloudWatch or CloudTrail and Amazon
Inspector or Trusted Advisor etc.
So just remember what CloudTrail is.
It's really, really important.
It's just a CCTV system for monitoring and logging
what goes on in your AWS account
using the API calls or the console.
So that is it for this lecture.
Everyone, if you have any questions, please let me know.
If not, feel free to move on to the next lecture, thank you.

Protecting applications with shield
===================================

Hello Cloud Gurus,
and welcome to this lecture. In this lecture,
we're going to look at how we can protect
our applications using Shield.
So, the first thing we'll do is learn what Shield is.
We'll then look at what Shield Advanced is.
We'll look at what the costs are.
I'll show you how to turn on Shield Advanced in the console
and then we'll go onto my exam tips. So what is AWS Shield?
Well, essentially it's free DDoS protection
and it protects all AWS customers on Elastic Load Balancers,
on Amazon Cloudfront, and on Route 53.
And essentially, what it does is it protects against SYN
or UDP floods, reflection attacks,
and other Layer 3 and Layer 4 attacks.
So, that's all that AWS Shield is.
So, moving on to AWS Shield Advanced
and what's the difference? Well, this gives you more
enhanced protection for your applications
that are running on your Elastic Load Balancers,
CloudFront, or Route 53 against larger
and more sophisticated attacks.
And, it offers always on a flow-based monitoring
of network traffic and active application monitoring
to provide near real-time notifications of DDoS attacks.
So, if you want to know if you're getting DDOSed
in real time, you want AWS Shield Advanced.
It also gives you 24/7 access to the DDoS Response Team,
or the DRT, to help manage and mitigate application-layer
DDoS attacks, and it protects your AWS bills
against higher fees due to Elastic Load Balancers,
CloudFront, or Route 53 usage spikes during a DDoS attack.
So if you want to protect yourself against having
an insane bill because you've been DDoSed,
you probably do want to turn on AWS Shield Advanced.
But there's a catch, what are the costs?
Well, we looked at Shield, it came free,
so you don't need to worry about just normal Shield,
but with Shield Advanced, it does cost 3,000 U.S. dollars
per month, so if you're a small startup,
maybe you don't need it, but if you're a large
fintech company or something like that,
then it's probably worth turning it on.
So, let's look at how we can turn on Shield Advanced.
It's super, super easy, first thing we need to do is just go
down to Security and you will find Shield here
under WAF and Shield, go in and click on that.
In here, it will give you an overview
of the different features for WAF.
If you click over here, you can go and have a look
at getting started with AWS Shield,
and it'll show you basically global activity
that's been detected by Shield,
so over the last couple of weeks.
And then, if we scroll down, it will give you the comparison
between Shield Standard and Shield Advanced.
Now to turn it on, it's super easy.
You just go back up to the top and you click in here,
subscribe to Shield Advanced, and you just need
to tick those 4 boxes and hit Subscribe.
And then, it will be set up for you.
I'm not going to do that though because I don't want
to spend $3,000 a month on my test and dev account.
So onto my exam tips, just remember that Shield protects
against Layer 3 and Layer 4 attacks only.
You need to remember what Shield is at a high-level
and that it's used to protect against DDoS attacks.
And, if you see a scenario-based question where it's talking
about DDoS mitigation or protection against Layer 3
and Layer 4 attacks, I want you to think of AWS Shield.
If it's talking about application level attacks,
that's going to be AWS WAF, also,
remember going into your exam, what Advanced Shield is.
It cost $3,000 a month, but it's going
to give you a dedicated 24 by 7 DDoS Response Team or DRT.
So again, if you see a scenario question where it's talking
about having a dedicated team at AWS to respond
to DDoS attacks, I want you to think of AWS Shield Advanced.
So, that is it for this lecture,
everyone, if you have any questions,
please let me know, if not, feel free to move
on to the next lecture, thank you.


Filtering Traffic with AWS WAF
==============================

Hello Cloud Gurus, and welcome to this lecture.
In this lecture, we're going to look at filtering traffic
with AWS Web Application Firewall or AWS WAF.
So we'll look at what a WAF is.
We'll look at Layer 7.
We'll look at the different behaviors we can configure.
We'll look at different conditions,
and then we will look at my exam tips.
So what is the AWS WAF?
Well, it just simply stands for Web Application Firewall
and it lets you monitor the HTTP and HTTPS requests
that are forwarded to Amazon CloudFront
or an Application Load Balancer.
And essentially it just lets you control access
to your content.
So using AWS WAF, you can configure conditions
such as what IP addresses are allowed to make this request
or what query strings parameters need to be passed
for the request to be allowed.
And the Application Load Balancer or CloudFront
will either allow this content to be received,
or it will give a HTTP 403 status code.
So some important Information about AWS WAF,
it operates at Layer 7.
So you are going to get scenario-based questions
where it talks about Layer 4 attacks,
what should you use. Well, you should use shield in that
case. If you are getting attacked at Layer 7,
then you want to be using AWS WAF.
So that's the most important thing to remember
going into your exam.
If you see any scenario-based questions
where it's talking about Layer 7 attacks,
you want AWS WAF.
And so at its most basic level,
AWS WAF allows 3 different behaviors.
So it'll allow all requests except the ones you specify,
it'll block all requests except the ones that you specify,
or it'll count the requests that match the properties
that you specify.
So you can define conditions by using characteristics
of web requests, such as the following.
So you can do it through IP addresses
that the requests originate from.
You can do it from the country
that the requests originate from.
And then you can also do it in the values
in the request headers.
You can do it from the presence of SQL code
that is likely to be malicious.
So this is known as a SQL injection.
So it will say, drop table for example.
Or you can do it using the presence of a script
that is likely to be malicious,
and this is known as cross-site scripting.
Or you can do it through using strings
that appear in requests, either specific strings
or strings that match regular expression patterns
or rejects patterns.
So you can define all these conditions
by using characteristics of the web requests
as we've just shown you.
So onto my exam tips.
Really the main thing to remember
is WAF operates at Layer 7.
And in the exam you will get scenario-based questions
asking how to block Layer 7 attacks.
And I want you to always think of web application firewalls
whenever you hear the term Layer 7.
WAF can block Layer 7 DDoS attacks,
as well as things like SQL injections
and cross-site scripting.
And if you need to block access to specific countries
or IP addresses, you can also achieve this using AWS WAF.
So that is it for this lecture everyone.
If you have any questions, please let me know.
If not, feel free to move on to the next lecture.
Thank you.

Guarding your network with GuardDuty
====================================

Okay.
Hello, Cloud Gurus,
and welcome to this lecture.
In this lecture, we're going to look
at how you can guard your network with GuardDuty.
So, to begin with we'll look at what is GuardDuty,
we'll look at the different features of it,
we'll look at how you can set it up,
and then we'll move on to my exam tips.
So what is GuardDuty?
Well, it's a threat detection service
that uses machine learning
to continuously monitor for malicious behavior.
And it's basically looking for things
like unusual API calls,
or calls known from a malicious IP address,
and it also looks at things
like attempts to disable CloudTrail logging,
unauthorized deployments, compromise instances,
and then reconnaissance by would-be attackers
as well as port scanning and failed logins.
So, in terms of GuardDuty's features,
basically alerts appear in the GuardDuty console,
and also in Cloudwatch events,
and it receives feeds from third parties
like Proofpoint and CrowdStrike,
as well as AWS security about
known malicious domains and IP addresses.
And it monitors your Cloudtrail logs,
it monitors your VPC flow logs, and it can also
monitor your DNS logs as well.
And essentially it allows you to centralize threat detection
across multiple AWS accounts.
And you can also have automated responses built
in using CloudWatch events and Lambda, and it gives you
machine learning and anomaly detection.
So, basically GuardDuty is threat detection with AI.
It takes 7 to 14 days to set up a baseline,
so it's basically looking for what
is normal behavior on your account.
And once active, you'll see findings
on the GuardDuty console, and in CloudWatch events.
Only if GuardDuty detects behavior it considers a threat.
In terms of GuardDuty's pricing,
you get 30 days free, and then your charges
are based on the quantity of your CloudTrail events,
and the volume of DNS and VPC flow logs data.
So, onto my exam tips, going into the exam, just
remember what GuardDuty is, and what it allows you to do.
It uses AI to learn what normal behavior looks
like in your account,
and to alert you over any abnormal or malicious behavior,
and it updates a database
of known malicious domains using external feeds
from third parties, and it monitors your CloudTrail logs,
your VPC flow logs, and your DNS logs.
And then all the findings will appear
in the GuardDuty dashboard.
CloudWatch events can be used
to trigger a Lambda function to address a threat,
so you can have a proactive threat response
that's completely automated.
So, if you see any scenario based questions where
it's talking about using AI and automation to
protect your whole AWS account and to monitor things
like CloudTrail, VPC flow logs, DNS logs, etc.,
I want you to think of AWS GuardDuty.
So, that is it for this lecture everyone,
if you have any questions, please let me know.
If not, feel free to move on to the next lecture.
Thank you.


Monitoring S3 Buckets with Macie
================================

Hello Cloud Gurus,
and welcome to this lecture.
In this lecture we're going to look
at how we can monitor our S3 buckets with Macie.
So what we're going to learn is what is Macie?
We're going to learn about personal identifiable
information, or PII.
We're going to look at automated analysis of data.
We'll look at what Macie alerts we can have,
and then we'll move on to my exam tips.
So almost any business that you deal with now
stores some kind of information about you
on their computer system.
So we have our social networks
who track who we're friends with,
what we like, even our conversations.
But even your local plumber, or your local electrician
might be storing things such as your name, your address,
your credit card information, or even your banking,
either on their local servers
or as more businesses move to AWS they're starting
to store it in the cloud.
And a lot of businesses choose
to store this information in S3.
And in the past, there's been a lot of businesses
who will store this stuff in S3
but then forget to really lock down their S3 buckets
because they are new to the cloud.
And what they're storing is what's called PII,
or personally identifiable information.
So this is personal data used
to establish an individual's identity,
and this data could be exploited by criminals,
used in identity theft, as well as financial fraud.
And this could be things like your home address,
your email address, your Social Security number--
in the UK we call it our National Insurance number.
It could be your passport number,
or your driver's license number,
or it could be your date of birth, your phone number,
your bank account, and even your credit card numbers.
So what Macie does is it gives us automated analysis
of data.
And basically what Macie is is it's a service
that uses machine learning and pattern matching
to discover sensitive data that's stored in S3.
So essentially it's using AI to recognize
if your S3 objects contain sensitive data
such as personally identifiable information,
personal health information, as well as your financial data.
And it's going to alert you
if you have any unencrypted buckets,
and it's going to alert you about your public buckets.
And it can also alert you about buckets
that are shared with AWS accounts
that are outside of those that are defined
in your AWS organizations.
And basically Macie is really great for frameworks
like HIPAA, which governs the way
we store health information in the United States,
as well as GDPR, which basically governs the way
we store personally identifiable information in the UK.
So Macie comes with alerts.
You can basically go ahead and filter and search
for Macie alerts in the AWS console.
Alerts are sent to Amazon EventBridge
and they can be integrated with your security incident
and event management or CM systems.
And they can also be integrated
with AWS Security Hub for a broader analysis
of your organization's security posture.
And it can also be integrated
with other AWS services such as step functions
to automatically take remediation actions.
So that's all Macie is. On to my exam tips.
Just remember going into the exam what Macie is
and what it allows you to do.
So basically it's an automated way
of discovering personally identifiable information.
So it's using AI to analyze data in S3
and it'll look for PII, PHI, and financial data.
It's great for HIPAA and GDPR compliance
as well as preventing identity theft.
Macie alerts can be sent to Amazon EventBridge
and integrated with your event management systems.
And you can also automate remediation actions
using other AWS services such as step functions.
So if you go into your exam
and you see a scenario-based question
where it's talking about PII and how you can prevent this
from being leaked accidentally in S3,
I want you to immediately think of Macie.
So that is it for this lecture everyone.
If you have any questions please let me know,
if not feel free to move on to the next lecture.
Thank you.


Securing Operating Systems with Inspector
=========================================

Okay, Hello Cloud Gurus
and welcome to this lecture.
In this lecture, we're going to look at how we can secure
our operating systems using Inspector.
So the first thing we're going to look at
is what is Amazon Inspector?
We're going to look at our assessment findings
and assessment types, we're going to look at how it works,
whether or not you have to use an agent or not,
and then we'll go on to my exam tips.
So what is Inspector?
Well it's an automated security assessment service
that helps improve security and compliance
of applications deployed on AWS.
What does that mean?
Well, basically, it's going to inspect
your network and your EC2 instances.
So it automatically accesses applications
for vulnerabilities or deviations from best practice.
So it could be that you've left port 22 open
on your security group, for example,
Amazon Inspector will pick that up.
So we have our assessment findings and, basically,
after performing an assessment, Amazon Inspector
is going to produce a detailed list of security findings,
prioritized by the level of severity.
And these findings can be reviewed directly
or as part of a detailed assessment report
that are available via the Amazon Inspector console or API.
So there's 2 different types of assessment.
We've got our network assessments,
and this is, basically, looking at network configuration
analysis and it checks what ports
are reachable from outside the VPC.
You don't need an agent for this,
it, basically, is just telling you,
"Hey, you've left port 22 open, what are you doing?"
And then we have host assessments, and the host assessments
are, actually, vulnerable software assessments
as well as host hardening, so it's using the center
for information security benchmarks
as to how you should configure your operating system
and then, also, just security best practices.
Now to do a host assessment, you will need to go
into your hosts and install an Inspector agent,
so you do need an agent on Inspector.
So how does it work?
Well, what you do is you go into the Inspector dashboard,
you create an assessment target, you then install
your agents on your EC2 instances.
Now AWS will, actually, automatically install the agents
or instances that allow System Manager Run Command.
You then create an assessment template,
you perform an assessment run, and then you go ahead
and you review your findings against the rules.
And, like I said, it will give you a great report,
and it will tell you how severe its findings are.
So Inspector, in terms of going into your exam,
just remember what Inspector is.
It's used to perform vulnerability scans
on both your EC2 instances and your VPCs.
These are called host assessments and network assessments,
and you can run these assessments once
or, alternatively, run them weekly.
So if you see any scenario-based questions
talking about vulnerability scans
and what service you should use,
I want you to think of AWS Inspector.
So that is it for this lecture everyone.
If you have any questions, please let me know,
if not, feel free to move on to the next lecture, thank you.

Managing Encryption Keys with Key Managment Service (KMS) and CloudHSM
=======================================================================

Okay, hello, Cloud Gurus,
and welcome to this lecture.
In this lecture, we're going to look at
managing encryption keys with Key Management Service
or KMS, and CloudHSM.
So we're going to learn what KMS is.
We're then going to look at encryption keys.
We'll learn what AWS CloudHSM is.
We'll look at KMS versus CloudHSM,
and then we will go onto my exam tips.
So what is KMS?
Well, basically KMS stands for Key Management Service
and it's a managed service that makes it easy for you
to create and control the encryption keys
used to encrypt your data.
So there's literally just a way
of managing your encryption keys.
In terms of integration, it integrates with other
AWS services, such as EBS, S3, RDS,
as well as other services to make it simple
to encrypt your data with the encryption keys
that you manage.
In terms of controlling your keys,
basically KMS provides you with centralized control
over the lifecycle and permission of your keys,
and you can create new keys whenever you want,
and you can control who can manage keys separately
from who can use them.
So let's talk about the CMK,
and that just stands for customer master key.
And basically it's a logical representation of a master key.
And the CMK includes things like metadata,
such as the key ID, the creation date,
the description and the key state.
So the CMK also contains the key material that's used
to encrypt and decrypt your data.
So when we get started with KMS,
basically you start using the service by requesting
the creation of a CMK, and you control the lifecycle
of the CMK, as well as who can use or manage it.
Now let's talk about HSM.
So HSM just stands for hardware security module.
And basically this is a physical computing device
that safeguards and manages digital keys
and performs encryption and decryption functions.
And a HSM will usually contain one or more
secure crypto processor chips.
So like I said, you start using KMS by generating a CMK,
and there's 3 ways to generate a CMK.
This first one is the easiest
which is where AWS creates the CMK for you.
The key material for a CMK is generated within HSMs
or hardware security modules that are managed by AWS KMS.
The second way to do it is to import key material
from your own key management infrastructure
and then associate it with the customer master key,
and the third way is to have the key material generated
and used in an AWS CloudHSM cluster
as part of the custom key store feature in AWS KMS.
So we will talk about AWS CloudHSM in a second.
Before we do that, let's just have a look at key rotations.
So you can choose to have AWS KMS automatically
rotate your customer master keys every year,
provided that those keys were generated within AWS KMS HSMs.
So basically you can do key rotation automatically
if you did the first methodology in the last slide.
So this is where AWS generate the CMK for you,
and they're doing that using their own HSMs.
Automatic key rotation is not supported for imported keys,
asymmetric keys, or keys generated in AWS CloudHSM clusters
using the AWS KMS custom key store feature.
Moving on to policies.
So the primary way to manage access to your AWS KMS CMKs
is with policies, and policies are documents that describe
who has access to what.
So you've seen them with IAM,
but there is a fundamental difference here.
So policies that are attached to an IAM identity
are called identity-based policies or IAM policies,
but policies that are attached to other kinds of resources
are called resource-based policies.
And in terms of our key policies in AWS KMS
you must attach resource-based policies
to your customer master keys
and these are called key policies
and all KMS CMKs have a key policy.
Now there's 3 ways to control permission.
You can use the key policy, and controlling access this way
means the full scope of access to the CMK
is defined in a single document
and this is called the key policy.
You can use IAM policies in combination with the key policy.
So controlling access this way enables you to manage
all the permissions for your IAM identities in IAM.
And then you can use grants in combination
with the key policies and controlling access this way
enables you to allow access to the CMK in the key policy
as well as allow users to delegate their access to others.
Let's move on to CloudHSM.
So AWS CloudHSM is a cloud-based HSM that enables you
to easily generate and use your own encryption keys
on the AWS Cloud.
So it's just a hardware security module
and basically you're renting it from AWS.
It's a physical device that's entirely dedicated to you,
that can be deployed quickly in a highly available fashion.
So what's the difference between KMS and CloudHSM?
Well, with KMS, basically, you've got shared
tenancy of underlying hardware and this includes
the HSMs that are used to generate your CMKs.
You have automatic key rotation,
and you get automatic key generation.
With CloudHSM, you have a dedicated HSM to you.
You have full control of the underlying hardware.
So you have complete control in terms of all the users
that you can set up, the groups, the keys, et cetera
but there's no automatic key rotation.
So moving onto my exam tips, just remember what KMS is,
it's a managed service that makes it easy
for you to create and control encryption keys
used to encrypt your data.
You start the service by requesting the creation of a CMK,
and you control the lifecycle of the CMK
as well as who can use or manage it.
There's 3 ways to generate a CMK,
and this is really important to remember.
So it's where AWS creates the CMK for you,
and the key material for a CMK is generated
within the hardware security modules
that are managed by AWS KMS.
You can import key materials from your own
key management infrastructure, so maybe you have your own
on-site HSM, and associate it with a CMK,
or you can have the key material generated and used
in a AWS CloudHSM cluster, as part of the custom
key store feature in AWS KMS.
Remember that there's 3 ways to control permissions.
This is where you use the key policy.
So controlling access this way means
the full scope of access to the CMKs
is defined in a single document,
and this is the key policy document.
You can use IAM policies in combination
with your key policy, and controlling access this way
enables you to manage all the permissions
for your IAM identities in IAM.
And then you can actually use grants
in combination with the key policies.
And controlling access this way enables you
to allow access to the CMK in the key policy,
as well as allow users to delegate
their access to other people.
And just remember the key differences between
KMS versus CloudHSM, KMS you have shared tenancy
of the underlying hardware.
You have automatic key rotation
and you have automatic key generation.
CloudHSM, you've got a dedicated hardware security module
to you and you have full control of the underlying hardware.
You have full control of the users, groups, keys, et cetera,
and we have no automatic key rotation.
So that is it for this lecture everyone,
if you have any questions, please let me know,
if not, feel free to move on to the next lecture.
Thank you.


Storing your secrets in secrets manager
=======================================

Hello, Cloud Gurus.
And welcome to this lecture in this lecture,
we're going to look
at storing your secrets in Secrets Manager.
So first thing we're going to explore
is what is Secrets Manager.
Then we're going to look
at what we can store inside Secrets Manager.
We'll then look at automatic rotation and then we'll move on
to my exam tips.
What is Secrets Manager?
Well, basically it's a service that securely stores encrypts
and rotates your database credentials and other secrets.
It has encryption in transit and at rest using KMS
so it's always encrypted.
It automatically rotates your credentials
and you can apply fine grain access controls
using IAM policies.
And it does cost some money, but it is very highly scalable.
So what else can Secrets Manager do?
Well, basically your application makes an API call
to Secrets Manager to retrieve the secret programmatically.
So it could be going in there and trying to get the username
and password to your RDS database, for example
and this reduces the risk of credentials being compromised
because they're not hard-coded into your application.
Your application's actually making calls using
an API call to Secrets Manager.
So what can be stored, where we can store RDS credentials.
We can install credentials for non RDS databases
and basically any other type of secret provided
you can store it as a key value pair.
So think of things like SSH keys, for example, or API keys.
Now it's really important to remember this going
into the exam,
but if you enable rotations Secrets Manager
immediately rotates the secrets once
to test the configuration.
So you might get a scenario-based question
where it's talking about this.
You want to enable rotation
but your application might have something hard coded in it.
So essentially you have to ensure that all
of your applications that use these credentials
are updated to retrieve the credentials
from this secret using Secrets Manager.
If your applications are still using embedded credentials
do not enable rotation
because the embedded credentials will no longer work
and this will break your application.
So it is recommended
that you enable rotation for your secrets
if your applications are not already
using embedded credentials.
So they're not going to try
and connect to the database using the old credentials.
So onto my exam tips, just remember
that Secrets Manager can be used to securely
store your application secrets.
This could be database credentials,
API keys, SSH, keys, passwords, et cetera
and applications use the Secrets Manager API to interact
with Secrets Manager.
Now rotating credentials is super easy
but you need to be careful, very popular scenario question.
So when enabled Secrets Manager will rotate
the credentials immediately,
so you need to make sure all your application instances
are configured to use Secrets Manager
before enabling credential rotation.
So that is it for this lecture everyone,
if you have any questions, please let me know.
If not, feel free to move on to the next lecture.
Thank you.


Storing your secrets in Parameter Store
=======================================

Hello Cloud Gurus, and welcome
to this very quick lecture on Parameter Store.
So storing your secrets in Parameter Store.
So first thing we're going to do is explore
what Parameter Store is.
Then we'll look at the costs.
We'll then look at the limitations
and we'll go on to my exam tips.
So what is parameter store?
Well, it's a capability of AWS Systems Manager
that provides secure hierarchical storage
for configuration data management and secrets management.
So you can store things like passwords, database strings,
Amazon machine image IDs, license codes,
all as parameter values, and you can store values
as plain text or as encrypted data.
And the great thing about Parameter Store is it's free.
Now. I know what you're all thinking.
You're like, well, I just watched a video on Secrets Manager
and Parameter Store sounds exactly the same.
Well, Parameter Store is free, Secrets Manager costs money,
and there is 2 big limits to Parameter Store.
So there's a limit to the number of parameters
that you can store.
And currently this is set at 10,000
and there is no key rotation with Parameter Store.
So basically going into the exam
you're going to get scenario-based questions where
you have to choose Parameter Store or Secrets Manager.
And if you're trying to minimize your cost
then you should choose Parameter Store.
If you need more than 10,000 parameters, key rotation
or the ability to generate passwords using CloudFormation,
then you want to use Secrets Manager
and that's all you need to know
about Parameter Store going into your exam.
So that is it for this lecture, everyone.
If you have any questions, please let me know.
If not, feel free to move on to the next lecture.
Thank you.


Temporarily sharing S3 objects using presigned URLs or Cookies
===============================================================

Okay, hello Cloud Gurus,
and welcome to this lecture.
In this lecture we're going to look
at how we can temporarily share S3 objects
using either pre-signed URLs or cookies.
So first we'll start talking about privacy
and then we'll talk about pre-signed URLs.
We'll talk about access.
We'll look at the difference between a pre-signed URL
and a pre-signed cookie.
I'll show you how to set up a pre-signed URL
using the console, and then we'll move on to my exam tips.
So in terms of privacy
all objects in S3 are private by default
and only the object owner has permission
to access these objects.
However, the object owner can optionally share objects
with others by creating a pre-signed URL,
and using their own security credentials
to grant time limited permission to download the objects.
So when you create a pre-signed URL for your object
you must provide your security credentials.
You have to specify a bucket name
and an object key and indicate the HTTP method
or get to download the object
as well as the expiration date and time.
And the pre-signed URLs are only valid
for the specified duration.
So in terms of access,
anyone who receives the pre-signed URL
can then access the object.
For example, if you have a video in your bucket
and both the bucket and the object are private
you can share the video with others
by generating a pre-signed URL.
And this is a very popular scenario-based question.
So if they're talking about, you need to share a file
but it's in a private bucket, how can you do it?
Well, you're using a pre-signed URL.
Now pre-signed cookies can be useful when you
want to provide access to multiple restricted files,
and the cookie will be saved on the user's computer
and there'll be able to browse the entire contents
of the restricted content.
So a good example of this is a photo template website.
So if you want template pictures,
like pictures of these cookies, for example,
and your users have to basically sign up to your website
and create an account
and then pay you some money,
and once they have enabled their subscription
they're then free to download any content they want.
So that's a typical scenario question you'll see.
if you want to give access to multiple files
that are in a restricted area,
then of course you're going to use pre-signed cookies.
So let's go ahead and have a look at how we can set up
a pre-signed URL in the AWS console.
Okay, so here I am in the AWS Management Console
I'm going to go over to S3
and we're going to go ahead and create an S3 bucket.
So I'm going to go ahead and create my bucket,
I'm going to give it a name,
something like acloudgurukroonenburg123
something like that.
I'm going to deploy it into us-east-1.
I'm going to leave everything as default
and go ahead and hit Create, and there we go,
so, acloudgurukroonenburg123.
I'm just going to go in there and upload a photo of me
and one of my co-instructors Faye
So I've selected the Ryan and Faye JPEG file.
I'm going to ahead and hit Upload,
and that is now uploading.
And now I'm going to go ahead and hit Close.
So in there we can now see I've got my Ryan and Faye JPEG.
If I was to click on that and then click over here
onto the object URL, of course, I'm going to get an error
because by default, everything in S3 is private.
So what I'm going to do now is I'm going to go over to EC2
and I am just going to change my region to London
because I have a server that's already set up over
in the London region that has S3 admin access
as an IAM role.
So if I click on this instance in here,
this is my instance, you can see it here
and you can see the IAM role is S3 admin access.
So what I'm going to do is I'm going to go ahead
and SSH into this instance.
So I've SSHed into this instance,
if I just type in aws s3 and then ls
I'll be able to see my different buckets.
And then if I type in aws s3 and then ls
and then s3 and then it's acloudgurukroonenburg123
and hit Enter, I'll be able to see my objects
in that bucket.
Now, if I want to pre-sign that object
all I need to do is type in aws s3
and then presign and then the link,
so, s3:acloudgurukroonenburg123/ryanandfaye.jpeg,
and then I could also do expires and then in
and then if I enter a number in here,
so say it's 3,600, that will be a 3,600 seconds.
So that will be 60 times 60, which is exactly an hour.
And that is the default value.
So you basically use this to set the time
for how long this link is valid
and that has now generated this URL.
So if I cut and paste this URL into my browser,
so I've just opened up a new tab,
I hit Enter in here and there we go,
we can see me and Faye,
there's the object that I've just uploaded to S3,
and I didn't need to do anything to make this object public.
All I needed to do is go ahead and create a pre-signed URL.
So onto my exam tips, pre-signed URLs.
If you see a scenario question where you need
to share private files in your S3 buckets
then I want you to think of pre-signed URLs.
And they're very easy to generate.
You can do it through the command line
or you can do it programmatically as well.
And then you can also set the expiration date
which is in seconds.
So that is it for this lecture, everyone,
if you have any questions, please let me know.
If not, feel free to move on
to the next lecture, thank you.


Advanced IAM policy Documents
=============================

Okay, hello Cloud Gurus
and welcome to this lecture.
In this lecture we're going to talk
about Advanced IAM Policy Documents.
So first of all, we're going to start off
with Amazon Resource Names or ARNs,
we're then going to look at IAM policies,
we'll look at permission boundaries,
and then we'll move on to my exam tips.
So ARNs stand for Amazon Resource Names,
and they basically uniquely identify
a resource within Amazon.
And ARNs all follow the same syntax.
So we have our ARN,
we then have our partition,
and not many people know this
but Amazon is split into different partitions.
So the main one that you'll see most of the time is AWS,
but then there's a separate partition, which is AWS China.
So that partition is completely separate,
so it'd be AWS-CN.
We then have our service.
So we've got things like S3, EC2, RDS, et cetera.
We then have our regions, so us-east-1 or eu-central-1.
And then we have our account ID
and that's just a 12 digit account number.
And then it will end with either our resource,
our resource type/resource
or our resource type/resource/qualifier,
or any of those other ones underneath.
Now, you might be thinking, what does this mean?
What is this guy talking about?
Well, let's have a look at some examples
and it'll make a lot more sense.
So with our top line, we're under examples.
We've got ARN and then we've got AWS,
so it's in the AWS partition.
And then we've got colon, we've got IAM,
which is the service.
And then we've got 2 colons,
and that's called an omitted value.
And the reason it's an omitted value
is because there is no region for IAM.
IAM is a global service.
So we've omitted that value.
We then have our 12 digit account number
and then we have our resource type, which is user.
And then we have our resource itself, which is Ryan.
So it's me the user.
Underneath that we've got our ARN with AWS,
so it's in the AWS partition.
And then we have S3
and then we have an omitted value with 3 colons.
So we're missing out the region
and we're missing out the account ID.
Now, why is that?
Well, we know that S3 is a universal namespace
so you don't need a region and an account ID in there
because it's using DNS to resolve our S3 buckets.
And then we have our bucket name
and then we have our object within our bucket.
So it could be image.png.
Looking at the DynamoDB example,
we've got ARN, we've got AWS
and then we've got DynamoDB,
so that's our service.
Our region is us-east-1.
Our account ID is our 12 digit account number.
Then we've got our resource, which is our table.
And our resource type is our orders within that table.
And then finally, if we have a look at the very bottom one
we've got ARN, AWS, and then EC2.
This is in us-east-1 region.
We've got our account number and then we've got our instance
and then we've got a qualifier.
And this is basically saying,
Hey, this is a wildcard.
So I'm now referring to all my instances within this region.
So within us-east-1 for this account number within EC2.
So that's how Amazon Resource Names work.
We have IAM Policies,
and IAM policies are basically just a JSON document
that defines permissions.
And we talked a little bit about this in KMS
with the different types of policies.
So when we are applying policies to users and groups
that's an identity policy,
but we can also apply policies
to things like S3 buckets or to our KMS, CMKs, et cetera;
and that's a resource policy.
And it's all great building out these policies,
but they actually have no effect
until they are attached to something.
So you can give someone S3 admin access
or you can have a policy
that gives someone S3 admin access,
but until you apply that to the group that they're in
or to their user account
they won't be able to access S3.
And basically, it comes as a list of statements.
This basically looks like this.
So we always start with our version number.
And this just allows AWS to identify
what format the statement is going to be in.
So at the time of recording,
the version number is 2012-10-17.
We then have our statements.
And in here, the statement start with the square brackets
and then a policy document is just a list of statements.
And so we've got 3 different statements in here,
they're inside our curly brackets.
So we've got our 3 in there.
And each statement matches an AWS API request.
So every time we do something in AWS,
whether it's through using the console
or if it's programmatically through the command line,
we're making an API request.
So if we provision an EC2 instance,
that's an API request.
If we go ahead and create a user in IAM,
that's an API request.
If we retrieve an object from S3, that's an API request.
So let's have a look
at a more concrete example of an IAM policy.
And so here, we've got our version
and then we've got our statement.
And then the first thing you'll see
within our statement is the SID,
so the Statement ID,
and this is basically just a human readable string
to tell you what this policy is going to do.
And then we have an effect.
An effect is either an allow or deny.
So in this particular case,
we're going to allow an action
and we're going to define what actions we allow
with which AWS Service.
So you can see the actions we've got DynamoDB.
So we're going to allow
these following actions with DynamoDB.
And then it's basically matched based on their actions.
We always have our resource,
so DynamoDB and then the action.
And some of these actions have wildcards after them.
So we've got Batch Get wildcard,
we've got Get wildcard,
we've got Batch Write wildcard,
we've got Delete wildcard,
Update wildcard and that's it.
Finally, underneath that we have our resource.
So the resource is always what the action is against.
So with DynamoDB, we're allowed to go
and do gets to this resource
and you can see the resource is called table
and then my table.
Moving on to permission boundaries.
So permission boundaries are used
to delegate administration to other users,
and basically it prevents privilege escalations
or unnecessarily broad permissions.
So for example you probably don't want your developers
to have full admin access to the AWS Console;
you probably only want them to be able to create roles
that they could attach to EC2 instances
or to Lambda functions, for example.
So permission boundaries
basically control maximum permissions
that an IAM policy can grant.
So use cases is like what I said.
Developers creating roles for Lambda functions,
application owners creating roles for EC2 instances
and then admin creating ad hoc users
so it might be people in your finance team need
to be able to go in
and check the AWS billing and pull reports, et cetera.
So that's what permission boundaries are.
So onto my exam tips.
Going into your exam you are going to get some IAM policies
that you have to read and understand.
So I would go into the IAM Policy Generator
and just get a feel for how IAM policies work.
Remember that if you don't explicitly allow something
it's going to be implicitly denied.
So if you haven't specifically allowed
an access to a particular service,
then by default, it's going to be implicitly denied.
And if you have an explicit deny,
that's going to trump everything else.
So let's say you had an explicit deny
for a user to access S3,
but then you had an allow
for them to access a particular S3 bucket.
Well, they're not going to be able to access that bucket
because the explicit deny is greater than everything else.
Remember that you can write all the policies that you want
but you have to attach them to something,
so only attached policies have an effect.
Also, just remember
that you can have multiple policies joined
to a specific resource.
So we could have an EC2 admin access policy
that's attached to our developers,
and then we could also have an S3 admin access policy
that's attached to our developer group.
And what AWS will do is join all these applicable policies.
Now, if you have something
that's explicitly denying something in one policy
and then it ends up being allowed in another policy,
like I said, an explicit deny always
is greater than everything else.
And then don't forget,
we've got AWS managed policies
and then customer managed policies.
And there's a ton of AWS managed policies,
so if you just want a administrator access to S3
that's an AWS managed policy,
but if you want to create your own custom managed policies
so only certain people can do certain things,
you can do that and save those policies.
But remember, only attached policies have an effect.
So that is it for this lecture everyone.
If you have any questions, please let me know.
If not, feel free to move on to the next lecture.
Thank you.


AWS Certificate Manager
========================

Okay. Hello, Cloud Gurus
And welcome to this lecture.
This is a really quick lecture.
We're just going to look at what AWS Certificate Manager is.
So explore what it is, what are the benefits
and then we'll move on to my exam tips.
So what is AWS Certificate Manager?
Well basically it allows you to create,
manage and deploy, public and private SSL certificates
for use with other AWS services.
Actually integrates with other services,
such as Elastic Load Balancing,
CloudFront distributions, and API Gateway.
And it allows you to easily manage and deploy your
SSL certificates in your AWS environment.
So that's all that it is, in terms of the benefits
one of the major benefits of AWS Certificate Manager
is cost.
So you don't have to pay for SSL certificates anymore
if you use Certificate Manager. Basically
AWS Certificate Manager provisions both public
and private certificates for free, and you'll still pay
for the resources that are utilizing your certificates such
as Elastic Load Balancing, but it means you don't need to go
to Thawte or Verisign
or any of the other SSL providers
and pay for a certificate.
AWS will give it to you for free.
And the cool thing is it actually automates the renewal
and deployment of your certificates.
So Certificate Manager can automate the renewal
of your SSL certificate and then automatically
update the new certificate with ACM-integrated services.
Again, such as Elastic Load Balancing, Cloudfront,
and API Gateway.
And it's really, really easy to set up.
So it removes a lot of the manual processes
such as generating key pairs
or creating certificate signing request or CSRs.
And you can create your own SSL certificates
with just a few clicks in the AWS management console.
So really going into the exam
all you need to remember is what AWS Certificate Manager is.
You're going to get scenario-based questions
around SSL certificates and what services you
should use to integrate SSL certificates with.
So the supported services are going to be things
like Elastic Load Balancer, CloudFront, and API Gateway
and just remember the benefits that essentially
it's a free service that saves you time and money.
And the cool thing is it automatically renews
your SSL certificates and rotates the old certificates
with the new certificates with those supported AWS services.
So that is it for this lecture
everyone. If you have any questions, please let me know.
If not, feel free to move on to the next lecture.
Thank you.


Security Exam Tips
====================

Okay, hello Cloud Gurus
and welcome to this lecture.
In this lecture we're just going to review
everything that we've learned in the security section
of the course.
So we'll start with DDoS, and we had 3 exam tips.
So just remember what a DDoS is.
It's a Distributed Denial of Service attack
and it basically attempts to make your website
or application unavailable to your end users.
And common DDoS attacks include layer 4 attacks such
as SYN floods or NTP amplification attacks.
And then we have Common Layer 7 attacks which include
things like floods of GET or POST requests.
We then had a quick look at CloudTrail,
and what CloudTrail allows, gives you after the
fact incident investigation.
You get near real-time intrusion detection,
and you get industry and regulatory compliance.
Going into the exam you're going to get a whole bunch of
different scenario questions asking if you should use
CloudWatch, or CloudTrail, or Inspector, or Trusted Advisor.
Always just remember that CloudTrail is basically a CCTV
for your AWS account.
This is going to log all API calls that are made
to your AWS account.
And it's going to store these logs in S3.
Moving on to Shield,
Shield protects against Layer 3 and Layer 4 attacks only.
And basically you just have to remember what Shield is
at a high level.
And it's used to protect against DDoS attacks.
And if you see a scenario question that's talking
about DDoS mitigation or protecting against Layer 3
and Layer 4 attacks
I want you to think of AWS Shield.
And you get Shield for free,
but then you can also use Advanced.
That costs $3,000 a month,
but it will give you a dedicated 24/7 DDoS response team.
We then looked at AWS WAF.
And at its most basic level
it allows 3 different behaviors.
It's going to allow all requests
except the ones you specify.
Or you can block all requests except the ones you specify.
Or it can count the requests that match the properties
that you specify.
So just remember that WAF operates
at Layer 7 and in the exam, like I said,
you're going to get scenario-based questions
asking how to block Layer 7 attacks.
So at Layer 7, you want to use a WAF.
Layer 4 you're going to use Shield.
Always think of a WAF whenever you hear the term
Layer 7.
WAF can block Layer 7 DDoS attacks as well
as things like SQL injections and cross-site scripting.
And if you need to block access to specific countries
or IP addresses, you can also achieve this using AWS WAF.
Moving on to GuardDuty, just remember what GuardDuty is
and what it allows you to do.
It uses AI to learn what normal behavior looks
like in your account to alert you
of any abnormal or malicious behavior.
And it updates a database
of known malicious domains
using external feeds from third parties.
And it also monitors your CloudTrail logs
VPC Flow logs, and DNS logs.
And your findings appear in the GuardDuty dashboard.
CloudWatch events can also be used to
trigger a Lambda function to proactively address a threat.
We then looked at Macie, so just remember what Macie is
and what it allows you to do.
Basically it's using AI to analyze your data in S3
and help you identify personally identifiable information,
personal health information, as well as financial data.
And it's great for HIPAA and GDPR compliance
as well as preventing identity theft.
Remember, you can set up alerts with Macie
and these can be sent to Amazon EventBridge
and then integrated with your event management systems.
And then you can also automate the remediation
actions using other AWS services, such as Step functions.
So if you see any a scenario-based question where
it's talking about analyzing S3 using AI
or preventing the leak
of personally identifiable information.
I want you to automatically think of Macie.
We then looked at Inspector.
And again, just remember what Inspector is.
It's used to perform a vulnerability scans
on both EC2 instances and your VPCs.
On EC2 instances, it's called host assessments.
And on your VPCs it's called network assessments.
And you can run these assessments once or alternatively
you can run them weekly.
We then looked at KMS and CloudHSM.
So KMS is a managed service that makes it easy
for you to create and control the encryption keys
used to encrypt your data.
And you can start the service
by requesting creation of a customer master key.
And you control the lifecycle of customer master keys
as well as those who can use or manage it.
There's 3 ways to generate a customer master key.
First of all, AWS creates the customer master key for you.
The key material for a CMK is generated
within a hardware security module that's managed by AWS KMS.
You can also import key material from your own
key management infrastructure and associate it
with a customer master key.
Or you can have the key material generated
and used in an AWS CloudHSM cluster as part
of the custom key store feature in AWS KMS.
Now there's 3 ways to control permissions within KMS.
You can use the key policy and controlling access
this way means the full scope of access to the CMK is
defined in a single document.
So this is the key policy.
You can use IAM policies in combination with the key policy.
And controlling access this way enables you to manage
all the permissions for your IAM identities
in identity access management.
And then you can also use grants in combination
with the key policy.
And controlling access this way enables
you to allow access to the CMK in the key policy,
as well as allowing other users to
delegate their access to others.
So in terms of KMS versus CloudHSM,
KMS basically you're using a shared tenancy
of underlying AWS hardware.
You do get automatic key rotation and you get automatic
key generation.
If you're going to use your own CloudHSM
it's a dedicated physical hardware device
that's dedicated to you.
You get full control of the underlying hardware,
and you have full control of users, groups, and keys.
But there is no automatic key rotation.
Moving on to Secrets Manager.
Just remember that Secrets Manager can be used to securely
store your application secrets, your database credentials,
your API keys, SSH keys, passwords, et cetera.
Applications use the Secrets Manager's API.
And rotating credentials is super easy,
but just be careful because when enabled,
Secrets Manager will rotate these credentials immediately.
So you need to make sure all your application
instances are configured to use Secrets Manager
before enabling credential rotation.
Now you will get scenario based questions
where it's going to test if you should
use Parameter Store or Secrets Manager.
So that will come up quite a bit.
If you're trying to minimize costs
you always choose Parameter Store.
If you need more than 10,000 parameters, key rotations
or the ability to generate passwords using CloudFormation,
then you want Secrets Manager.
Moving on to presigned URLs.
If you see a scenario question where you need to
share private files in your S3 bucket
think of presigned URLs.
And then finally we moved on to Advanced IAM policies.
So remember if something is not explicitly allowed
then it's going to be implicitly denied.
An explicit deny always trumps everything else.
It's always greater than everything else.
Only attached policies have an effect.
So if you have multiple policies
but you haven't attached them to any groups for example,
then of course they're not going to have an effect.
And you can have multiple policies attached
to a group and AWS joins all of these.
Now, if there's any kind of conflict, like an explicit deny
on one policy and then an allow on the other policy,
remember that explicit deny will always beat that allow.
And then remember you've got your
different types of policies.
So you've got your AWS managed policies
and then you've got your customer managed policies as well.
And then finally we looked at AWS Certificate Manager.
So basically all you need to do is know what it is.
In the exam,
you're going to get scenario-based questions
around SSL certificates and what services you
should use to integrate SSL certificates with.
So just remember the supported services.
So things like Elastic Load Balancer,
CloudFront, and API Gateway.
And also just remember the benefits of Certificate Manager.
It is a free service that saves time and money.
You can automatically renew your SSL certificates
and rotate the old certificates with new certificates
so long as it's with the supported AWS services.
So that is it for this section of the course.
Hopefully you've learned an awful lot about security.
If you have any questions, please let me know.
If not, feel free to move on to the next section.
Thank you.



Automation
===========

Why Do we automate ?
====================

Hello Cloud Gurus,
welcome back.
In this lesson we're going to be answering
a very important question,
and that is why do we automate?
Now you might have some sort of experience
with automation before.
Maybe you've automated something in your day to day life
with that schedule for your internet of things, light bulb
to come on and off at a certain point in the day,
or maybe you just set your thermostat to turn on and off
whenever you go to bed.
Those are all examples of when we automate
in our actual lives
and it makes our lives a little bit simpler
and a little bit easier.
We're going to apply those same principles
to our cloud architecture.
We're going to see why do I want to consider automation?
And really why do I want to avoid all of those manual steps
that I might be doing right now?
We'll get a little bit of a peak
at what tools are we going to be covering
in the next few upcoming lessons?
What options does AWS give us to make our lives simpler?
And then finally we'll wrap it all up with some exam tips.
So let's go ahead and jump right on in.
Now why do we automate?
Well, the whole problem is that manual steps are a gamble.
If you're going through and building out a VPC yourself.
Well, you've probably done that
in one of our previous lessons,
or you've seen seen us do that for you.
Now why do we care about automation?
Well, the answer is because manual steps are so dangerous.
Think about those VPCs that you've been creating
in the labs, watching us do it in the lessons.
How many steps are there
to appropriately select the correct IP range,
the amount of subnets that you're going to use,
setting up your routing tables, and your NAT gateway,
and your internet gateway, and your security groups.
Imagine how many errors you might make.
I can vouch for this. I can
tell you that I have done these things manually before.
It will take me a day to build it out
and then 2 days to troubleshoot it
because I introduce problems.
If you ask me to do the same task 10 times in a row,
I might do it once correctly
and the other 8 or 9 times
are going to be chock full of errors.
That's because we're people, we make mistakes.
Computers, however, don't make mistakes.
So the more that I can offload to the computers
to do for me, the more I get consistent repeatable results
that avoid errors and avoid introducing costly problems
into my environment.
So what benefits are we going to take away from these tools?
Why do we want to avoid those manual processes?
You have more important things to do with your life.
I guarantee it.
Nobody wants to sit down
and click that same button over and over and over again.
Imagine having to spin up an EC2 instance once a day.
That's not too bad.
Now imagine spinning up that same EC2 instance
300 times in a day. That's not going to work.
You have other things to do in your job.
Security, you will introduce security holes
if you do things manually.
I have done this.
In my previous job as a system administrator,
I have accidentally introduced a security group rule
that said open all ports
to all traffic from anywhere on the internet.
And that sat in our architecture for a couple of months
until I discovered it.
Yeah, that's not a good place to be.
When I have everything automated
I can do a security review once.
And every time I deploy that automation
I get a secure environment time after time after time.
Consistency, those networks,
those EC2 instances, are all going to look different
if you do it yourself.
So not only do we have better things to do with our time
and we need it to be secure,
we need it to be the same every single time.
Computers don't make mistakes,
if we give them the appropriate set of instructions.
So that's what these upcoming lessons
are going to be all about.
How do I use use AWS automation tools and techniques
to ensure that my architecture is consistent, secure,
and created in that timely manner.
AWS has a variety of options to automate.
There are 3 big tools that you might see
on some of your exam questions. So
those are going to be the ones that we're going to focus on
but this is by no means the extent
of all of our automation possibilities.
The first, and I would say probably the most important,
is called CloudFormation.
Now, if you've never used CloudFormation before
it's simply infrastructure as code.
Instead of clicking all those buttons to build my VPC,
all I have to do is go in and define in text form,
I want my VPC to be this size
and be in these AZs and have this IP range.
Basically I fill in all the settings,
I pass it to CloudFormation.
CloudFormation interprets those instructions,
runs that code, and makes those AWS API calls for me
so I don't have to do it myself.
Now, if you're thinking Alex, that sounds great
but I don't want to write code. I'm with you.
Writing code is not always the most fun.
Now, if you're a developer, it's a lot of fun for you
but I think we could all use this solution
every once in a while
that just holds your hand the entire way through.
And that's where Elastic Beanstalk comes into play.
Elastic Beanstalk is effectively
your IT department in a can.
All you have to do is take your application,
your web app, drag and drop, and Elastic Beanstalk
will build out the infrastructure,
build out the load balancers, deploy your applications,
set up auto-scaling.
So it combines all those previous lectures
that we've been going through into one service
that just does it on your behalf.
Now the last tool that we'll take a look at
it's called Systems Manager.
Systems Manager gives you the ability to patch, update,
manage, configure those EC2 instances
along with that on-premise architecture
so you don't have to hop into all those boxes yourself
and do the work manually.
Now, this was just a really quick overview
of all of these tools. I
promise we're going to go much deeper into each one of them.
Before we do those deep dives though,
let's close it out with some exam tips.
Now I know you might've heard in your life
that being lazy is not a good thing.
It's not a desirable trait
but when it comes to the cloud, it is.
It is if we're automating.
I don't want to have manual steps.
So if you ever find yourself clicking a button
or typing in a command think, can I automate?
And we want to keep this idea going on the exam.
Whenever possible in our scenarios
replace manual steps with automated tools.
If you are given 2 answers with everything the same
except one answer uses CloudFormation and another answer
requires you to manually build out the architecture
that it's describing,
the right answer is do use CloudFormation.
The right answer is to use automated tools
whenever possible, automate.
Now a few more tips here for everybody.
Automate everything, automate yourself out of a job.
That's some advice that I got a long long time ago
in my career.
Now we're never going to be out of our jobs
because there's always something else that we can automate,
but this is the same idea
that we want to keep in mind on that test.
Manual steps are not good.
Doesn't mean that they're wrong if they're the only option,
but whenever possible, focus on solutions,
focus on answers, that have automation.
You're going to need to be able to look
for the right tool, for the right job.
There's no one perfect solution that says,
oh, this automation tool does everything for me.
You're going to have to be able to read through the scenario
and kind of pick out
is this more of an Elastic Beanstalk situation?
Is this more CloudFormation?
Is this more Systems Manager?
Or maybe do I even need to combine a few tools here?
Now in the upcoming lessons,
we'll arm you with the knowledge that you need to know
to get those questions right.
There are benefits to automation.
We've talked about them here.
It's faster.
It's more reliable.
It's more secure.
Keep these in mind when you're reviewing those answers.
Because if you're looking for an answer
that's faster, better, and more secure,
you can probably bet they're looking for an automation step
in that answer.
You always want to keep in mind
that automation has those benefits that we've covered.
It's faster, it's more secure, it's more reliable.
So if you ever see a question
that's talking about improving, or generally anything
but more specifically, that reliability,
that consistency, and that security,
think through, can we automate some of these steps?
And automation allows you to create
immutable infrastructure.
The idea is, we can easily tear it down
when we don't need it and create it when we do.
So we want to think of everything that's been automated
as this is disposable. Now
we're going to keep this in mind and we'll talk a bit more
about that immutability of our resources
in our upcoming lesson
when we take a look at CloudFormation.
And finally,
just a heads up for all those pet owners out there,
if your dog is ever having digestive problems,
adding a scoop of canned pumpkin to their food
can help settle their stomach.
Alright folks, thanks for taking a quick look at
why do we want to automate?
I can't wait to see it in the next lesson.


CloudFormation
===============

Welcome back.
In this lesson, we're going to be taking a look
at what is CloudFormation and how do we use it
to start automating our infrastructure deployments.
So the first topic that we have to see
is CloudFormation itself.
Now, we're going to take a really brief look
at what goes into a CloudFormation template.
Before, we'd take that code
and then go and do something with it.
So the majority of this lesson
we'll be going through the console
because we really need to see this in practice
to get an idea of what can we do build using CloudFormation
and how does it function
as it's creating all of that architecture.
And then of course, we'll come back together in the end
to wrap it all up with some exam tips.
Now, when it comes to CloudFormation,
we really can't get away from it.
CloudFormation requires us to write code.
Now, I know that's painful to hear.
Whenever I hear somebody say,
"Alex, you've got to write some code."
I think, oh, here it goes.
Got to figure out how to put this together.
I promise, it's not quite as bad as actually learning
what I want to call a real language.
It's not like sitting down
and trying to learn Python or C++ or Node.js,
it's a little bit more simplistic than that.
Now on the left-hand side here,
we can see a small portion of a CloudFormation template.
Now, the basics of CloudFormation
play into the resource section of this template.
Now, at first glance, you might be thinking,
I have no idea what's going on in this.
But I promise, you actually know enough
after taking our previous lessons
to describe to me what this says.
So let's walk through this together.
The first resource that it's creating.
Well, it kind of looks like an Internet Gateway.
Now, I might not know all of these different values
or functions, and for the exam, we don't require that.
Now, I might not know all of the different values in here,
but that's not actually required for this particular exam.
You just need a general high-level overview
of what the service is and how do we use it.
So we're not going to be going through all of the
nitty-gritty because that's not required,
but we'll keep this pretty high-level.
So I'm creating that Internet Gateway,
and then if I go down a little bit farther,
it really just looks like I'm taking that Internet gateway
and I'm attaching it to a VPC.
I like to say that CloudFormation
is a little bit like duck typing.
If it looks like a duck and sounds like a duck,
it's probably a duck.
If the template says we're moving an EC2 instance
or we're making a load balancer,
you already know what goes into creating that architecture
and what those words mean.
So it's not as difficult as it might seem at first glance
to take a CloudFormation template, read through,
and get a general overview of what's happening. Now,
like I said, that's not going to be required on the exam
but I would be doing you as a learner a disservice
if we didn't at least give you a brief overview
of what these templates look like.
Now, CloudFormation templates
support either JSON or YAML formatting.
It really is personal decision
of which one you'd like to use.
I prefer YAML
because I'm a little bit more familiar with that structure,
and I also enjoy the fact that you can add in comments
where JSON does not support commenting.
So I have to write this code,
or what I'd personally recommend,
take it from some other location that you find,
maybe a coworker, Stack Overflow,
or one of those AWS getting started guides.
You take that code from wherever you grabbed it,
you bundle it all up, and then you deploy that template.
This just means you're going to take the code
and feed it through the CloudFormation engine.
And it's going to take these instructions and make it so,
basically build out everything that you've defined.
Now, it's really difficult to get a feel for CloudFormation
without actually using CloudFormation.
So let's open up my console
and start deploying some of this architecture.
Alright.
So I've got my CloudFormation console open,
and we're going to go ahead and create our very first stack.
Now I've already built my template,
so I'm going to go ahead and choose that file now.
Now, just a heads up for everybody,
I will include this specific template
in the resource section of this video.
Quick note though,
this template is only configured to run in Oregon.
It will not work in any other region.
It is also not necessarily a free template.
It doesn't spin up a lot,
it's just a basic VPC as we will see,
but just keep that in mind
that it might cost you a tiny fractional amount of money
if you do create this in your own personal account.
So you've been warned.
Now I'm going to go ahead and click Next.
I'm going to give it a simple name here.
Now this next section is called the Parameter section.
And parameters are an important exam topic
because this might come up on the test.
What these are, are they are questions
that whoever designed the template
is building into the code itself.
Then when I upload and run this template,
I get to answer these questions.
Now, I included a single parameter here.
Just says, what size of an instance would I like to create?
It provides a default value of t2.micro,
and I've also allowed my user
to select m1.small or m1.large,
but I am restricting what they type.
So I couldn't fill in something else.
So in this case, I am going to leave this as the t2.micro.
But if you'd like to at some point,
you can change that to be a different value,
whether it's small or large.
Effectively, it's giving the user some sort of input,
but it's also locking them down.
So I'm not allowed to type in something like the x1.32xl,
which is going to be much more expensive.
Next I'm going to go through
and just leave most of these blank,
because really it's all about creating this architecture
at this point.
It uploads it to an S3 bucket automatically.
So we do store that template.
And then down here, I just get that quick review
of these parameters that I wanted to create with,
and anything else that I have defined.
Now, if you do see this warning box, this is just a heads up
that the template is creating or might create IAM resources.
In this case, I'm creating a role for my EC2 architecture.
If you ever see this box, just give a quick check
to make sure you know what you're doing
so you're not accidentally creating a security vulnerability
by spinning up some unauthorized credentials
inside of your IAM service.
In this case, I designed this template myself,
so I'm good to go.
So we're going to go ahead and click Create.
Now that the wizard has been filled out
and the template is running, well, we wait.
Now, while we're waiting for these resources to come online,
I do want to show you a few quick things
in the actual code itself.
Now, if I open up the template here,
we can see that parameter.
Notice, I'm only allowing my users to select that t2.micro,
small, large, right?
I've locked them in.
It gives them some flexibility, but not too much.
The next section that I want to call out
is the Mapping section.
Now these mappings are effectively like parameters
that fill themselves in.
For example, in this situation, when my VPC is created,
it will go in and dynamically look up
these particular values.
Which site or notation range do I want to use
for my different subnets, or for the VPC itself?
This means if I want to change this going forward,
I don't have to go through and hit Ctrl+F
and find all the places this was hard-coded in.
I could also create a mapping
for something like my AMI values, or a resource ID, and say,
if I'm in Oregon, then I use this value.
If I'm in California, I use that value.
If I'm in Virginia, I use a third value.
This is so you can create one template
that can be used in multiple regions.
A common exam scenario might be
that your template is not creating correctly
in another region.
It worked in one, but not another.
The fix for that could be adding that new region
and that new appropriate ID to your Mappings section.
Now, we don't unfortunately have time
to go through everything in this template.
Like I said, this is all a little bit like that duck typing.
If it looks like a VPC and sounds like a VPC,
you're probably making a VPC.
I would really encourage you,
take this template that I've provided and go through.
What happens if you change something?
What happens when you break something?
What happens when you just upload it yourself?
Explore the actual resources.
So while I can't give you some homework,
I'd recommend you do take a look.
Now, while we've been talking,
there's been a lot of things going on behind the scenes.
Let's go ahead and check in on those.
If I click on my Events tab,
it looks like it actually just finished.
If I look through here, it's doing all the basic things
that I would need to do to bootstrap a VPC.
And in my case, create that bastion EC2 instance.
Scrolling down here, I'm making subnets,
I'm attaching route tables.
Effectively, I'm doing everything
that I would have done manually,
but I just saved myself about 5 hours of work
not having to configure that myself.
And just to prove to you
that I did in fact create something,
let's go ahead and open up my services, flip over to EC2
and go to my Instance section.
Here we can see that I actually just created that CLI host.
It is now coming online, it is currently initializing.
And that was all created in a specific VPC made just for it
because I uploaded that template.
Alright, let's go ahead and flip back to our slides
and take a look at some of those exam tips.
Alright.
My first exam tip for everybody
is that CloudFormation is perfect
for creating immutable architecture.
Now, what this really means
is that I can easily create things that I want,
and what I don't, throw it all away.
I can easily pick up that template
and run it in any environment
to recreate that exact same architecture.
This is something that we want to keep in mind on the exam.
If possible, use templates to automate the creation
and eventual destruction of my resources
because it is faster, it is more cost-effective,
because we can throw everything away when we don't need it,
and it's more secure because I can easily audit
what's going into this template
before it's actually deployed.
This consistency is what we want to keep in mind.
We want everything to be disposable.
Now a few more tips here for everybody.
I'm not going to ask you to go through
and memorize every part of a CloudFormation template.
That's probably not going to be realistic
for this particular test.
What you are going to have to know
are the really 3 general sections
that we've talked about:
Parameters.
Those are questions that the end user is asked
when the template comes online.
Mappings.
These are values that fill themselves in
based on something like the size of the VPC
or what region am I in.
And the resource section.
This is naturally where all the resources go.
You won't have to read JSON or YAML on the test
but you will have to know at a high-level
what those 3 sections are for.
If you're in a situation, like I mentioned earlier,
where you find that a template deploys in one region
but not another, hard-coded resource IDs,
such as that hard-coded AMI from that region in my template
can cause template creation to fail.
I'm going to repeat that.
Hard-coded IDs, especially AMIs,
can cause templates in other regions to fail.
Please remember this for the test
because I've seen a lot of questions
around those hard-coded IDs.
And if we put them all in a Mapping section,
we don't run into that problem.
If CloudFormation fails, which it does,
you might experience this actually.
If you start taking my template
and making some changes to it,
it's going to roll back to the last known good state,
which for a brand new template
means it's going to delete everything that it created.
This is because AWS doesn't want to leave you
with some broken architecture that you're paying for
but you're not getting any value out of.
It's just an API call.
CloudFormation is doing the same calls
that you can do manually, it just does it a lot faster
and it's a lot simpler to have CloudFormation
make the calls for you.
That is important to keep in mind.
CloudFormation can really do anything
that you can manually do.
And it's immutable.
We can easily throw it all away.
I can delete that VPC and recreate it 100 times
and I get that same consistent results 100 times in a row.
And my final tip for everybody,
contrary to that popular childhood rhyme,
stepping on a crack
will not in fact break your mother's back.
You are welcome to step on any sidewalk cracks that you see.
Alright, folks, thanks for going through this
and learning a little bit about CloudFormation with me.
I can't wait to see you in the next video.


Elastic Beanstalk
===================

Hello Cloud Gurus and welcome back.
In this lesson,
we're going to take a look at the Elastic Beanstalk service.
Now I know that's kind of a silly name,
but it's actually a pretty exciting tool.
Now, before we explore the Beanstalk service itself,
we have to define what is PaaS
or namely platform as a service.
Then we'll take a look at the Elastic Beanstalk service
itself before, of course, having that console demo
and then wrapping it all up with some exam tips.
So let's go ahead and dive right on it.
So that question of what is PaaS?
Well, PaaS stands for Platform as a Service.
The idea with Platform as a Service
is you don't really have to do anything.
The provider does everything for you.
So it's a single stop solution.
I like to effectively think of PaaS as my IT shop in a can.
All I have to do is say, here there's my web application,
this is what I've written and developed, go.
And the PaaS service will provision all the architecture,
deploy all the architecture, manage all the architecture,
monitor all of the architecture.
And basically I get to sit back and do well,
not a lot.
Everything is done for me.
Now we've really learned how to do everything ourselves
so far inside of AWS.
So that's a great place to start.
But sometimes we want a little bit more help.
We'd like somebody to hold our hands.
We have that simple web application that we want to deploy
and we can use Beanstalk to do everything for us.
So this really falls under that automation category.
Because it automates it.
It builds out the EC2 instances.
It builds out the load balancers, the auto scaling groups,
the CloudWatch alarms, and deploys your code into that.
Now you still have control over what it looks like,
as we'll see in a few minutes when we go to the console.
You still get to decide how large of infrastructure
do you want?
How small do you want?
Where do you want it to live?
So it's not that Amazon is actually taking
all of that control from you.
You just get to answer a few questions
and then they do the heavy lifting.
One of the really cool things about Elastic Beanstalk
is that it handles the deployments for us.
So we don't have to sign onto the host
and manually download something or configure my dependencies
or even use user data to try to bootstrap the architecture.
Elastic Beanstalk will handle that on our behalf.
This makes it really easy so I can work on the code
and then just hand it off to the tool and I'm done.
It also keeps the instances online and healthy.
So while you can still control the insides
of the operating systems if you want to,
it's not actually required.
Elastic Beanstalk will do all of the work that you need.
And that's the entire point of this service.
Now we can talk about Elastic Beanstalk all day long,
but I think it'll be a little bit more fun to show you.
So let's go ahead and hop over to the console
and well we'll take a look at deploying our very first
application using this tool.
So I've got my Elastic Beanstalk console open,
and if anybody's following along from home
I haven't done any additional work or setup
or anything like that, so we're all on the same page.
I'm going to go ahead and start this off
by clicking that Create Application button.
And we've got to give it a name.
I think that's appropriate for this service.
Down here we can add tags if that's something
that we need to apply, but that's optional.
Now the platform is probably the most important step
for configuring Elastic Beanstalk.
If we go ahead and click on that,
we can see there's support for a lot of different engines
or languages I should say.
We've got.NET, GlassFish, Go, Java, Node.js, PHP, Python--
I think that's in here somewhere
if I scroll down a little bit.
Yeah there's a lot.
There's also support,
and I want to call this out for the exam for Docker,
which means we can run any sort of application language
or configuration inside of a container on Elastic Beanstalk.
In general though, at least with my experience,
folks usually choose to use ECS or EKS
for containerized applications
and don't bother with Elastic Beanstalk.
Let's go ahead and pick my Python application here.
And I can pick a few different configurations
if I want to do a different version of Python.
I can select that.
Now if I had my own Python application, I could upload that.
But just for simplicity, we're
going to go with the stock standard sample application.
Now technically all I have to do is click Create,
but I'm going to show you a few more additional settings.
Now a quick note for everybody,
Elastic Beanstalk will not be featured heavily on the exam
as far as configurations go.
You need to know what does it do at a high-level
and that's kind of about it.
I'm showing you this though
because it is important to understand
that it's automating all of those standard tasks
that you would normally do.
For example, going through here,
I can pick the security group that I want.
I can decide the AMI or the size of the EC2 instance
that I'd like to bring online.
Does it need to be on demand?
Do I need to create an auto scaling group?
Those are all configurations that I can set.
I can pick the role,
I can handle how deployments are configured,
I can create a load balancer
and the list goes on and on and on.
It's kind of like, I just bundled up all of the wizards
from all of the previous services that we've talked about
and placed it on one page.
Now a quick note here,
when it says the environment is not part of a VPC
that just means it's going to use the default DPC.
It doesn't mean that it's going to magically create
this architecture outside of the VPC.
Because that's not possible from my EC2 instances.
So I'm going to go ahead and scroll all the way down
and just click on the Create App button.
Now this will take a few minutes. So,
we're going to go ahead and let this run in the background
and we'll check back in as soon as it's done.
So it looks like that just finished.
Now, if we scroll down here just a little bit,
we can see all of the recent events.
And we notice that it created an instance,
it waited for it to come online,
it deployed my application into it.
Right, it made all of the architecture for me.
Now it even gives me this nice handy dandy looking
public URL to check out my awesome website.
Now if we open it up it is well just the standard website.
Obviously you'd want to customize this
and configure it yourself.
But the point of Elastic Beanstalk is that it does basically
everything that we have just spent all of those hours
learning in the course of about 2 minutes.
Now, it's still important to know how all of these things
work underneath the scenes,
because at some point you're going to have to build
this yourself and Elastic Beanstalk
will be just a little bit too much handholding for you.
Now there is one other thing that I'd like to show you
real fast.
And that is if we go to services here,
and go to EC2,
and open up my instances,
this right here is a standard EC2 instance
that it just brought online.
There is no fun magic going on behind the scenes.
It's all just automation and API calls. Alright,
that's about as much as we're going to need to know
for the exam regarding Elastic Beanstalk.
So let's go ahead and jump back over to the slides
and take a look at some exam tips.
My very first exam tip is that Elastic Beanstalk is easy.
If you're on the exam and you're looking
for a one-stop solution,
this is going to be the service that you'll want to pick.
So if the exam is talking about speed,
ease of use, you want to automate,
but you don't really want to do much work,
that's when Elastic Beanstalk comes into play.
As we saw that approach of bring your own code,
well it's pretty much accurate.
You bring your own code, bring your own application
and that's it. Beanstalk does the rest
of the lifting for you.
So a few more tips here for us.
Elastic Beanstalk is truly that Platform as a Service.
It builds the platform
and just stacks your application on top of it.
Now for the test,
you don't have to memorize every single platform
that it supports
because that would be kind of painful.
What you should know is that it supports Windows
and Linux applications and it also supports
containerized applications.
Because while we probably would never do this in real life,
on the exam I have seen a situation in which we do select
to run in a containerized application on Elastic Beanstalk
because ECS and EKS were not listed in the answers
and the scenario was specifically asking
for an automated solution.
So in that kind of very narrow, narrow description,
Elastic Beanstalk and containers can work together.
Elastic Beanstalk does a lot of handholding.
It's great for simpler applications,
but as your applications grow and develop
you're probably going to outgrow that Platform as a Service.
It's going to do too much for you
and you're going to kind of clash
with how it's handling things.
So it's great to start, but long-term,
well, that's why you're learning how to do
all of this stuff yourself.
Now, it handles traditional architecture.
As we saw, is just an EC2 instance.
It's a VPC.
It's auto-scaling.
It's a load balancer.
It's not spinning up Lambda functions
or handling anything in the serverless architecture realm.
That's important to keep in mind
because if the exam is requesting a serverless solution,
Elastic Beanstalk's not it.
And finally, if you ever find damaged money
such as a ripped or torn bill,
as long as you have more than 51%
you can exchange that at the bank for an undamaged copy.
Alright folks, thanks for learning about
Elastic Beanstalk with me
And I can't wait to see you in the next video.


Systems Manager
===============

Hello, Cloud Gurus and welcome back.
In this lesson, we're going to be looking at
how do we manage systems using Systems Manager.
Now these are the systems that we need to manage
as we've provisioned them and lack a Systems Manager tool.
If you couldn't guess by the title,
we'll be taking a look at the Systems Manager service.
Now, the first question is what is Systems Manager?
You could probably guess by the name
that it's managing systems.
What does that actually mean?
Once we've established that,
we'll get to see all the different features
that the Systems Manager's suite of tools gives us.
It's not just one individual service,
Systems Manager wears a whole lot of hats.
Of course, we're going to dive into the console
right after that,
and then we'll come back together to wrap it all up
with some exam tips.
So let's go ahead and dive right on it.
So what is Systems Manager?
Systems Manager is really just a collection of tools
that we can use to automate the management
of my EC2 architecture.
It also supports your on-premise resources.
So it's not just for the cloud.
Now, this is actually a free set of tools
that AWS has given us to basically
just make living in the cloud
with those traditional operating systems
a little bit easier.
What things can Systems Manager do for us?
Well, we've got some pretty common sysadmin style tasks
that we might run into on a day-to-day basis
that systems manager can assist with.
So what tools does Systems Manager bring to the table?
Well, the very first one is called automation documents.
Now automation documents allow you to configure
the inside of your EC2 instances or configure AWS resources,
such as setting an S3 bucket policy
or configuring something inside of your account. Now,
I just want to call something out really quickly though.
We're going to call automation documents
well, automation documents.
Technically there has been a name change.
They're now referred to officially as runbooks.
However, the exam from what I've seen still refers
to these as automation documents.
Now, if you're watching this much farther in the future
and you see runbooks on the exam,
know that they are the same thing as automation documents.
And we'll get to see these in the console
in just a few minutes.
Now that run command.
The run command uses the Systems Manager agent
that's installed on your EC2 instances to do things.
What you can do is you can run scripts.
You can run calls inside of your operating system
and then get a report back to see did it work
or did it fail?
This means if you had thousands of instances
you wouldn't have to manually go in one by one
and run commands.
You can do it across an entire fleet.
You can patch your applications.
Now back when I started in IT, this was done manually.
I had to go through server by server, one after the other
and click that Windows Update button
over and over and over again
until it felt like my fingers were going to fall off.
Now I can just schedule a time,
and what patches do I want to apply,
and Systems Manager can handle that for me.
Parameter store.
This is where you're going to keep all of your secrets.
Now not your personal secrets,
although those would technically work in here.
But more like your passwords and usernames
and things that you should not be hard-coding
into the code itself.
And we'll see this as well in a minute
inside of my AWS account.
Now for the exam, we want to keep in mind
the Systems Manager requires an agent to be installed
onto either your EC2 instances
or your on-premise architecture.
Because systems manager supports both.
And finally, it also allows you to connect and remotely
interact with your architecture through your browser.
So you don't have to manage all those SSH keys yourself.
Now, this is by no means the entire suite
of Systems Manager tools.
It does a whole lot more but this particular exam
that we're studying for,
is not going to cover it in a whole lot of depth.
So we'll hit on the important points here.
If you're curious about some of the other features,
I'd recommend taking a look at the service yourself,
whether that's in your own account
or one of the sandbox accounts.
But we're going to limit it here to just
what do we need to know
for the Certified Solutions Architect Associate exam?
Alright, now this is great to talk about in theory
but let's hop into my console and see this in action.
Okay, so I've got the console open here
and for those folks following along at home,
there are a few prereqs for this.
First, you do need to have an EC2 instance
that has been created.
And second, you need to have the Systems Manager agent
installed on that host.
Now I will include a link to the documentation
in the resource section of this video.
So be sure to check that out.
Now, the first thing that I want to look for here
is on the left-hand side.
If we scroll down to my Managed Instances section.
Now all it takes for my instance to be managed
by Systems Manager is to install the Systems Manager agent
and give the instance a role or permissions to communicate
to the Systems Manager service.
Now, if I go ahead and look at this instance here,
I can see a few things here.
First, if I take a look at the file system
I can actually browse the entire disk content
through my browser.
That's kind of cool.
So I can open up etc
and see what exactly do I have inside of this folder.
Now I'm not going to explore this instance all day long
because there's a lot of other things to look at.
But if I go back to my instance here
and click on Instance Actions,
I can go ahead and start a session.
I just signed in the host.
Let's take a look.
Cool, I'm on the box.
I can run my updates.
I can type correctly.
And yeah, it looks like there's some
that needs to be applied.
So I'm going to go ahead and let that run in the background.
But the point is I don't have to manage SSH anymore.
That's just handled for me.
Now, if we flip back to Systems Manager here,
I want to call out a few other things.
First is that parameter store. This
is going to be where we want to store our secret values.
So let's create a parameter right now.
So this is basically just a key value storage.
Supposed to be my secret-password.
And I can define a string, a string list,
or a secure string.
Now a secure string is actually encrypted.
So it's not holding onto anything in plain text
and this is what I'm going to want to select
for those passwords.
I think that's an appropriate password to have there.
And I'm going to go ahead and click Create
and that's literally it.
Now I can see this value in the console.
If I open it up and I click show, notice,
it's not kept in plain text and I can get my value back.
Now you might ask, why would I do this?
Because this is kind of not that exciting
just at first glance and I'm with you.
But what you want to do is couple this
with your deployments, right?
This is you don't have to store your values
in plain text in things like GitHub or your source code
or configuration files.
Simply store it in parameter store,
and then when the application needs to download this value,
have it dynamically look it up out of parameter store.
I like to think of parameter store
kind of like the last pass or key pass of AWS.
This is where you want to store your super secret values.
Now, we will talk about one other option here
a little bit later in an upcoming lesson.
And that is going to be secrets manager.
And we'll talk about the differences between the two then.
But we do want to keep parameters store
in mind for the exam.
Now let's scroll all the way down
and I want to show you some of those automation documents
also known as runbooks.
Now, there are a lot that are available here.
Apply an Ansible playbook, apply patches to windows hosts,
apply Chef Recipes, attach EBS volumes.
You don't need to know what all of these do
because that would be just a crazy amount of memorization.
But what you need to know is we can use automation documents
or runbooks to control the insides of our operating systems
or the AWS environment itself.
Here we see
I'm attaching an EBS volume.
Here, I'm configuring a package on an operating system.
So Systems Manager is kind of the jack of all trades
for managing everything EC2 or on-premise
or even AWS related.
Alright, let's go ahead and switch back
and wrap this all up with some exam tips.
My first exam tip is the parts are bigger than the whole.
What that really means is on the exam.
It's very rare for Systems Manager to be called out by name.
Basically, we'll just use the name of the features instead.
So instead of saying it's the Systems Manager parameter
store, it'll just be called parameter store
or session manager, or automation documents.
So keep this in mind as you're going through
and don't get tripped up looking for Systems Manager.
Out of this lesson,
you want to take away what session manager does,
what those automation documents can do, and parameter store.
Those are going to be the big ones
that you might encounter on the test.
Now, a few more tips here.
It's important to remember that Systems Manager
supports on-premise architecture
and you have to have that Systems Manager agent
installed on the instance.
You don't need to do a deep dive.
That's going to be the more SysOps style exams.
What you need to know is that it's generally used
for patching, updating, configuring,
and those automation documents can be used
with another service called AWS Config,
which we're going to explore in a future lesson.
Basically, Systems Manager is your unpaid SysAdmin.
If an admin can do it,
Systems Manager pretty much can as well.
In fact, Systems Manager is free.
There's no cost associated with it
as long as it's inside of AWS.
There is a small fee for managing on-premise architecture.
And finally keep those automation documents in mind
when we're talking about config
or when you need to automatically enforce a standard.
And I know we haven't talked about config yet
but I promise we're going to get to that
in just a little bit.
And one last tip here for everybody.
If you're ever attacked by a bear, it's best to play dead.
You want to lay on your stomach with your fingers
interlocked over your neck, spread your legs and elbows.
So it makes it harder for the bear to roll you over.
Hopefully that never happens to you though.
All right folks, thanks for learning about systems manager
with me and I can't wait to see you in the next video.


Automation Exam Tips
=====================

Hello Cloud Gurus, and welcome back.
In this lesson, we're going to take a look at some exam tips
for all of the automation tools
that we've covered in the previous lessons.
So let's go ahead and dive right on in.
I have 4 questions here that I want you to keep in mind
and these 4 questions are going to apply
to really every exam question
that you're going to read through.
The first question is, can you automate?
The answer is most likely yes.
If you're ever reading through a scenario
and it's talking about a manual step,
you should be thinking which one of my automation tools
can avoid that manual procedure.
Can I use CloudFormation to build all the architecture?
Can I use Elastic Beanstalk to offload
all of that work of setting everything up myself?
Or could I use Systems Manager
to handle configuring the insides of my EC2 instances?
What kind of automation is going to work
in this particular scenario?
There's no one size-fits-all.
We covered that a little bit in the previous lessons.
I can't just say, oh this is the automation tool
that I'm using in every single situation,
because they all kind of have their own use case.
Systems Manager for the inside of those instances,
Elastic Beanstalk to really act as that one-stop IT shop,
and CloudFormation to make that immutable AWS architecture
that we can easily deploy and then throw away.
Is the automation repeatable?
It's a good question.
Sometimes we can write a script
or a program ourself to do something using API calls,
or a CLI or an SDK, and it works.
But if it's not built with one of our frameworks,
sometimes it's not a repeatable situation.
So while you could see the answer on the exam
of, oh I wrote my own CLI script,
We generally want to focus on answers
that use one of the AWS-provided frameworks,
such as the few that we've gone through in this section.
And then finally,
with this work cross-region or cross-account?
I've seen scenarios that say, hey, we've got a template
that's working in region A, but it doesn't work in region B,
or I deployed in my account,
but it doesn't work in my co-workers account.
How do we solve this problem?
We've talked about this a little bit
with not hard-coding those IDs,
not hard-coding those values inside of my templates,
but we want to keep this in mind.
Hard-coding IDs or even those soft limits
inside of an AWS account,
are 2 of the biggest reasons
that you're going to see your automation fail
when it moves cross-region or when it moves cross-account.
Let's take a look at a couple of tips
that are specifically CloudFormation-related.
The first one is we really need to understand
those CloudFormation sections.
Like I said, in the CloudFormation lecture,
we don't have to be a coding expert.
They're not going to ask you to debug in-depth
why this particular block of code isn't working.
You're not going to have to go through the nuances
of JSON versus YAML.
In general, you just need to know
that use case for the parameter.
It's those questions that you're asking the end user
who's building the template,
the mappings, how they can control
or automatically fill in a value
such as what is the particular AMI
that I would like to use for this region,
and the resource section.
The resource section
is going to contain all of the resources, right?
Those instances, those networks, those databases,
everything that you're trying to build out.
So just keep all 3 of those in mind
as you're looking at CloudFormation questions.
Immutable architecture is the best.
On the exam we like to prefer answers
that include a stateless environment.
Something that I can easily pick up and throw away,
and reprovision, it's that disposable architecture.
That's what I want.
We want to have automation built out that is so robust,
that I can turn it off. I can chuck it out the window
when it's no longer needed.
Now that doesn't mean that I'm proactively
terminating the database every single day,
because obviously that wouldn't work out so well for me,
but it could mean that I'm throwing away those web servers
those backend servers,
I'm hosting my data in S3 or EFS,
and I'm treating that compute
as something that is disposable.
Now you might run into a scenario
where you're asked to deal with particular IDs.
This could be an AMI ID, a snapshot ID,
and remember we never want to hard-code those.
So the right answer is going to be,
store those in the mapping section,
or use Parameter Store.
Now the little bit about Parameter Store
in that Systems Manager lesson.
Parameter Store remember,
it's kind of like that universal key value storage,
and CloudFormation can look up values
that are stored in there.
So mappings and parameters are an appropriate location
to store those IDs,
as long as we are never hard-coding them directly
into the resource section itself,
as that's much more likely to cause that breakage
when you move that template from one environment to another.
Now we've got a few more tips on that Elastic Beanstalk
and Systems Manager service.
So we generally just need to know with the Elastic Beanstalk
that it's your one-stop shop for everything AWS.
You're building out a web server,
that's really what it excels in.
If it's looking for a situation that is simple,
I've seen questions that say,
yeah, we want to make a quick migration,
or my coworkers are not interested
in learning CloudFormation.
Well, then we can focus on using Elastic Beanstalk
over that CloudFormation architecture.
If the scenario is a little bit more complex,
if it's talking about things like Lambda functions
and sqsqs and load balancers, and API Gateway,
and a little bit more robust architecture,
then we'd want to think CloudFormation,
as that gives us the most granular level
of automation control over everything that we're building.
Now, when it comes to Systems Manager,
those automation documents
are going to be very useful to you.
These automation documents can be used
to configure the insides of EC2 instances,
as well as parts of the AWS environment.
So they're kind of the jack of all trades automation tool.
Now, speaking of Systems Manager,
you don't have to dive too deeply.
That's going to be featured a lot more on the SysOps exam.
So in general, they want to see that you understand,
hey, we can use automation documents to control things
inside of my AWS account and inside of my EC2 instances,
potentially you might see a question or two
around storing new value in Parameter Store.
And that session manager that we walked through in that demo
might come up,
however, I'd say that's pretty unlikely
but it couldn't hurt to have just a little bit of practice
utilizing that tool.
All right folks, that's going to wrap it up
for our automation section.
Thanks for going through these exam tips with me
and I can't wait to see in the next video.


Caching Overview
================

Hello, Cloud Gurus,
and welcome back.
In this lesson,
we're going to be taking a high-level look
at an IT concept known as caching.
So the very first thing that we have to answer is,
what is a cache?
Why does this matter,
and when do I want to use it?
And we'll see kind of a quick review
of where do we cache.
Where inside of our environment
can we actually put caches?
We're going to take a look
at some upcoming AWS caching solutions
that we'll see in future videos.
And then of course,
we'll round it all out
with some of those handy dandy exam tips.
So let's go ahead and dive right on in.
Now to understand what caching is,
we're going to take a quick look
at what life is like without a cache.
On the screen in front of you,
we have Fred.
Now, Fred is is very hungry.
Recently, Fred's been thinking a lot about ice cream,
specifically purple ice cream.
He wants some.
How does Fred get that purple ice cream?
Well, of course,
because he doesn't have a cache,
Fred gets in his car and drives 25 miles down
to the ice cream factory.
Where else would you get ice cream?
He buys his purple ice cream
and drives it home and eats it,
and life is grand.
Except Fred doesn't like to drive 25 miles
every time he thinks of purple ice cream.
If only there was a better solution.
Now this is what life is like without a cache.
Without a cache
if I want to view some web content,
I got to go all the way across the world to get it
and have it delivered back to me,
and it just takes a long time.
So let's fix this problem for Fred.
Now let's give Fred the ability
to cache that purple ice cream.
So in this case, what Fred's going to do,
is Fred's going to buy a freezer.
Fred puts that freezer in his house,
and now instead of having to get in his car
and drive 25 miles to go get that ice cream
every time he wants it,
he simply throws on his slippers,
walks out to the garage,
and gets some.
Now at some point,
Fred has eaten all that ice cream
or maybe the ice cream's gone bad.
It's been in the freezer for too long,
and isn't that a travesty.
So in that case,
Fred can then go that extra 25 miles down
to the ice cream factory,
buy all the ice cream that he wants,
store it in the freezer,
and then continue to hit that freezer
every time he needs it
until eventually it's out.
Now, this same process works inside of AWS.
We can put a cache close to our end users
so they're not traversing all the way
across the globe
to download or view our content.
So obviously life is better with a cache.
So where do we cache in our AWS applications?
Well, we kind of already talked
a little bit about that external caching,
the idea that I'm cashing those images, videos,
and static content closer to our end users.
And we'll talk about that content delivery network
in a future lesson
that we can use to distribute our content externally.
Now we can also cache internally as well.
We can put a cache in front of our databases.
Now this means we can cash queries
that are performed often.
So we don't have to talk to the database as much.
Now, if you've ever built an application before
you can probably relate to this,
that that database is generally a bottleneck.
So the less reads that we have to do
from that database,
the better my application is going to perform.
So what specific caching options
are we going to see inside of AWS?
Well, there are 4 solutions
that we're going to cover
in those future lessons.
Our first cache is called CloudFront.
CloudFront is actually an external caching solution,
a content distribution network that allows us
to spread out our resources all over the globe
to make those delivery times
just a little bit smoother.
We'll also take a look at ElastiCache.
Now, ElastiCache is going
to be an internal caching solution.
That's going to sit in front of our databases
to help offload some of that work
that the database normally has to do.
We'll also see DAX,
or DynamoDB Accelerator.
Now DAX is a caching solution
that is built specifically for DynamoDB.
So if you already thought that DynamoDB was fast,
well we're going to make it just a bit faster.
And finally, we'll take a look at Global Accelerator.
Now, Global Accelerator is another tool
that we can use to help speed up external connections.
So by the end of this section,
you'll have a good overview
of external versus internal caching.
Now let's go ahead and wrap this up
with some exam tips.
My first exam tip is that AWS loves caches.
They really do.
Whenever possible,
pick a solution that includes caching.
Even if it's not specifically called out,
you want to favor answers
that have caches built in.
We'll take a look at the various types coming up,
but you just want to keep that big heart emoji
in mind as you're going through the exam.
Now, a few more tips here for us.
Put caches everywhere.
If it doesn't have a cache,
put one in front of it.
We'll take a look at how we can do this,
but we want to cache everything
and cache everywhere.
Caches help with speed.
That's just a fact.
Now there are other benefits
that we're going to take a look at as well.
Sometimes it helps with failure.
Other times it helps with security,
but the primary one is going to be that performance.
As we go through this,
you'll need to keep in mind
which caching solution works
internally versus externally.
If something's an internal cache,
it's really not going to be an external cache.
You're going to have to pick one versus the other,
and make sure that in the exam,
they're not being misused.
And finally,
before you ever trim a hedge,
lay down small sheets underneath it.
This will make collecting the hedge trimmings
much easier when you're done.
Alright, folks,
thanks for going through this caching overview with me,
and I can't wait to see you in the next lesson.


Global Caching with CloudFront
==============================

Welcome back.
In this lesson, we're going to be taking a look
at one of AWS's most popular caching solutions, CloudFront.
We're going to start it off by taking a look at
what is CloudFront,
where do we want to put it inside of our architecture,
and how does it use those edge locations?
We'll dive into some important settings
that you need to be aware of
when you're setting up your CloudFront distribution.
And then we'll go ahead and jump into my console
and build one out.
And then we'll loop back after that
with some of those exam tips.
So let's go ahead and dive right on in.
So what is CloudFront?
Well at its core, CloudFront is a content delivery network
commonly called a CDN.
What CloudFront allows us to do
is to take our static content, our images, our videos,
our applications, and distribute them out
to those edge locations all over the globe.
You remember those edge locations from a previous lesson?
Those 100 plus locations all over the world?
Well instead of storing all of your content in
one single location and making your users come and get it,
we can store it basically everywhere.
So let's go ahead and actually see what this looks like.
Now imagine for a second that you live in Europe
and for my European friends,
this is probably a very real reality for you.
You would like to look at
some of those amazing American cat photos
that I've stored in my Oregon bucket in S3.
Now if you're sitting there in London or in Paris
or somewhere in Germany trying to download this,
your cats have to travel a very long distance
and that makes you sad because they load line by line
and it's just not fun.
You want your cats delivered quickly. So
what we're going to do is we're going to insert CloudFront
in between.
Now what's going to happen is my very first user
is going to make a request through CloudFront
to say, hey give me some of those American cats.
CloudFront is going to say, great, I'm going to go to S3,
grab those cats, I'm going to hold on to a copy
in that local edge location,
and then I'm going to return those cats to that user.
Now that didn't really speed much up on the first request.
But what about the second request?
The third request, the fourth request, the nth request.
Because there is a copy of those cats
stored in that edge location,
those users get their content quickly.
So this allows me to take my single location
for those cat photos
and effectively make it a global cat distribution service.
Now there are reasons to consider CloudFront
besides just the blistering speed that it brings
to your cat distribution service.
The first reason is security.
It actually defaults to an HTTPS connection.
Now that's great out of the box
because we always want to have HTTPS endpoints.
It also allows you to attach a custom SSL certificate
if you don't want to use the CloudFront end point.
Effectively, if you want to route
your entire domain through CloudFront,
it's really simple to do.
It also allows you to put a secure connection
on your S3 static websites
and that's an important factor remember for the exam.
With CloudFront, we actually can't pick specific countries
or specific edge locations to distribute your content to.
Now as we'll see in a few minutes,
you can pick general areas of the globe
but it's maybe more continent based than anything else.
You can create a very rudimentary allow or deny list
for particular countries.
But if you do have to block or allow particular countries,
you don't really want to use CloudFront to do it.
You want to use the web application firewall
to make that happen because it's a lot more feature rich.
CloudFront supports AWS endpoints.
Now that's kind of a no brainer.
You could have all guessed that.
But you might be surprised to know it actually supports
non AWS endpoints.
So if you've taken this course up until now
and you've said, I hate everything AWS-related
but I love content distribution networks,
you could use the CDN to front your data center
or another cloud provider.
It's up to you.
Now whenever you think of a cache,
you should be thinking about steal content.
Normally you want to rely on that TTL, that time to live.
It's really going to set how long your data is going to live
at those edge locations before it gets refreshed.
Now if you make a mistake,
namely, maybe you set that to wait too long
before the content is refreshed,
you can go out and proactively expire that content.
You can force a refresh
to kick your content out of the cache
and go and get something new.
Now CloudFront is great to talk about in theory.
Let's go ahead and jump into my console
and build out a distribution for one of my websites.
I've got my CloudFront console up.
Now for those folks following along at home,
I do have a static S3 website that I've created
before I started this demo.
Now a quick note, this console is likely to change
in the near future.
It is ripe for a refresh
using the new Amazon design language.
So if it has changed by the time you've seen this video,
please don't panic.
I promise the content is all still relevant.
They just like to move the buttons around.
So let's go ahead and create our first distribution here.
So I'm going to create a web distribution
and I'm going to start off here with the origin domain name.
Now I'm actually going to pick from here
a list of my S3 buckets, maybe my load balancers,
right--effectively what's serving out that content.
In my case, I actually just want to paste in here
my S3 bucket URL
because this is a static S3 website that I'm going to use.
Scrolling down a little bit farther,
unfortunately, in this video,
I'm not able to talk about all of these settings
because there are a lot.
The good news for you is
most of them aren't actually relevant
for the associate level test.
We'll circle back to this content in the professional exam
if you choose to take that step,
but I will call out a few important features
that you will need to know for this particular exam.
Right now, I'm allowing both HTTP and HTTPS.
I can restrict that via redirect or just have HTTPS only.
Little bit farther down here.
It is important to know that I can create signed URLs
or have signed cookies to restrict what user or users
can access my content
and for how long that's going to be accessible.
It is important to know that you can restrict access
to your content via CloudFront using signed URLs
or signed cookies.
You don't need to know how to do this
for this particular exam.
You just need to know that this is an option
that you can add in those temporary access points
to your architecture.
Little bit farther here.
We can see that
I actually can't pick particular edge locations.
In fact, it's really just continent-based.
Here, North America and Europe, the rest of the world.
I'm just going to leave it with all edge locations.
We can't pick a particular edge location
nor really would we want to.
Now if you remember from our security lecture,
we talked about that web application firewall.
We can attach a WAF to our CloudFront distribution
if we need that additional level of security and control.
I'm using the default SSL certificate.
But if you want,
you can upload your own custom SSL certificate
and run your entire domain through it.
If you have something a little bit better than mine,
maybe you own catvideos.com
and you want to build your cat empire.
You can serve it out securely.
Now the rest of this,
I'm just going to go ahead and leave blank
or default, I should say,
and I'm going to click Create.
Now this is going to take a few minutes.
So I'm going to go ahead and pause the video
and we'll check back in as soon as this is finished.
Alright, looks like that just finished.
So let's go ahead and open up my distribution.
And there's a lot of settings on here,
but the one that we really care about
is going to be that domain name.
So let's go ahead and grab that real fast.
I'm going to open up a new tab and paste that in.
Now I feel the need, the need for speed.
Alright, not the most exciting website
but it is using those edge locations
to get that content out quickly
and notice up here in my browser,
this is a static S3 website and it has magically HTTPS.
Well, it's not magic, it's CloudFront.
But that's a good thing to remember.
Now I know this was a whirlwind tour of CloudFront
but let's go ahead and jump back into the slides
and cover some of those exam tips.
My first exam tip is CloudFront fixes all connection issues.
Now I understand in the real world
things are a little bit more nuanced
but remember on this exam, AWS loves caches.
So put a cache in front of your application.
Use CloudFront to cache your content externally.
Slow connections from around the globe, cache it.
Images or videos aren't loading, cache it.
And we use CloudFront to make all of that happen.
Oh, a few more tips here for us.
CloudFront is fast.
In fact, speed is the main purpose
of what we want to use this for.
So we can cache those static images, videos, right?
The things that take a long time.
All of those cat pictures that we want to look at.
Cache them in those edge locations using CloudFront.
Supports both on-prem and cloud architecture.
So it's not just for AWS.
That's going to be an important tidbit for you to remember.
CloudFront can be used to block individual countries
but the WAF is going to be a better option.
Because remember the WAF gives us more flexibility
as far as rate-limiting, SQL injection attacks,
has more nuance around it.
Where CloudFront's just going to be allow or deny.
It's very simplistic.
So in general, we want to front our CloudFront distributions
with a WAF to give us that finer level of control.
You can use all the locations
or you can kind of dictate particular continents.
That's really about it.
So on the exam, make sure you don't pick answers
that say, oh we're only using this specific edge location
because that's not actually offered by CloudFront.
And one final tip here for us,
if you're trying to melt snow while you're on a hike,
pour some water into the pot.
It will dramatically reduce the time that it takes
for the snow to melt and you'll save a lot of fuel.
All right folks,
thanks for going through CloudFront here with me
and I can't wait to see you in the next lesson.


Caching your data with Elasticache and DAX
===========================================

Hello, Cloud Gurus and welcome back.
In this lesson, we're going to take a look
at 2 different kinds of AWS database caching solutions:
ElastiCache and DAX.
We're going to go ahead and kick this off with ElastiCache.
What is it?
How do I use it?
And when do I want to start applying this
into my architecture?
We'll take a little bit of a look at the different forms
of ElastiCache, namely Memcached versus Redis.
And then we'll dive into Dynamo DB Accelerator,
or as it's more commonly known DAX.
We'll have a quick review
of when do I want to use ElastiCache versus DAX
before I round this all out with some exam tips.
So let's go ahead and dive right on it.
So to that first question, what is ElastiCache?
Well, ElastiCache is kind of a unique service.
It's an AWS managed version of 2 open-source technologies:
Memcached and Redis.
Now, neither these tools are specifically built for AWS
but they're common in a lot of architecture patterns.
So AWS designed ElastiCache to allow you to spin up
one or the other, or even both sometimes,
to avoid a lot of common issues that you would have
as an architect rolling these out yourself.
So if it's really just Memcached or Redis in disguise,
well, what are those tools?
Well, let's take a quick look at
what is Memcached compared to Redis.
So both of these tools are generally going to sit
in front of your database.
So they sit inside of your architecture.
They're not external, they are internal.
And they're going to cache those common queries
that you would need to make.
As we talked about earlier, this means less reads back
to that data store.
So Memcached is just a database caching solution.
Nothing you're going to store in here
is going to be permanent,
so it's not a source of truth for that data.
Memcached is not a database by itself,
it is just a caching solution.
It doesn't offer failover
or multi-Availability Zone support or backups even.
So even if we wanted to use it as a database
it really wouldn't be a very good idea.
Now, Redis on the other hand,
it's supported as a caching solution
but it also has the ability to function as a standalone,
non-relational or noSQL database.
Now, this is a really important point for the exam.
You will see scenarios where it is asking
for a caching solution and Redis might be the right choice.
Or you might also see a situation where it's asking
for a noSQL database.
And if DynamoDB isn't present,
the next best choice is most likely going to be Redis.
Now because we're using it as a standalone database
or a caching solution, it has to have a little bit
more built into it than Memcached does.
So Redis does have the ability to failover
and provide multi-Availability Zone support
for that high availability deployment.
It supports backups as well.
So if you're required to pick a caching solution
that has backups, Redis is going to be your go-to.
Now traditionally Memcached and Redis
are going to sit in front of a relational database,
something built out in RDS most likely,
but what about that noSQL solution?
What about DynamoDB?
Well, that's where DynamoDB Accelerator or DAX
comes into play.
What DAX does is it really takes that performance
on DynamoDB and cranks it all the way up to 11.
So it is an in-memory caching solution
and it can help you reduce database response times
from milliseconds down to microseconds.
Let's just think about that for a minute.
That is a crazy reduction.
If you would've told me 10 years ago
that we would be talking about microseconds for a response
to a database call, I would have said you're crazy.
But now, well, that's just commonplace when we use DAX.
DAX does live inside of a VPC.
You get to specify where you'd like it to live
and it is going to be highly available.
You have control over DAX.
In fact, maybe even a little bit more control
than DynamoDB itself.
You get to decide the size of the nodes,
how many you're going to have in the cluster,
that time to leave for that database,
your maintenance windows, reminds me a little bit
of some of those settings for RDS,
but it's just for Dynamo DB.
Now this is a lot of caching solutions real quick.
So how do I know which type of cache I want to use? Because
on the exam, we're going to have to make that decision.
Well, it really comes down to DAX is only for DynamoDB.
It is not a source of truth, it's highly available.
It's an in-memory caching solution only for Dynamo.
That's it,
you cannot use DAX in front of another type of database.
That means that ElastiCache
is going to be a little bit more flexible.
This is going to sit in front of RDS,
or if we're using a Redis,
it could offer that database solution on its own.
Now I know we've gone through these concepts rather quickly
and that's by design.
While we could spend hours talking about the individual
nuances of each solution
that's not really going to be required for the exam.
We only have to have that high-level overview
of when do I want to use one versus the other?
Now, speaking of the exam,
let's take a look at our first exam tip here,
put a cache on it.
That's a great piece of advice to keep in mind on the test.
You're going to be given scenarios
and you're going to have to pick the right kind
of caching solution to sit in front of that database.
So to sum it all up, DAX is for DynamoDB,
ElastiCache is generally for those relational databases
with Memcached, just being a cache,
and Redis having the ability to be a cache
as well as a non-relational database by itself.
So a few more tips here for us, always have a cache.
It's just great advice. On the exam,
it makes things go faster, it's better.
You just want to have one.
So pick answers that include a caching solution.
Like I said a few minutes ago,
you want to keep it generally high-level.
It's not going to dive into the nuances
of how do we structure our data?
Or how do I have the correct TTL?
You just need to know from the architecture level
which kind you want to pick
and the fact that it needs to sit in front of that database
and their internal caching solutions.
Now DAX and Memcached are just a cache,
but Redis could be a cache as well as a standalone database.
I've mentioned this point a few times now
because that's really important to keep in mind.
I've seen quite a few questions
asking about using Redis as a standalone database solution.
So it's good to know that it has that support
from multi-Availability Zones.
It has that support for failover.
It has that support for backups,
where the other solutions don't.
And one last tip here for us,
if you're ever trying to build an emergency shelter
don't forget to build a floor.
Most people underestimate how much heat the ground will draw
from your body while you're trying to sleep.
Alright folks,
thanks for going through ElastiCache and DAX with me,
and I can't wait to see you in the next lesson.


Fixing IP Caching with Global Accelerator
==========================================

Hello, Cloud Gurus and welcome back.
In this lesson, we're going to be taking a look
at how can we fix some IP caching issues
that we might run into using
an AWS service known as Global Accelerator.
Now, as always, we're going to have to start off with
what is Global Accelerator
and what does it exactly accelerate?
Why do I want to put this in front of my applications?
We'll take a look at our top 3 features
that we need to know about this particular service.
Then we'll go ahead and jump into the console
to see this hands-on
before we wrap it all up with some of those exam tips.
So let's go ahead and dive right on in.
To answer that first question,
what is Global Accelerator?
Well at a high-level,
Global Accelerator is a networking service
that sits in front of our applications.
It uses that AWS global network infrastructure,
those edge locations,
to effectively increase performance for your application
and help deal with a really sticky problem
known as IP caching.
Now that's great to know from a high-level,
but what does it actually do?
Why do I want to use this?
Well, the problem that we're trying to solve
is that up until now,
our infrastructure has looked kind of like this.
I've got my happy little user go into my load balancer,
and that load balancer then distributes it out
to by suite of EC2 instances,
serving out those great cat photos
that my happy little user wants to see.
Now, what happens is when that user makes that request
to my load balancer,
that user is going to cache that IP address.
Now, as long as that IP address never changes, we're fine.
As we've talked about with AWS,
everything fails all the time,
things are going to go offline.
So when that load balancer eventually goes offline
and then gets replaced
with a new load balancer that has a new IP address,
my happy user, well, it becomes a sad user,
because they've cached that wrong IP address, right?
They're still pointing to the old end point.
Now it doesn't mean my website's offline,
but if my user's not respecting that TTL,
that time to live,
they're going to have an issue.
They're not going to get to see those cat photos.
So how do we solve this?
Well, this is where we're going to take our infrastructure
and put Global Accelerator in front of it.
Now in this situation, Global Accelerator
is going to give me 2 static IP addresses.
That's it.
It does not matter what it's pointing to behind,
it does not matter what Global Accelerator is pointing to,
my happy user is going to stay a happy user
because they get those same 2 IP addresses,
no matter what happens.
So if that load balancer changes that IP
from 4.4.4.4 to 1.2.3.4
or any other address, who cares?
Because Global Accelerator effectively sits
in between those happy users and my infrastructure.
So Global Accelerator can solve
that IP caching problem for me,
and that's an important thing
that you're going to want to know on the exam,
but that's not all that it does.
Global Accelerator allows you to mask complex architecture.
Now, this is great because our environment
is going to grow and contract based on need,
IP addresses are going to come and go,
except for the ones that are provided by Global Accelerator.
Effectively, those become the IP addresses
that I can rely on.
It also speeds things up.
Traffic is routed
through that global network infrastructure.
Select edge locations
all around the globe handle your connections,
meaning your user is not spent traversing
the entirety of the request
going over their standard internet service provided route
to get to your infrastructure.
They make a hop to those local endpoints
and then use that AWS backbone
to get a much faster connection to your application.
You also have the ability to create weighted pools
and define different endpoints.
Global Accelerator, well, it works great on a global scale.
So let's go ahead and see this in practice.
We're going to go ahead and hop into my environment
and set one of these up.
I've got my Global Accelerator console open,
and for those folks following along at home,
I have done some work ahead of this demo.
I have created an EC2 instance in Oregon and in London
and I've set up a load balancer in front of each of them.
They're serving out a pretty simple static website
that we'll go ahead and see here in a few minutes.
So let's jump right in
and click on that Create Accelerator button.
Now here, we're going to go ahead and give it a name.
Lets-go-fast, I think that works.
Scrolling down here.
Now we do have the ability to pick from custom IP addresses
or just grab the ones out of AWS's provided pool.
I'm just going to go ahead and stick with the defaults.
Now I do have to give it a listener,
in this case, my website's just listening on port 80,
maybe I use 443 or a custom range for my application,
in this case 80 will suffice.
I'm going to give it that TCP traffic
and I'm good to go.
Now I'm going to create here what's called a listener.
Now, a listener is going to live in a particular region.
So I'm going to go ahead and just select Oregon
and we'll create one in London in just a few minutes.
Now, if I wanted to,
I could adjust the weight using that traffic dial,
similar to those weighted Route53 endpoints.
We're going to go ahead and leave this at 100 for now.
I'm going to go ahead and click Next.
Alright, I have my one endpoint for now
and we're going to go ahead and click Create Accelerator.
Now this will take a minute or two to update,
so we'll go ahead and check back in
as soon as it's done.
It looks like it just finished.
Now let's go ahead and open this up
and set up those load balancers,
because right now it's actually not pointing anywhere.
So scrolling down here,
I'm going to go ahead and add in a load balancer
to my Oregon end point.
Now, inside of this endpoint group, right?
It's currently not pointing to anything.
Let's go ahead and fix that.
So I'm going to click Add Endpoint.
Let's add one in.
I'm going to click on Application Load Balancer
and I'm going to select the only ALB
that I currently have built.
And if I wanted to, I could set weights
for that weighted pool inside of this endpoint.
So I could have different environments,
maybe I'm testing one to see if I like the current layout
and another to see what would happen if I change it.
I could have different endpoints inside of this listener,
but I'm going to go ahead and just stick with one.
Awesome.
Now all of my traffic is going to be pointed directly
to my Oregon ALB.
Now that I've got this ALB configured,
well, it's only going to Oregon.
So taking a look at my listener here,
I've got my Oregon listener,
but let's add a new endpoint
for that London region as well.
So I'm going to go ahead and scroll down to eu-west-2,
click Next,
and then let's add that endpoint.
I want to add in that ALB that I have built in London
and I'm going to go ahead and click Save.
Okay, so we now have 2 end points:
one in Oregon and one in London.
Now this will actually take a minute.
If we go back up here
to my let's go fast Global Accelerator,
noticing it's in that provisioning status.
It'll take a couple of minutes
to build out the needed architecture.
So I'm going to go ahead and pause this
and we'll check right back in
as soon as this is done.
Looks like that just finished.
Now, if I take this DNS entry and copy it.
Now, since I'm in Seattle,
I'm going to get the Oregon website by default.
Yeah, pretty standard,
this is what I was expecting.
Hello from North America!
Because it's automatically directing users
to the appropriate endpoint
based on their physical location.
Now I'm going to go ahead and switch using my VPN
and route my traffic through the UK.
I've got my traffic cut over here
and I open up a browser.
Yep, it looks like I'm connecting
to my London endpoint there.
Now the point of this is
it's managing that global connection.
No matter where my users are coming from,
they're going to be routed to the appropriate endpoint
and I have full control over those weights
and where my users need to go.
Let's go ahead and hop back over to our slides
and we'll go through some of our exam tips.
My first exam tip is that Global Accelerator
is going to solve our IP caching problems.
Keep that in mind on the exam.
If you're looking at a scenario that talks about IP caching,
you automatically want to think Global Accelerator.
Those 2 static IP addresses are going to solve that issue
of what happens when the underlying IP changes,
what happens when that architecture rotates out
for one reason or another.
Now, a few more tips here for us.
It speeds it up.
I don't really have a good way
to show you this in my quick demo,
so you'll just have to take my word for it.
Using that global network of infrastructure
that AWS has deployed,
it's going to make my connections speedier.
We can set up weights between our different endpoints.
If I want to have different weights
between different countries, I can.
If I want to have weights between different regions, I can.
If I want to have different weights
inside of that region, I can.
Effectively, I have the dial
and I can move that as much as I need to.
It's important to keep in mind
that we get those 2 static IP addresses
with Global Accelerator, and they're not going to change.
They're not going to rotate.
Now, my last tip here.
If you were headed out to the mountains,
make sure to bring your sunglasses.
Sun reflecting off the snow
can easily cause snow blindness,
effectively, it's going to burn your eyes
and nobody wants that.
Alright, folks,
thanks for going through Global Accelerator with me
and I can't wait to see in the next lesson.


Caching Exam Tips
===================

Hello, Cloud Gurus, and welcome back.
In this lesson, we're going to be taking a look
at some exam tips for those caching services
that we just finished talking about.
So let's go ahead and dive right on in.
Now as always, we have those 4 questions
that we want to ask ourselves
with every question that we see on the exam.
The first one is can it be cached?
Is there a situation in the application scenario
in which I can cache my data?
Is there a way that I can use caching
to speed up performance of my application to reduce my costs
or to even just deal with a technical problem,
such as users seeing a stale version of my application
because of a failure?
We always want to be thinking,
can a cache benefit this architecture?
If it can, what kind of cache should we put there?
We've covered quite a few so far in the previous lessons.
And hopefully at this point,
you should have an understanding
of where do they sit inside of my architecture.
So the question is asking for
how do we cache a database solution?
We shouldn't be putting CloudFront there.
We should be thinking Redis, Memcached, Dax
depending on the type of database that we're going to see.
If it's talking about how do we distribute our content
out to our users faster.
And then we should be thinking CloudFront
as that's the natural solution that should be used there.
In the cache, how does the content
in your cache get updated?
Well, usually it's by setting those TTLs correctly, right,
those time to live settings.
This is simply going to dictate
how long will the data live in that cache?
And we need to have that approach of not too hot,
not too cold, just right for our data.
It's important to remember with CloudFront,
how do we get our content out of the cache,
whether it's through that purge,
that expiration of our data,
or just simply letting it expire,
waiting for that TTL to pass.
Does that caching solution help with anything besides speed?
That's kind of an interesting question to think about
because normally we do associate caches equal speed,
but with things like CloudFront,
we can attach that web application firewall to add in
that additional layer of security at those edge locations,
closer to the end users.
So it helps with security and it helps with speed.
With Global Accelerator, it helps with speed,
but it also helps deal with that technical problem
of I have an issue with those IP addresses being cached.
So caching is not always just for speed.
We have to think, does it add additional benefits?
Now let's take a look at some exam tips for CloudFront,
as well as, Global Accelerator.
CloudFront is the only option to add HTTPS, right,
that secure connection to a static website
that is being hosted in an S3 bucket.
I have seen this as an exam question, right,
We want to add security to that static website.
It's not supported by only using S3.
We have to front that S3 website
with a CloudFront distribution.
And CloudFront, as we recall uses HTTPS natively, right,
that is built right in.
When given a situation that compares
one solution to the other.
If one has a cache and the other doesn't,
we want to focus on the one that includes a caching option.
Now it's generally not going to be diving in too deep
on caching solutions, we just want to know, okay,
is there an appropriate cache in front of the part
of the architecture where it should be?
Are we using the correct kind of cache
in front of the database?
The correct kind of cache in front of my web application?
On the exam, it doesn't really dive too deeply
into the fact that it might cost a bit more money
versus not having it, if it's not configured correctly.
It's generally considered it as a high level solution,
and it only paints those caches in a positive light.
On the exam, if you see a scenario
that's talking about IP caching,
you should immediately be thinking Global Accelerator.
As we talked about in our previous lessons,
Global Accelerator gives you those 2 static IP addresses
that way customers can cache those IP addresses,
and when the architecture behind them rotates
such as the load balancer changing out,
or you're failing from one region to another,
it doesn't really matter for your end user.
They still hit those 2 Global Accelerator IP addresses,
and everything's good.
Now while Global Accelerator can help with speed
and waiting and there's additional benefits to using it,
the example generally focus on using it
to solve that problem of IP caching.
Well, that covers getting data into our environment.
Oh, let's take a look at those database caches.
What do we need to know for that exam?
If we see the words in memory database,
we have 2 options, Redis and DynamoDB.
These are both no SQL solutions and they can be used
when it doesn't make sense to have that relational database
when we want to focus on speed.
So Redis is both a cache and a standalone database
if you want it to be.
Now, given a situation where you're comparing the two,
pick DynamoDB.
I'm sure in the real world,
you can come up with situations
where Redis is a better answer compared to DynamoDB,
but on the AWS exam, they tend to favor answers
that are using the more managed AWS tools
rather than a managed open source tool,
like ElastiCache, and Redis.
On the exam. if you see the terminology in memory database,
you should be immediately thinking
either Redis or DynamoDB.
Now Redis, could be a standalone database.
It could also be used as a cache where DynamoDB
is going to be used in general,
just as that database solution.
Now, if given the option to pick between the 2
generally favored DynamoDB, I wouldn't expect something
like that to come up too often though.
You're not going to be asked to take a technical deep dive
of the differences between the 2 types,
but we need to know that both
are in memory database solutions and provide flexibility
as they are non-relational databases and speed.
That's where they both excel.
Now when comparing the 2 versions of ElastiCache,
either Redis or Memcached, we need to know that Redis,
has more features.
It can be used as that persistent data storage.
It can be used as a cache.
It has the support for backups.
It's just a bit more robust,
where Memcached at the end of the day is just a cache.
Now that means that you should not be using
a caching solution as the source of truth for your data.
The only caching or slash database solution
that supports backups is Redis.
Memcached and Dax do not support backups
and are not a source of truth for your data.
They are simply there to help with performance.
So you need to ensure that you're picking answers,
that stores that data in a permanent data storage solution,
such as RDS or DynamoDB.
Alright, folks.
Thanks for going through these caching exam tips with me.
I can't wait to see you in the next video.



Governance
===========

Managing Accounts with Oraganizations
=====================================

Hello, Cloud Gurus, and welcome back.
In this lesson, we're going to take a look
at managing multiple AWS accounts
using a service known as Organizations.
So we have to start this off with what is Organizations,
and why do I want to have multiple AWS accounts?
We'll see some of the key features that we get
when we start to manage our accounts using this tool.
And then we'll talk about, in my opinion,
maybe one of the most important ones:
service control policies, also known as SCPs.
We'll dive into the console to see an example
of these service control policies,
and then we'll wrap it all up
with some of those great exam tips.
So let's go ahead and dive right on in.
So what is Organizations?
Well, Organizations at a high level
is a free governance tool.
Basically what it allows us to do
is create multiple AWS accounts
and then apply standards to those AWS accounts.
We can start to control what's going on
in that prod account, in that dev account,
in that beta account.
Now it's a good idea
to start separating out our architecture
so we can limit the potential blast radius
of any problems that we might run into.
If we separate out accounts,
nothing that happens in one account
can ever affect another account,
and Organizations just allows us a framework
to basically keep everything in check.
So what exactly are we going to use this for?
Well, we have some key features,
some key things that we can look for
when we start building out multiple AWS accounts.
The first--and this is a really important thing
to know for the exam--
it is vital to set up a logging account.
This is generally an AWS account that has some S3 buckets
whose sole purpose in life is just to collect logs.
CloudTrail can actually take all of its logs,
those API calls,
and ship it to this one centralized location,
and this is very easy to set up using Organizations.
You're going to see
that a lot of these tools that we're going to talk about
in upcoming lessons have integration with Organizations
and CloudTrail is one of them.
You can easily create and destroy AWS accounts
using the Organizations API.
Now this means you don't have
to go through that manual wizard startup process
every single time that new developer comes on board
and needs their own AWS account.
You can effectively create that vending machine
where you just press that button,
and you get that product served out to you.
You get that AWS account,
and it's still bound your organization.
It allows the developer freedom,
while still being controlled.
You can combine and share reserved instances
across all of your accounts.
Now this means if you have one account
where you bought RIs in,
and another account has an C2 instance
that matches that reservation,
it can get that billing discount.
So there's a monetary incentive
for setting up Organizations.
Speaking of money,
you also have one primary place to pay your bill.
So you don't have to go around with a credit card
every single month to all of those different accounts.
That primary main account is going to be the one
that you pay the bill from,
and you can easily see which account
is spending how much money
when you go to checkout.
The last and maybe the most important reason
to use Organizations is called service control policies.
These can set limits
on what users can do inside of your accounts.
Let's actually take a look at one of these policies.
Now I know at first glance,
you're looking at this policy and you're thinking,
Alex this is just an IAM policy document,
and you're right.
It's using the same JSON structure,
that same effect action resource
that we learned about so many lessons ago.
But in this case,
when applied as a service control policy,
it has the final say.
So let's take a look at it here.
This just says that I am denying the ability
to stop an EC2 instance or terminate an EC2 instance.
That's pretty powerful.
I can't shut anything down.
I can't throw anything away.
This applies effectively
as a global policy for that account,
meaning, no matter what the user permissions are
this would override it.
This is even applied to the root account.
This is in fact the only way
that we can restrict what the root account is able to do.
Now, it's one thing
to talk about these service control policies.
It's another to go hands-on with them.
Let's go ahead and jump into my account
and see how powerful these rules can be.
Alright, folks,
so I've got my AWS Organization service up.
Now, before this demo, if you are following along at home,
I did go ahead and create 2 service control policies
that we're going to see in a minute,
and I've already set up a new account
that I'll be signing into momentarily.
Now, as we talked about service control policies are one
of the most important features of Organizations.
So let's take a look at 2 service control policies
that I have attached to my demo account.
Let's go ahead and open that up here.
Now we actually just saw this service control policy
a minute ago.
It's just saying I am not allowed to stop instances
or terminate instances.
That's it. Right?
Deny statements are pretty simple.
They just mean across the board,
nobody can make those particular API calls.
Let's take a quick step back,
and let's see this other policy I have
called OnlyS3andEC2.
Now, an allow statement,
allows statements are not quite what you think they are
with service control policies.
An allows statement just says
you now only have the potential to make calls,
in this case to S3 or EC2.
It doesn't actually give you the permissions to do so.
It just narrows down every possible service
that's not listed in here and throws it out the window.
So service control policies will never give permissions;
they only take permissions away.
That's really important to remember.
So I'll say it again.
They never give you permissions,
they never create an allow that lets you do something,
they only take it away.
So it was kind of opposite.
Where a deny statement is only limiting particular calls,
where an allowed statement,
that's actually very restrictive.
Now let's actually see this in practice
because I know it's confusing
when we first walk through it.
So I'm going to take a look at my AWS accounts.
And here I've created a demo organizational unit,
an OU is just a, well, group for accounts
to make things easy.
So when I open up this account here,
scrolling down a little bit.
If I take a look at my policies,
here I see I've only attacked directly to this account
that S3 EC2 and NoEC2SStopTerminate.
Now let's flip over to my new account
that I created, this demo account, to see this in action.
I've got my demo console open here
and if we take a look at the account,
well I'm actually signed in as the root user.
So normally I'd be able to do anything,
except we have those pesky SCPs applied.
So let's go ahead and open up the EC2 service
to see this in action.
And right off the bat we're already seeing some errors
that normally we wouldn't expect to have as that root user.
I already have an EC2 instance that I've spun up
and let's go ahead and terminate that host.
Terminate, throw it away.
And we can't.
Big red API error.
Because that service control policy says
I have no permissions,
I cannot terminate an instance.
Now it did say that I can do things in S3.
So, let's test that out.
So I'm going to go to the S3 service.
Now not signed in as the root user,
I would also have to have a policy document attached
to my user account to give me permissions,
because remember SCPs don't give permissions,
they only take it away.
I think that's a reasonable name for my bucket.
I'm going to go ahead and click Create.
Now this only worked once again
because I am the root account.
And this was included in that allow statement
provided by that SCP.
Let's take a look at one other service.
Take a look at DynamoDB.
Let's go ahead and create a table.
Pretty simple task I should be able to do as that root user.
If I can spell correctly.
Just something quick.
And it doesn't work right?
Because it is not included
in the scope of that service control policy.
So these SCPs are very powerful and you need to know
that they are a number one way to restrict permissions.
Now it's not the only reason to use Organizations
but it's a big one.
So let's go ahead and flip back to the slides
and we'll take a look at some of those exam tips.
For my first exam tip,
you're going to be given scenarios
about wanting to ensure that logs are centralized
into one account and no one can edit or delete those logs.
You can probably guess where I'm going with this.
The answer that you're going to want to pick
is going to be an answer that includes Organizations
to centralize your logs,
and then applying service control policies
to restrict anyone from editing or changing that content.
SCPs are the best way to ensure that users
don't break the guardrails that you have set up
for those accounts that they're working in.
Now a few more tips here for us.
When you need to have that final say
as to what happens in your AWS accounts,
service control policies
are the most powerful way to do this.
Because as we saw, it even restricts that root account.
Organizations helps with that billing management,
because those bills roll up to that primary accounts.
So you don't have to be hunting around
with all of your different bills,
you just get one single one at the end of the month.
It's important to know that a lot of AWS services
have integration with Organizations,
meaning it's very simple.
Just click a few buttons
to set up that common logging account,
and have all of your CloudTrail logs
from your other accounts placed
into that centralized location.
RIs are shared across all of your accounts.
That's why you generally want to have
well your main account, just handle the bill,
maybe even hold the RIs, and that's kind of about it.
If you'd like to turn this off, you can,
and that might be a good fact know for the exam,
but I can't really think of a real world situation
where you'd actually want to do that.
But it's a good fact to know for the test.
And my last tip here for us,
veterinarians recommend brushing your dog's teeth
every single day.
This can help prevent plaque buildup
and gum disease in your canine friend.
All right, folks,
thanks for going through Organizations with me
and I can't wait to see you in the next lesson.


Sharing resource with AWS RAM
=============================

Hello Cloud Gurus, and welcome back.
In this lesson, we're going to take a look
at sharing our AWS resources
using the Resource Access Manager.
So of course, we have to start off with what is RAM
and why do we want to use it?
We'll take a look at some of the different kinds
of resources that we can share using RAM,
and then I'm going to walk you through a console demo
where we're going to share part of my VPC
into another AWS account.
And once that's done, we'll circle back
for some of those handy exam tips.
So let's go ahead and dive right on in.
So the Resource Access Manager
is a free service that allows you to share resources
with other AWS accounts.
Specifically, we're going to focus here
on other accounts that are in your organization.
This means that we can easily say,
hey, you know what?
I'd like to share this networking space
with all of my org, or part of my org,
or just a single account.
And there's no complicated networking that I have to do.
Not that VPC period is particularly complicated,
but trust me, when we walk through that demo,
it's literally just a few button clicks
and a copy paste away,
and you're sharing out your networks or other AWS resources.
Yep.
Speaking of other resources, what exactly can we share?
Well, we can share transit gateways,
meaning we don't have to go through
and set one up in every single account.
We can share VPC subnets,
so we don't have to rebuild in every single account.
We can share licenses
so we don't have to duplicate these.
You should be sensing a theme here.
The entire purpose of the Resource Access Manager
is deduplication.
Why would I want to continually copy paste
and spend extra money on architecture
in multiple environments
when I can have just one location
for that architecture and then spread out access?
We can share Route 53 Resolver, Dedicated Hosts,
honestly a lot more.
You don't have to memorize all of these for the exam.
If you do see an exam question,
it's probably going to focus on those VPC subnets.
Now, speaking of VPC subnets,
let's actually just hop into my account
to see how easily this can be done.
Alright, so I've got my Resource Access Manager
console pulled up here.
Now for those folks who'd like to follow along at home,
I did do a little bit of work before I started this demo.
I used Organizations to create a second AWS account,
and that's really about it.
So let's go through the process of now sharing a subnet
from this account into that secondary one.
So I'm going to go up here and click Create Resource Share,
and I'm just going to give it a test name.
I think that's appropriate here.
Now, scrolling down,
We can see all of the different resource types
that we can share.
Now as we can tell, there is a lot that are available here.
There's a lot of things that we can share out
between our different accounts.
We're not going to go through all of them
because they're not particularly relevant for the exam,
what you're really going to want to focus on are the
subnets. So now I only have a single subnet
that I'm going to share out or that I can share out.
Please do note, you're not able to share that default VPC.
So I've selected my subnet here,
I'm going to go to my next page.
Now I can give particular permissions.
I can decide what's allowed to happen
inside of this subnet.
For the time being, I'm just going to go
with this stock permissions of creating EC2 Instances,
describing them, all that fun jazz.
Next, I can select, do I want to share
with a particular AWS account
that's in or out of the organization?
Or I can select just to share with only the accounts
in my org.
In my case, this is where I'm going to place
that AWS account number.
I'm going to go ahead and click Add,
and go to the next page.
Here, I just get my quick summary
of everything that's going on,
and I click Create.
Now, you can't tell me that peering VPCs together
is easier than this.
RAM is about as simple as it gets
to share architecture cross account.
So it does say that it is currently going through
this associating process.
Let's go ahead and refresh the page
and it should finish in just a second.
And looks like it's up.
So my next step is I'm going to switch over
into my other AWS account.
So let's go ahead and grab that role,
and I'm in to the second AWS account that I have.
Now real quick, we can take a look at these resources
in our VPC console.
So I'm going to pull that up,
and what I'm going to show you here
is that I have a new VPC that just appeared for me,
this ten. space, that subnet that I shared out.
Now how do I know whether this is mine or not?
If I scroll down here,
it actually shows that this is a shared resource.
This is not my account ID.
So let's go ahead and start spinning up an instance
inside of this new VPC.
So I'm going to go over to my EC2 console,
and I'm just going to quickly run through
creating a new instance.
So you should be very familiar
with these steps at this point.
Just going to go with that nice free t2.micro.
Now here, I would like to select
that I'm going to be using once again,
this shared network.
So it's in there,
and yeah, that's pretty much all I have to do.
Going to skip the storage page,
I'll give it a nice name.
Security groups here, for
simplicity, I'm just going to say all traffic from anywhere.
Once again, not a good idea
in an actual production setting,
but for testing right now, it's fine.
I'm going to launch, and I don't need a key pair,
because we're not actually going to connect to this host.
We're going to launch this instance.
Now, this will take a second to come online,
so I'm going to go ahead, pause the video,
and we'll check back in as soon as it's done.
And it looks like it just finished.
So we're quickly going to check this instance.
And what I want to grab right here
is the private IP address.
So I'm going to go ahead and write that down
just so I have something to connect to
for my other EC2 Instance.
And I'm going to switch back to my primary account.
Now notice when I clear this search page,
I only have access to that bastion host
in my primary account.
I'm not seeing that shared EC2 Instance.
So I still have separation of my architecture
in the distinct AWS accounts,
but they're actually living inside of the same subnet.
So let's go ahead and connect to this instance right here
and I'll try pinging that other EC2 Instance
that we just built.
Alright, so I did just connect to that bastion host,
and let's go ahead and ping that new EC2 Instance
in the other account.
I'm going to go ahead and paste in that IP address,
and yeah, looks like it's connecting.
This allows me to not have to duplicate my resources.
As we can see, it's very simple
to start sharing networking space
along with a lot of other AWS architecture.
So let's go ahead and flip back to our slides,
and we'll take a look at some of those exam tips.
So after walking through that,
you're probably asking yourself,
Alex, when do I use RAM versus VPC peering?
That's a question that you might see on the exam.
If you're asked to pick between the two,
in general, if you're sharing resources
within the same region,
because of the ease of use, select RAM.
If you need to share resources across regions,
select VPC peering.
Now that doesn't mean that if you only see VPC peering
on the exam, that it's wrong, it's still a great option.
And in fact, you'll probably see more questions
around VPC peering than you will see around RAM,
although it is good to know that RAM exists,
in general, it's a little bit easier to use.
So let's see a few more tips here.
Remember, when you share out your resources, you save money.
You don't have to duplicate that architecture.
Now, VPC period excels when you're connecting two,
maybe fully built separated networks.
Maybe you've got a client that you're working with
who already has an architecture pattern
built out in another region,
and you want to connect your VPC to that VPC,
we'd probably go with peering.
Generally, I think of using RAM as intra-organization.
Now paying for this, well RAM is free,
but the user is going to pay for the architecture created.
So if I make an EC2 instance in somebody else's network,
I'm still going to pay for it.
RAM easily allows you to share out your architecture
inside of an organization.
You can even just check a checkbox
and say share this resource with everything
inside of my organization.
That makes it very simple.
If you have a license, if you have a networking space,
if you have a dedicated host,
if you have something that effectively you want to broadcast
to the rest of your org.
And finally, if you sign up for an online account,
use the company's name as your middle name.
That way you can easily tell who's selling your information.
Those spam emails will start to look
just a little bit funnier.
Alright, folks, thanks for going through RAM with me,
and I can't wait to see you in the next lesson.


Setting up Cross-Account Role Access
=====================================

Hello, Cloud Gurus, and welcome back.
In this lesson, we're going to take a look at
Setting up Cross-Account Role Access.
We'll start off by taking a look at
why do we care about this?
Why does cross-account role access matter?
Then we'll quickly walk through what are the steps
that we need to take to actually set this up.
And then of course, I'm going to show you how to do this
inside of my AWS account
before following up with some of those exam tips.
So let's go ahead and dive right on it.
So that cross-account role access.
Now, cross-account role access is a very important topic
for the exam, and it's a great skill to have
in the real world when you start managing
multiple AWS accounts.
This stops you from having to duplicate account credentials
over and over and over again.
So you could set up a primary AWS account
to manage your sign-ins
and then from there have users just assume roles.
I assume a role to hop into the production account
with very bare bones credentials.
I assume a role to hop into the dev account,
where I can do most things.
That way I don't have to have
multiple sets of credentials out there
because that would create a horrible security vulnerability.
So what steps do I actually have to take to set this up?
Well, I have to go through the process
of creating an IAM role.
I have to say, yep.
This is the role that I would like to be assumed
at some point.
I have to give it a set of permissions.
What do I want that user to be able to do?
From here, I can grant access to that role.
I can say, ep.
This particular user, this account ID,
this individual can assume this role.
Then the user just makes the call to do it, right?
They put on that hard hat.
They temporarily assume those sets of credentials
to be able to work in, say, that production AWS account.
And by default, out of the box,
nobody else can assume this role besides the ones
that I have purposefully allowed.
Now that's great to see in theory,
but let's actually walk through this in practice.
Alright.
So I've got my IAM console open.
Now for those folks following along at home,
all you're going to need is 2 AWS accounts.
My first step here is I'm going to create a new role.
Now, AWS knows what's going on.
They have a pretty quick walkthrough for this one.
I'm going to go ahead and select just Another AWS account.
Now from here, I'm going to paste in my account ID.
Now, there are some restrictions that you can add in,
multi-factor authentication, external IDs.
For now, we're just going to allow anybody
that has permissions to assume roles
in my other AWS account that's listed here
to assume this one.
Obviously, you can get more restrictive if you need to,
but that's going to be a bit outside of the scope
of what we need to know for this exam.
Next I'm coming to permissions and you know what?
I'm going to give my user S3 access.
And for simplicity, I'm just going to say S3 full access.
Next, I'll give it a nice tag.
Just letting me know that it's my role that I created.
Give it that final review.
I'm going to call this my cross-account-role.
Pretty good name if I do say so myself.
And I'm going to click Create.
Now, that'll take just a second here.
Alright.
All the hard work is done.
Now I'm going to switch over to my other AWS account.
So I'm in my secondary AWS account here
and in order to switch into that role that I just created,
all I need to do is go up, click on Switch Roles.
And I just need to paste the account ID
from my first account in here, along with the role name.
Click on Switch Role.
And that's literally it.
I am now in that first AWS account
that I had created that role in.
So just to test this out, let's open up my S3 service here.
And I can see all of those fun S3 buckets that I've created.
Now, just to show you
that I do have those limited permissions,
let's go to Services and I'm going to open up EC2.
Notice I get a lot of errors
because I don't have permissions to make these API calls.
I'm not authorized to perform this operation.
Let's see if I could expand my permissions via IAM.
I'll open that up here.
And I don't--I can't--I only have those S3 permissions
that were given to me when this role was created.
Alright, folks, that's going to be it for this demo.
Let's go ahead and hop back to the slides
and review some of those exam tips.
Alright.
Well, as we saw, roles are your friend.
If you are seeing a question on the exam
that talks about credentials, that talks about security,
honestly, just every single answer that you see,
always look for roles.
Roles, roles, roles.
It is preferred to create cross-account roles,
rather using additional IAM credentials.
Anytime that you're talking about security,
automatically think about roles.
So a few more tips here for us.
I wish I'd just had a giant slide
that said use roles everywhere.
This is the single most important piece of advice
for the exam.
I'm not saying you only have to know about roles
to be successful on the test, but if you do,
there will be a lot of free points
that you'll be able to collect.
Anytime you see a question talking about auditing,
temporary credentials, temporary access, create a role.
Do not give the auditor full, permanent, complete access.
Because if you do, you're going to miss that question.
Now, this is another service
where you don't actually have to know the specifics.
You're not going to be asked about
creating that trust relationship.
You just need to know in general,
that you can set up cross-account roles.
Now, I'd still recommend creating roles and practicing this
in your own AWS accounts, so you have those skills
when you get out into the real world
and actually have to start creating these.
Now, remember roles are temporary.
You can never permanently assume a role.
Temporary access is always more secure
than permanent access.
And finally, if you own a house,
poke your head in the crawl space once a month.
It's a great way to catch leaks or other problems
before they escalate.
I know that saved me
from quite a few very expensive problems.
Alright, folks.
Thanks for going through cross-account roles with me,
and I can't wait to see you in the next lesson.


Inventory Management with AWS config
======================================

Hello, Cloud Gurus and welcome back. In this
lesson, we're going to be taking a look at AWS Config.
So of course, we have to start off with what is Config,
why do I want to use it,
and I guess, what does it Configure?
We'll see how Config can help us with the past history
of our architecture, as well as taking a look
at the current state of our resources.
Then we're going to jump into the console
to see how it works inside of my account,
before we wrap it all up with some of those exam tips.
So let's go ahead and dive right on it.
So what is Config?
Well, at a high level,
Config is our inventory management and control tool.
Effectively, what it can tell us,
is what do I have inside of my AWS account?
What's all that inventory looking like?
What is the state of all of my resources?
And it can then notify us
and even sometimes automatically fix
when that state of my architecture changes
to be something that I don't want.
So there are 3 big things that Config brings
to the table.
The very first one is, it allows us to query our resources.
I don't know about you,
but I have definitely lost things inside of my AWS account.
How many instances do I have?
I want to search based on this tag.
It looks like something was deleted, but what was it?
We can use Config to answer all of those questions.
We can easily discover what do I have inside
of my environment.
We can also create rules to enforce what's happening.
For example, maybe my security team has so rightly concluded
that public S3 buckets inside of my AWS account
are a bad idea.
I could create a rule to automatically kick
off when an S3 bucket goes out of compliance
when that bucket becomes public when it shouldn't be.
Config also can tell me the history of my environment.
This has been so much help when it comes to troubleshooting.
I've gotten that page at 3 o'clock
in the morning that says that database is offline.
Well, we can easily hop into Config and see what happened.
We can see when was that call made,
when did that database change,
and it can help me track down that issue
so I can solve that ticket and go back to bed.
Now, that's all great to see in theory,
but let's actually hop in and in practice
get a feel for this inside of my AWS account.
So I've got my Config console open here,
and now don't panic.
I know at first glance it can be a little bit overwhelming
with everything going on,
but let's break this down step by step.
So let's take a look first at our resource inventory
because that's what we have initially talked about.
So I'm going to go over here on my left-hand side,
and open up my resources.
Now, what this is going to do,
is it's going to list every single possible resource
that I have inside of my AWS account.
Now I can query against all of this,
I can include deleted resources in my view,
I've got all that good stuff.
But, for the sake of simplicity here,
let's just go ahead and open up.
We'll just pick this EC2 instance right here.
Now I do get a quick overview of what's going on.
And if I want to click the Manage button here,
it'll actually dump me right into the console
and I can start messing around with it.
We can see that this instance is currently off.
Now, what I probably care the most about,
if I click on this resource timeline,
this is going to give me a history
of everything that happened in, on,
and around that resource.
So let's take a look here. Now
I'm just going to randomly grab one of these events here.
Well, it looks like that on March 18th,
my user created a tag for this EC2 instance.
And if I want to, I can cross reference this in CloudTrail.
This is one of the really cool features
where I can simply open up the exact CloudTrail event
that's related to that tag being created
or that instance going offline.
I can take a look here at a Configuration change.
Well, that's a lot of JSON, right?
But it gives me the difference between the two.
So it looks like there was a change
in that public IP address.
I got a new public DNS entry.
Now let's take a look at another resource here.
So I'm going to go back
to my resource section and let's search
for an S3 bucket that I've created.
Horses.design.
Well, let's open this resource up.
Let's go to that resource timeline.
And I can see a few things in here.
Namely, the big one that I'm looking for is this resource
is not compliant with 7 Config rules
that I have created.
This means, that I've created rules to watch the state
of my architecture,
and this S3 bucket isn't compliant with a lot of them,
namely 7 out of 9.
So let's take a look at one of them here.
S3-bucket-public-read-prohibited.
This is a pretty stock check,
I haven't built this myself,
I just went in and said, yeah, I'd like to use this rule.
So, over that 24 hour window,
it's going to scan all of my S3 buckets,
and it's going to look for S3 buckets that allow
that read access
because that read is not prohibited in the bucket settings.
So that's an issue here.
And if we scroll down,
I can actually see that my horses.design bucket
is listed as a non-compliant resource.
So I want to fix this.
Because just telling me that something
is happening is great,
but I'd like to automatically remediate this problem.
Now, the awesome thing for Config,
is that this is actually built right in.
I can change the remediation action.
So in this case here,
I have it actually set to kick off an automation document.
Remember those ones that we talked
about from Systems Manager?
Yeah, it will run this and block public access.
Now I can have this set to happen automatically
and that's called automatic remediation,
but just for simplicity sake here for the demo,
I'm going to manually kick this off.
So let's go ahead and just verify this real quick.
This is my horses.design bucket.
If I scroll down,
notice that block public access is currently turned off
for ACLs and bucket policies.
Let's go ahead and fix that.
I'm going to switch back to Config here.
I'm going to select this bucket
and I'm going to click Remediate,
and that is literally it.
Behind the scenes,
it's kicking off that automation document,
and when I flip back to the console here,
it'll have changed those permissions.
So let's go ahead and take a look at my bucket itself,
then I refresh the page.
Notice, that changed these two settings here,
and now if I wanted to,
I could craft my own automation document.
Maybe I want to block all access
no matter what, this one is just configured
to block the bucket policies and the ACLs.
Alright, well, this has been a whirlwind tour of Config.
I'd strongly recommend you jump in and play with Config
just a little bit yourself.
We've covered all of the important points
that you need to be aware of,
but it's always worth getting
some hands on exercise yourself.
Just please be aware Config is not a free service,
so be mindful of that small additional charge
to your AWS account.
Alright, let's go ahead and hop back
into my slides and finish it out
with some of those exam tips.
For my first tip here, just remember,
if you're on the exam
and it's talking about enforcing standards, think Config.
As we saw in that demo, we can set up those rules
and set up automatic remediation to fix those problems.
So if you need to make sure
that your buckets aren't publicly available,
that you're only using approved AMIs,
that all of your EBS volumes are encrypted,
those are just a few of the many rules that Config offers.
You also have the ability to create your own custom rule
if you have a specific need
that's not met by the standard checks.
So a few more tips here for us.
Config is the best way to check those standards,
otherwise you'd have to jump into the console
and click all those buttons yourself
and nobody wants to do that.
It's important to know
that you can track deleted resources using Config.
So just because that instance has been terminated,
doesn't mean we can't hop in and see that Configuration
timeline for that architecture.
Enforcement is maybe one of the biggest reasons
to use Config.
As we saw, you can easily use automation documents
or you can also set up Lambda functions
to enforce those standards.
Please be aware that both of these are options
on the exam because I've seen both types
of scenarios pop up on the test.
If you'd like to, you can actually consolidate
all of your findings, all of your rules,
into one single region.
This means you don't have to go region by region
using Config, and it's just a nice quality of life change.
And for my last tip here,
if you're cooking popcorn and you managed to burn it,
you can use Bar Keepers Friend to remove that charcoal
from the pot.
Trust me on this one.
All right, folks.
Thanks for taking a look at Config with me,
and I can't wait to see you in the next video.


Offboarding Active Directory to Directory Service
==================================================

Hello Cloud Gurus, and welcome back. In this lesson,
we're going to take a look
at how do we deal with active directory inside of AWS.
Now that means we're going to have to define
what is the Directory Service,
what are the different types of Directory Service
that we're going to encounter, and when do we use each one?
And then we're going to go ahead
and wrap this all up with some exam tips.
So those will be a little bit of a shorter lesson.
So let's go ahead and dive right on in.
So what is the Directory Service?
Well, AWS Directory Service,
it's a fully managed version of active directory.
So if you've ever worked with active directory before,
you've basically worked with the Directory Service.
Effectively, what it allows you to do
is run AD inside of AWS
without having to do all that heavy lifting setup
that normally goes with running active directory on-prem.
Now that's great to know at a high level,
but we need to take a look
at the different kinds of Directory Service
that we might see.
So, each type that we're going to talk about here
has a particular use case
that you're going to need to be aware of.
The first is called Managed Microsoft AD.
You can probably guess by the name,
this is an entire active directory suite.
It's managed, it's built by AWS,
it gives you all of the AD tools that you know and love,
but without the heavy lifting
of having to set it up yourself.
So this is great if you're building out
a complete active directory suite inside of AWS,
and you don't want to do it anywhere else,
you don't want to have it on-prem.
Now, a lot of times--especially in the exam--
you're going to see situations presented to you
where maybe you want to leave AD in a physical data center
in the physical office,
but you need to have something inside of the cloud,
something in AWS.
This is where AD Connector comes into play.
And what it does, is it's going to create a tunnel
between your AWS environment and your on-prem AD.
Now that basically means that you get an endpoint
that you can authenticate against inside of AWS
while leaving all of your actual users in data on-prem.
And the third type it's called Simple AD.
Now has the name makes it sound, this one's pretty simple.
It doesn't have all of the standard AD architecture
that you love, but it's got the basics.
It's effectively just an authentication service
that's up and running inside of AWS
and it's actually powered
by Linux Samba Active Directory compatible server.
That's great to know.
The exam itself is going to focus primarily
on Managed Microsoft AD and AD Connector,
but it can't hurt to know
that there is that third type of Simple AD
if you don't want all of the bells and whistles
that you normally get from the full suite.
So this lesson was rather quick.
We just need to know the Directory Service
at a very high level.
And in particular, the use cases
behind the Managed Microsoft AD and AD connector.
You're going to have to understand that Managed Microsoft AD
is for when we want to migrate everything into AWS.
And AD Connector is for when we want to leave that on-prem
or somewhere else
and don't want to move that into the cloud just yet.
You don't have to know all of the steps
around group policies and password expirations,
that's an entire other sort of exam.
And that knowledge is not going to be particularly useful
on this test.
So a few more tips here for us before we call it quits.
Know the types;
you have to know manage Microsoft AD, AD Connector
and Simple AD and know what they each do.
It is a fully managed service.
So when possible, use the Directory Service
over just running AD on an EC2 instance,
and remember, the exam always favors using managed services
over unmanaged services.
It's okay to leave AD on-prem, it happens.
Some of the scenarios you might see,
might talk about a customer not being comfortable,
moving their user accounts into the cloud just yet
and that's okay.
That's where the AD Connector comes into play.
So it's one of the few tools on the exam
that you might leave during that lift and shift
sort of question.
And my final tip here for us,
in order to prevent car door dings,
make sure you park very far away
from anyone else in the parking lot.
In fact, sometimes I actually just walk to where I'm going
and leave my car parked at home.
Alright folks, thanks for taking that quick look
at the Directory Service with me
and I can't wait to see you in our next lesson.


Exploring with Cost Explorer
==============================

Hello Cloud Gurus and welcome back.
In this lesson we're going to put on
our expedition hats and go exploring
with Cost Explorer.
Before we explore the service itself,
we have to answer a fundamental question and that is,
why do we budget?
Once we've solved that, we can take a look at Cost Explorer,
and then we're going to see some features
that Cost Explorer allows us to use
to better track down where our money goes.
After that you're going to get to see
how I'm wasting all of A Cloud Guru's money--
I mean how I am diligently spending all
of A Cloud Guru's money inside of my AWS account.
And then we'll circle back for some exam tips.
So let's go ahead and dive right on in.
Now our very first question, why do we budget?
Well, the answer should be pretty obvious, money.
If we don't know where that money is going,
we are burning cash.
You are going to get yelled at by your boss, trust me,
I have been there, that's not a fun place to be.
Now, thankfully for us,
AWS gives us a free and easy to use tool
to find out where is that money going?
And that's where Cost Explorer comes into play.
Now the official definition is that it's
"an easy to use tool that allows you
"to visualize your cloud costs",
and that's actually just a great summary for it.
What we can do is create reports
to see where is my money going?
Where is it being spent?
And more importantly,
we can allocate those costs based on our resource tags.
So we can say how much is the data science team spending?
How much is the development team spending, right?
We can figure out where that money goes based on our tags.
So what exactly can Cost Explorer do for us?
Or what do we have to know about Cost Explorer for the exam?
Well, the first is it will break down
your service utilization by the service
that's spending the money.
As we'll see in a minute,
I can very easily see where that expense is coming from.
And I've got a few doozies I'd like to show you.
We can take a look at a timeframe,
you can break it down and say,
how much was my spend last month, the last 6 months,
the last 2 years,
and it also allows us to look forward.
Now this isn't some magic crystal ball in the future,
this is just taking a best guess and estimate
based on our previous months
and our previous utilization history
of what are we looking for at the end of the month,
about how much will we spend if current trends continue?
We can also filter and break down this data
in just about any way that we'd like.
We can do it by service, by region, by tag,
by instance type. The world is your oyster,
or at least it is when it comes to filtering
where that cost is coming from.
Now it's fine to talk about it in theory,
but let's see how I'm spending that money
inside of my account.
Alright, folks,
so I've got my AWS Console open
and I'm here on the Cost Explorer page.
Now we're taking a look at the last 6 months of spend
inside of my account and you can probably see where,
oh, I made a few mistakes.
So let's take a look at March 2021.
We can see here that I spent nearly $600
on a database that I honestly didn't really need.
I spun it up for a video and I just left it running
for far too long.
Now you'll see in April, my engineering team nicely told me,
yeah, you need to fix that.
Namely, turn it off and throw it away.
Now in an upcoming lesson,
we'll take a look at budgets and how I can maybe
be a little bit more proactive shutting down
that spend rather than being reactive.
Now in my portal here, I can easily go through and say,
yeah, you know what,
I'd like to filter by region.
How much money am I spending in, let's say, Virginia.
Well, not nearly as much as namely my other region, Oregon.
I can break it down by tag as well.
Now, opening this up here,
you're going to notice I only have one tag available to me,
in order to filter based on tags,
you have to enable that particular tag
such as created by, name, department,
whatever you'd like to use as a cost allocation tag
in the billing portal.
You simply just have to say thumbs up, yes,
I'd like to use this,
and then going forward from the moment you enable that
into the future, you can then filter your reports
or your spend based on those tags,
but it does not work retroactively,
and you have to opt in on a per tag basis.
What Cost Explore also allows us to do--
so I'm going to go ahead and clear out my region here--
we can look into the future.
That's not quite as exciting as it sounds.
We're just taking a best guess of where
that spend is coming from.
Now that's not as exciting as it sounds,
AWS is really just taking a best guess of, well,
what that forecast is going to look like.
So let's go ahead and see here.
It's betting that at the end of this month,
I'm going to be spending, well, maybe a hundred bucks,
something like that.
As I move on, somewhere about 250.
It's guessing based on my history of creating architecture
and then destroying architecture.
My costs are going to start to ramp up.
Now it's a good idea to pay attention
to these values as well,
because if you have a really big outlier month
where it thinks you're going to spend a lot of money,
maybe take a look at why does it think it is.
Right, am I spending a lot of money early in the month,
and maybe I'm wasting some cash?
Cost Explorer is a good service
to check in with every once in a while,
as well as setting up reports and budgets,
but we'll be covering those here in a future video.
I would also recommend that you play around
with this yourself,
open this up inside of your own personal account
or work account, if you have access to that,
and just take a look at where is the money going
and what can you filter on?
Alright folks, let's call it quits inside of my account
and we're going to hop back over to some of those exam tips.
Alright, let's come back together for some exam tips.
Then my first tip here is if you're looking at the exam
and it's asking about cost, think Cost Explorer.
You don't really have to know it in depth.
They're not going to ask you how do you generate a report
or how do you build one?
You just need to know you can filter based on tags,
based on regions,
based on some of those general structures that we saw
just a few seconds ago,
and it helps you create that budget and track spend
inside of your account.
So a few more tips here, use those tags, right?
Remember you have to set the tag as a cost allocation tag.
You have to say, I'd like to track based
on this particular tag going forward,
but once you do, it becomes really easy to say,
how much is each department, is each group,
or is each person spending?
We're going to talk about budgets in an upcoming lecture,
but just know for now Cost Explorer
and budgets go hand in hand.
You'll get to use Cost Explorer to help create a budget
for your department, for your team, or even an individual.
Cost Explorer can be predictive,
it can estimate your upcoming monthly costs
and show you well, what do we think we're going
to spend if current trends continue?
And my last tip here, are you having trouble falling asleep?
Try turning off your computer and putting down your phone
a few hours before you go to bed.
That's really helped me get a good night's sleep.
Alright, folks.
Thanks for going through how I'm spending money
inside of my account,
and I can't wait to see you in the next video.


Using AWS Budgets
===================

Hello Cloud Gurus, and welcome back.
In this lesson, we're going to continue our conversation
about cost inside of AWS
by focusing on another AWS cost control tool called Budgets.
So what is AWS Budgets?
Well, you probably have a pretty good guess
from just the name and you'd be right,
but we'll give it an official definition.
We'll take a look at the different kinds
of budgets we can create.
We'll then create one together
and purposely kick off an alarm
just to see what one of those notifications looks like.
From there, we'll circle back to some of those exam tips.
So let's go ahead and dive right on in.
Our first question is, what is AWS Budgets?
Officially it's defined as a tool that "allows organizations
"to easily plan and set expectations around cloud costs."
It's basically what you think it is.
It allows you to track your ongoing spend,
create different buckets that you can pull money from,
basically allocate that spend and set alerts
to let users know when they're close to
or had exceeded the amount of money
that they can use for the month.
There are 4 different kinds of budgets
that we can create.
The first type of budget is called the cost budget.
This is probably what you imagined
when you thought about budgets.
Just how much are we spending?
Just tracking my architecture and seeing
how close are we to that limit for the month.
Our usage budget, how much are we using?
Maybe I want to take a look at, well, I have a set amount
of architecture that I'm allowed to create.
How close are we to using all of that architecture?
Reservation budgets.
Are we being efficient with our reserved instances?
Does our on-demand utilization
or creation of architecture exceed the amount
of RIs that we've bought?
Am I not using those RIs that I've purchased?
Well, we probably want to solve that problem.
And then at the top we have the savings plan budgets.
So what are we building
and is that covered by our savings plan?
If not, how can we adjust that to make sure
that we're getting the best possible deal?
Now, this is great to learn in theory, but once again,
we need to see this in practice.
So let's hop over to my AWS account
and we'll walk through creating a budget together.
I've got my budget service open here.
Let's go ahead and create our first one together.
So I'm going to go in here
and I'm just going to start out with
what I would say is the easiest option, the cost-based.
I just want to make sure
that I'm not spending too much money.
This is going to be the quickest and simplest one
for newer users or organizations to get set up with.
I'm going to go ahead and click Next.
Now the steps are pretty simple.
I can specify my different month, day, quarterly, annually.
I'm just going to stick with monthly for now,
but I get pretty granular with these timeframes if I like.
I consider it as recurring, expiring.
In this case here, I just wanted to start this month,
but I could start it next month, in 6 months,
it's up to you to pick.
And in this case, I'm just going to specify a fixed amount.
So for me, I don't want to spend more than $25 this month.
I think that's a pretty good goal.
I can spit up an extra instance or two,
but I've got to make sure I get rid of them
when they're no longer needed.
Then down here you can play around with Cost Explorer
to see what exactly you're looking for.
This is why we talked about Cost Explorer
in the previous lesson.
You can get your filter set up with your tags
or your regions or your services,
and you can also get pretty advanced
if your accountants want to get involved
and talk about amortized costs or blended costs,
or that's not particularly relevant in this situation.
But I guess it's good to know that it's there
and I'm just going to give it a name.
Alright, I think that's a good budget name
and let's go ahead and get to the alert phase.
Now, why would I create a budget
if I'm not going to alert on it?
Well, there's not really a whole lot of purpose then.
Let's add an alert threshold.
Now in this case here, I'm just going to say,
when I have used--and I'm doing this on purpose--
1% of my monthly budgeted amount.
Now, normally this would be a pretty poor amount
to budget off of because that's just 25 cents,
but I actually want to kick this email off.
So I'm going to send myself an email here.
I've got that email in here.
I could use those SNS alerts as well.
Now in this case I'm just putting in my email directly.
So I don't have to create that SNS topic.
I'm going to go ahead and click Next.
Now, in this case here,
I've got my alert set up and if I wanted to,
I could add in an action.
So I can just say, let's grab that one.
When this kicks off,
I can automatically stop those EC2 instances.
I can automatically attach a policy to shut down calls
so I can kick off actions to happen inside of my account,
not just alerting those users.
Now, in my case here, I just want to get that nice email.
And so let's go ahead and remove that
and just stick to the notifications.
I get my quick summary here, scrolling down,
everything looks great.
I'm going to go ahead and click Create.
And that'll take just a second to come into play here
and oh no, my budget is exceeded.
Well, I wanted that to happen on purpose.
So let's go ahead and check out that email
that I just received in my inbox.
Well, this one's pretty basic, straightforward,
right to the point, just saying that I've spent $10 already.
I only wanted to spend less than 25 cents.
I'd really recommend that in your own personal accounts,
set up a budget.
You get 2 for free per month.
Set something up to make sure that you're not spending more
than what you're willing to spend,
especially as you start practicing and learning AWS,
because sometimes those account charges
can be a little bit sneaky.
Alright, let's go ahead and call it quits on this demo.
And we'll hop back over to take a look
at some of those exam tips.
But my first exam tip here is
it's good to know that you can make budgets.
You need to know that you can create them,
that you can alert off of them,
and that you can automate that response, if you'd like to.
We can use Cost Explorer to create those budgets.
We can use tagging as a filter, that's important to know.
Now you're not going to have to be able to dive
into each different type of budget in-depth.
Just keep it at that really high overview,
but you will have to know that the service exists
and can alert based off of spend.
We'll take a look at a few more tips here.
Our first tip here, be noisy, alert users.
It's the best way
to know that you're approaching that threshold.
That, oh yeah, you spent too much money
or you're about to spend too much money.
Because once the money is spent, it's gone.
Amazon's not going to give you that refund.
So get ahead of that spend and alert folks.
You're going to need to understand Cost Explorer
and how to use it to create fine-grain budgets.
Now that's just good experience to have
for potential employment.
For the exam, you just generally have to have
that high level review on both of them,
it's not going to get super in-depth
about the kinds of filters, just focus on using those tags.
And like I said a second ago, be proactive
once the money is gone, it's gone.
There's no getting it back.
Be especially proactive with those alerts,
if it's your own money,
because you want to be careful to not spend too much
of our own cash.
And then use those tags.
Once again, create those cost allocation tags.
Say, yeah, we're going to create a budget
based on my dev team, based on my frontend team,
based on my data science team.
You can create very specific budgets using Cost Explorer.
And finally, if you're tired of ads on your home network,
try creating Pi-hole as a network-wide ad blocker.
It's actually a great practice.
You get some experience setting up your own DNS server
and you get to look at a whole lot less ads
when you're all done.
Alright folks,
thanks for going through AWS budgeting with me,
and I can't wait to see you in the next lesson.


Auditing with Trusted Advisor
==============================

Hello Cloud Gurus, and welcome back.
In this lesson,
we're going to be taking a look at a free auditing tool
that AWS provides called Trusted Advisor.
So starting out,
the first question that we're going to have to answer is,
what is Trusted Advisor?
And why would I want to use it inside of my account?
We'll see the different best practices
that Trusted Advisor can check for
inside of our environment.
We'll get to see some best practices
that I'm following inside of my account
and maybe some things that I should change.
And then we'll wrap it all up
with some of those handy dandy exam tips.
So let's go ahead and dive right on in.
That first question that we have to answer is,
what is Trusted Advisor?
Our official definition is that it is a "fully managed
"best practice auditing tool" that scans your AWS account.
In reality, Trusted Advisor is just your little buddy
inside of your AWS account
helping you follow the best practices that you know about
but maybe sometimes you aren't always executing on,
myself included.
I've been doing this for years
and Trusted Advisor always flags something for me.
So what exactly does it look for?
Well, there are 5 areas that Trusted Advisor looks over.
The first is cost optimization.
Are you spending money on resources that aren't needed?
Are you wasting your cash?
Yes. Amazon will, in fact, tell you
when you are giving them too much money.
So we need to take that opportunity
to maybe throw something away,
spin something down,
at least just turn it off
because it's probably not being used
as much as it should be.
Performance.
Kind of the opposite of cost optimization.
Is my architecture falling over?
Maybe there's some settings
that I don't have set up properly.
Maybe that EBS optimization isn't enabled
for my performance IOPS volumes.
Things that I can tweak
to make my application run smoother.
Security.
Do I have holes in my environment?
Do my security groups look like Swiss cheese?
If they do, I should probably go in and fix that.
Fault tolerance.
What happens when something fails?
Do I have that multi-availability zone set up
for my RDS databases?
If that goes offline,
is it automatically going to recover?
And then finally, service limits.
Do I have room to scale?
Am I about to hit that maximum
of 5 elastic IP addresses inside of my particular region,
and I need to file a ticket to get that raised?
Well, it's great to know these in general,
but let's let me flip over to my AWS account
and we'll get to see this in practice.
So I've got my Trusted Advisor service up and running here
and we can see the different sections
that Trusted Advisor will check for me.
Cost, performance, security,
fault tolerance, service limits.
Let's actually take a look at this here. I'm
going to start off by taking a look at cost optimization.
Now, that says 0, 0, 0,
not because I'm doing everything correctly
but because I actually don't have full access
to Trusted Advisor.
So technically this service is 100% free
but you get additional value, additional checks,
if you pay for an AWS support plan.
Meaning you have to pay for support,
not just take advantage of those free forums.
If you do, then it can tell you things like,
are your EC2 instances underutilized?
Do you have unassociated elastic IP addresses?
All of that kind of fun stuff.
Now you do get some checks out of the box for free
and everybody gets these.
So let's take a look at some of those security things
that I might need to review.
The first thing here is,
I don't have MFA on my root account.
Now shame on me.
I don't have a reason for this,
I just need to go fix it.
And in fact, that's going to be the first thing that I do
after this video finishes.
Here we're checking for,
are my S3 buckets too open?
Now scrolling down here,
I can see that I actually have a couple of buckets
that I serve websites out of.
So I wouldn't want to action on this,
but it flags it, nonetheless.
Now I can exclude those values, if I'd like.
Scrolling down a little bit farther,
I have ports that are open on my security groups.
I'm not going to go through all of these checks,
nor is that required for this exam.
You don't have to understand every one of these checks
because that would just be a lot,
but you need to know the general purpose
of what Trusted Advisor does.
Let's scroll back up here
and take a look at my service limits.
Now this is going to be here to show me,
am I about to bump into one of those soft limits?
Let's actually see this with those VPCs.
So scrolling down a bit more.
Ah, I have 4 VPCs inside of my Virginia region.
My account limit is 5.
That doesn't mean I couldn't spin up 6.
I would have to contact AWS support
and everybody gets this option for free,
you don't have to pay for it.
But I'd have to say, hey, I'd like to raise my limit,
get rid of those training wheels
and be able to spin up 10, 15, 20 VPCs.
For now, if I tried to spin up a sixth VPC,
I would just get an error that says I've hit that limit.
So it's important to know
that Trusted Advisor can flag these
and you can also set up alarms
to let you know when you're getting close,
when you hit that 80% threshold.
I'd really recommend that you open this up
in your account or in that sandbox account
and go through to get familiar with the kinds of checks
that Trusted Advisor has for you.
For now, let's go ahead and flip back to those exam tips.
My first exam tip here is,
we want to automate a response.
So on the exam, focus on answers
that have an automated solution,
whether this is kicking off that Lambda function
to solve that problem
or it could also be just alerting a user.
But you don't want to have Trusted Advisor
just yelling into the void.
You need to be giving yourself or somebody else on the team
at least a heads up
that you're about to run into an issue
or there's something wrong,
so you can get ahead of that problem.
Now, a few more tips here for us.
Have that alerting setup.
Once again, SNS,
that Simple Notification Service is your friend
to let users know that something's going wrong.
Now, Trusted Advisor is free
but as we saw, the majority of checks are actually locked
behind that business or enterprise support plan.
Even if you don't have one of those plans,
at least review some of those security checks
because I would argue those are probably
the most important ones
and we do get those out of the box.
Trusted Advisor is not going to fix the problems for you.
That's actually a common trap
that I see folks run into on the exam.
You need another service, something like Lambda,
to actually solve that problem for you.
Trusted Advisor is just effectively
that canary in the coal mine
telling you that, hey, there's a problem,
now it's your turn to go fix it.
Now we want to automate this,
so use EventBridge, also known as CloudWatch Events,
to kick off that function, to kick off that solution
that's going to automate that response,
that's going to fix the problem for you,
so you don't have to manually intervene.
And we want to focus on exam questions
that use this sort of setup.
And lastly here, it's much better to be friends
with someone who owns a boat,
than to be the person who owns it.
It saves you a lot on repair costs
but you still get to have fun on the water.
Alright, folks, thanks for helping to audit my account
using Trusted Advisor.
I can't wait to see you in the next lesson.


Governance Exam Tips
=======================

Hello Cloud Gurus, and welcome back.
In this lesson, we're going to take a look
at some exam tips for all of those governance tools
that we just finished covering.
So let's go ahead and dive right on.
Now we've got our 4 questions that I want you to keep
in mind as you're reviewing scenarios in the AWS exam.
The very first one is can it be centralized?
Is this scenario asking for a tool
or a solution that I can store
in one single location instead of one single AWS account?
And then have that spread out
to the rest of my architecture,
rather than trying to duplicate that
over and over and over again.
Maybe this is a centralized storage location
for all of my CloudTrail logs
or my CloudWatch logs, or my authentication.
Maybe this is where I'm going to place Active Directory
or manage my roles out of.
But we need to keep this in mind.
Can I centralize this
or does it actually have to be copy and pasted everywhere?
How do we standardize?
Now that is an age old question
and it's up to you and your team
or your prospective employer to come up
with those standards that you want to enforce
across your organization.
As far as the exam goes, it's going to pretty much lay out
what we want to standardize
and then it's going to be up to you to decide
which one of the governance tools is the right tool
to standardize with.
This could be something along the lines of
how do I standardize my security group rules?
How do I ensure that all of my logs are going
to one centralized location?
Effectively, how do I use a config rule to make that happen?
Once we've defined the standard, how do I enforce it?
With that security group
that only has to have particular rules in it,
I could create that Config rule.
And then I can use the automated remediation steps
that Config offers
to kick off that Systems Manager document,
or maybe even that Lambda function
to enforce that standard.
Standards enforcement is not baked in
to that AWS architecture
so we're going to have to define the standard,
pick the tool to enforce it
and then set up those automated steps.
When it comes to managing users,
are we dealing with internal or external users?
Because as we saw in those previous lessons,
there's no one size fits all in this situation.
Those internal users might use something
like Active Directory, my IAM accounts,
where external users could be using something
like Cognito to handle their authentication
into my environment.
So we really have to focus
on what kind of users are being talked about
in the scenarios that we're given.
Now, let's take a review of AWS Organizations.
What do we need to know that's specifically related
to passing that exam?
Service control policies, or SCPs for short,
and you will see that abbreviation on the exam,
are the only way to restrict root accounts.
I'm going to repeat that again.
SCPs are the only way to restrict root accounts.
Please remember this for the exam.
They have the ultimate and final say,
and they override all other permission sets.
Centralized logs, they're always the right answer.
Now, let me back that up real fast.
They're always the right answer when talking about logging.
We always want to focus on having a logging solution
that centralizes everything into one single bucket
or even one single logging account.
And CloudTrail offers the ability to centralize all
of its logs using AWS Organizations.
So keep that in mind
because you never want to spread out your logs
to multiple locations because it increases the chances
that you're going to lose some data
and it makes it much harder to audit later on.
Isolating workloads into separate accounts
is always a good thing.
While it does add additional layers of management,
as far as the exam is concerned,
we're really focusing on those additional layers of security
and control that we're able to give our resources
by separating it out.
We want to avoid answers that just say, oh,
we're going to lump everything into one single account.
Having a dev account, a prod account, a staging account,
a QA account, a login account,
this is a better answer than just using one
because it gives us the ability to isolate everything
and reduce that blast radius if there is a problem.
So if one account is compromised or has an issue,
it's not going to affect the other accounts.
Let's take a look at those exam tips for Config.
Now, as we recall, Config is your best friend
when it comes to standardization.
Anytime we need to have a standard
and then enforce that standard inside of our AWS account,
we want to be thinking Config,
because Config can create that rule
to check for is this standard set up correctly?
Or is somebody violating my best practice?
And they've broken that rule.
Whenever we find that rule violation,
we need to automate that response
using those automation documents
that Systems Manager provides.
This is going to be your best remediation step.
Another option could be using Lambda
and a function that's written.
So if you don't see automation documents, don't panic.
Lambda can work, but in general,
we're probably going to focus on those documents.
Now, Config not only gives us the rules
but Config also gives us the ability to see what changed.
So if you're seeing as scenario that lays out, oh,
we need to take a look at what changed where
and when, Config gives you the ability to go back in time
and say, oh, that database was provisioned at this date
and shut down at this date and can link you
to those CloudTrail logs of who actually made those calls.
So it gives us a history of what happened
on top of giving us a set of rules
and the ability to enforce those rules.
And then finally, for those authentication services,
we want to keep in mind the right tool for the right job.
We're using single sign-on Active Directory
for internal users and Cognito for external.
If you see a question that's talking
about mobile external users,
automatically think Cognito.
Internal users, we do have a variety of choices.
There's no one solution that's perfect
but we're using
either single sign-on, on-premise Active Directory,
single sign-on linked to AWS Active Directory, right?
There's a variety of options there, but for external users,
always think Cognito.
Active Directory is a common topic
that will pop up on the test.
Now, thankfully for all of us,
it's not going to be a deep dive.
They're not going to say, how do we configure it?
How do we set up domain controllers?
How do we build group policies?
Beause that's more of a Microsoft test.
For us, we need to know where do we put Active Directory
and how does it work with our directory service?
If the scenario is laying out a lift and shift
into the cloud,
we want to pick that managed Microsoft Active Directory.
Because remember, that's the full suite of architecture.
If this migration is talking about leaving Active Directory
on-premise, leaving it in that physical data center,
you want to think AD Connector to connect back
into that physical architecture.
That's really about the amount
of Active Directory skills that we're going to have to have
for this exam.
Just know that at a high level,
remember it's covering those internal user accounts.
Never, ever, ever, and I cannot say that enough,
set up credentials where you don't need them.
We always want to use those cross-account roles
or even just roles in general,
rather than creating IAM credentials we don't need.
I have seen exam questions that talk
about an auditor is coming on-site.
What do you do to set them up?
And one answer would be you create an IAM user
and give them full permissions.
And if you did that, you would fail your audit.
Best practice, give them temporary access by the ability
to assume a role.
Use that to check all of your architecture
that's spread out across multiple accounts.
Use roles everywhere.
That should be a big theme
that hopefully we have iterated over and over on
over the previous lessons.
Roles are the best answer
when it comes to talk about credential.
When you're doing that cross-account access, use roles.
If you see a scenario that's talking
about cross-account access,
always use roles to make that happen.
You never want to duplicate user credentials
and make them in accounts that shouldn't be there.
Use roles so you'll have your primary IAM set of credentials
in one account, and then use that cross-role account to hop
into those other AWS accounts.
You will never see a correct answer on the exam
that includes creating duplicate IAM credentials.
So I just want to repeat that.
Use roles everywhere.
If you only take away one thing from every video
that we have talked about for this exam,
it should be roles are your best friend.
Now let's take a look
at some cost management strategies
that we want to keep in mind for the exam.
The first one is tracking costs is a common exam topic.
It's true.
Cost management pops up a lot.
Now in order to make sure
that you're not spending too much money, that you're falling
within those budgets that are laid out in the exam,
you want to make sure
that you have tags set up so you can audit your spend.
You can use Cost Explorer to run those reports
and you can use Budgets to create well, budgets.
Set up those alarms, set up those alerts
so you're being proactive about your cost notifications.
Get ahead of the problem.
SNS is your friend.
On the exam, focus on answers
that alert users using SNS that give them that heads up
when you're getting to that 80, 90, 40% threshold
whatever is specified on the test.
Automate, automate, automate.
If I had one word to sum up the entire test,
automate would be it.
Automate the response.
If you're spending too much money, shut things down.
Spending too much money, turn stuff off.
Eliminate that CloudFormation template.
Shut down that architecture
and you want to do it using Lambda,
using Automation Documents, using something
that doesn't require human interaction.
Always prefer answers on the exam that include automation
especially when it comes to cost.
Let's take a look at some exam tips for Trusted Advisor.
It's good to know that it's free
but if you want the extended set of checks,
you have to have a paid support plan
because those are going to be the really useful checks.
It would be good
for you to go back and review Trusted Advisor.
Just get a general idea of what those checks are.
You don't have to memorize all of them,
but keep in mind those different areas of focus
that Trusted Advisor looks into.
Now there are limits.
On the exam, you will see distractors
that paint Trusted Adviser as the end all be all
to every sort of auditing issue that you'll run into.
That's not actually the case.
Trusted Advisor can simply tell you when something is wrong
but you are going to have to fix the problem.
It will not automatically solve the problem by itself.
And that's where once again, we come back
to that important word, automate.
Use EventBridge to kick off Lambda
to solve the problem for you.
Focus on answers that have that complete resolution
that don't just say there's a problem,
but that identify the problem and then automate solving it.
Alright, folks.
Thanks for going through the governance section with me
and I can't wait to see you in the next set of videos.



Migration
=========

Migrating data with AWS Snow Family
===================================

Hello, Cloud Gurus, and welcome back.
In this lesson, we're going to take a look at moving data
in and out of AWS, using the Snow Family.
The first question that we have to ask
is: how do we migrate data?
Because there's not always a clear-cut answer
of how we make this happen.
So we'll explore and summarize
some of the options that we've talked about
in previous lessons,
and then we'll move into the Snow Family.
And what makes up the Snow Family?
What are those members?
We'll learn about Snowcone, Snowball Edge,
and last but not least, the Snowmobile--
all nicely snowed-themed, of course--
before we end it out with some of those exam tips.
So let's dive right on in.
How do we get data into the cloud?
How do we get data out of the cloud?
These are fundamental questions
that we have been asking for years and I can't tell you,
"Oh, here's the one perfect solution,"
because everybody's situation is going to be unique.
How we've been getting data into AWS so far
is using the internet,
copying data over that internet gateway
to get resources inside of the VPC access to our content,
or just copying data straight up into that S3 bucket.
And it works, but there's potentially a few issues here.
It could be slow.
I think we've all experienced
that slow home internet connection.
Maybe we don't have a very robust channel
in our office or data center,
and it could also be a security risk.
We're using that public networking space,
and that could pose problems with those auditors.
Now, we could use Direct Connect.
It could be faster,
depending on how big of a pipe we're paying for,
and it's going to be secure,
but it's not always the most practical.
What happens if I need to copy
terabytes or petabytes of data into AWS
and then I'm turning off my data center the very next day?
Well, it's not going to be cost effective or speed effective
to just use Direct Connect for a very short period of time.
That's really meant for more ongoing sort of situations.
That's where the physical transfer method comes into play.
Have you ever copied something to a flash drive
and walked it over to your coworker?
Yeah, the old reliable sneakernet.
That's what we're doing here,
but this is really sneakernet 2.0.
We're taking our data,
we're using a physical enclosure provided by AWS,
and we're bypassing the internet altogether.
That brings me to, what is the Snow Family?
Oh, our official definition is that
it's a set of secure appliances
that provide petabyte-scale data collection and processing
and migration and computed... all those good words.
In reality, what it is--
it's just a sack of hard drives that Amazon ships to you.
Some sacks are small, some sacks are large,
varying amount of storage,
and some of these have built in computing,
but it allows you to load up your data,
load up your content,
throw it on the back of your friendly mail carrier truck,
and ship it physically to AWS.
Now, the first part of the Snow Family
that we're going to take a look at is the smallest,
and what I would say is the cutest, the Snowcone.
The Snowcone gives you 8 TB of storage,
4 GB of memory, and 2 vCPUs.
So that's a pretty--
kind of an M style sort of EC2 instance
with 8 TB of storage.
So what do you do with that storage? That compute?
Well, it's really designed for applications
that aren't in a data center.
That aren't even necessarily
by any sort of actual computing infrastructure.
Maybe you're collecting information from wind farms.
Maybe you're one of those techs that has to climb up
to the top of the windmill.
You don't want to haul a lot of stuff up there.
You take the small device, you download your data,
you process it onsite, and when you're ready,
you ship it back to AWS.
It has direct Internet of Things sensor integration,
it works with the AWS IoT platform,
and it's great for those edge computing environments
where power and space are constrained.
Now, if you need more storage
than 8 TB or a little bit more juice
for that computing side,
well, that's where the Snowball Edge comes into play.
A Snowball Edge is a much bigger physical device.
You can still pick it up by yourself,
but you're not going to be climbing up a ladder with this one.
It has between 48 TB and 81 TB in storage.
There's a few different flavors of this,
and it really depends on your needs.
That storage option, well,
it's really a device that focuses primarily on storage.
It has some compute,
but not as much as the compute Snowball Edge.
If you have that machine learning application
and you need an actual physical GPU,
you can get that one built in.
Now, for the exam, you don't have to know the specifics
around the actual numbers
or how much CPU or how much RAM there is.
Just know in general, 50 to 80 TB for Snowball Edge--
that's a good number to remember.
It has that varying amount of CPU and RAM.
It's good to know.
It's perfect for when you're on a boat
in the middle of the ocean.
You got space, you got data,
but you don't have that connection.
It's also great for doing that one-time migration into AWS.
Now, if you need more storage,
well, that's where the Snowmobile comes into play.
This is the heavy hitter, the big one.
This is literally a semi-truck full of hard drives
that drives up to your data center
and downloads 100 PB of storage.
Yeah, let that one sink in.
That is a lot of information that you are moving.
It's designed for companies
that have exabyte-scale data center migrations.
Before I started learning about AWS,
I didn't even know what an exabyte was.
That is more data than I could possibly comprehend,
but some companies have that much
that they're moving to AWS.
It's really cool.
You don't really need to know about it for the exam,
I just think it's awesome.
It might casually show up as a distracter.
You probably don't need this.
Good to know that it's there.
I'd focus the majority of your time looking at Snowball Edge
and to a lesser extent, Snowcone, Snowmobile,
maybe Google that photo of the semi-truck and call it a day.
Now folks, I got to say, unfortunately,
I could not get a Snowmobile to show you in person
so no hands-on demo this time,
but we will end it out with some handy-dandy exam tips.
So how do you transport your data?
Well, there's a variety of solutions.
You're going to have to pick the right one on the exam.
Pay attention to the restrictions that are given to you.
It might say that you cannot transfer the data
over the internet, that you have a large amount of data,
you have a slow internet connection, or very importantly,
the data needs to be encrypted at rest and in transit.
All of these, or a few of these, situations should lead you
to pick an answer that includes a Snowball Enclosure.
Now, a few more tips here for us.
Now, those general use cases.
Like I said,
primarily, it's going to be talking about either storage,
compute, or GPU versions of the Snowball Edge,
but generally know what Snowcone
and Snowmobile does as well.
Snowcone:
small, robust, little bit of compute,
decent amount of storage--8 TB--
nothing to really write home about,
but it's great for space and power constraints.
Snowball: this is what you're going to be using
most of the time.
Storage, compute, GPU varieties.
Remember, about that 50 to 80 TB range
is going to be a good number for you.
Snowmobile:
100 PB,
really cool,
very large truck,
You probably don't need it.
And it's good to know that Snowball, Snowmobile, Snowcone--
they all work in both directions.
You can use it to ship the data to AWS.
You can also use those same devices
to ship that data back to you.
That's important to remember for the test
because I've seen questions that talk about data migration
out of AWS, and Snowball will work for that.
Generally, timing on this
is something that a lot of folks ask about.
Normally, it's about a week.
It all depends, though, on mostly what you do.
If you get that Snowball Enclosure
and don't do anything with it for a couple of days,
obviously that transfer is going to take longer.
If you immediately upload your data to it
and throw it on the back of the mail truck,
you're looking at about a week.
And finally, make sure to keep an emergency blanket, shovel,
and first aid kit in your car.
You never know when you're going to need them
and it's better to be safe than sorry.
Alright, folks,
thanks for looking at the Snow Family with me,
and I can't wait to see you in the next video.


Storage Gateway
================

Hello, Cloud gurus and welcome back.
In this lesson, we're going to be taking
a look at all of the different flavors of Storage Gateway.
Now before we dive into the individual versions,
we have to look at Storage Gateway as a whole.
Why was the service built and how can we use it
inside of our environment?
Then we'll take a look at one of the most popular versions
of Storage Gateway called File Gateway.
We'll see how Volume Gateway can help us with our migrations
of our current virtual machines into AWS,
and then we'll take a look at Tape Gateway
and how we can eliminate those pesky physical tape backups.
And of course, we'll round it all out
with some of those exam tips.
So let's go ahead and dive right on in.
The first question that we have to answer
is: what is Storage Gateway?
Well, Storage Gateway is a hybrid cloud storage service
that allows us to merge on-prem resources with the cloud.
Basically, what it allows us to do
is take AWS and our physical on-prem resources
and just staple them together.
This can be really useful for when we do a lift
and shift or a one-time migration
out of the data center into the cloud
or it can allow for long-term pairings of our architecture.
It's really going to be based on our particular need.
Now Storage Gateway by itself doesn't really get us much.
There's a few different flavors that we have to discuss.
The first one, and the one I'd say
you're going to see on the exam the most,
is called File Gateway.
And what File Gateway does is, well,
it's an NFS or SMB mount
(SMB for those Windows folks out there)--
basically it's a network file share.
You can mount this file share locally
and it secretly, or well, not so secretly
because you've set it up, backs up your data into S3.
Now there's a few different versions of File Gateway
that we can set up.
You can either back up all of your data into the cloud
and it just acts as well, that gateway,
or you can keep a cached copy of the most recently used
files. This means you don't have to download
the recently used content out of S3 if you don't want to.
There's a few different versions of File Gateway
that we can set up.
I could keep all of my data locally on-prem
and back it up into AWS, specifically into S3,
or I could keep the most recently used copies
of my files on-prem and keep everything else inside of S3.
The scenario that you're going to run into on the test
is going to be one where the user, or users,
don't have enough on-prem storage space
and your solution is going to be set up
a cached file gateway.
This allows those users to extend on-prem into the cloud.
You get all the benefits of having the data physically
on-prem and all of the benefits
of having it backed up in S3.
This can help with migrations as well.
If you're working to move network file shares into AWS,
you can set this up and have it run for a bit.
Or have it run permanently,
if you're looking for more of that hybrid solution.
Now, when we say set this up,
it is important to know--
all of these Storage Gateway solutions are simply VMs
that you're going to run inside of your on-prem environment.
These VMs are provided by AWS.
Let's take a little bit of a shift away
from those network files shares into those volumes.
Now you've got all of those virtual machines
that are running on-prem,
how do we back up those drives into the cloud?
Well, that's where volume gateway comes into play.
So this is an iSCSI mount, meaning it's going to be backing
up those disks that those VMs are currently reading
and writing to.
You have that same cached or stored mode,
depending on how much data you want to keep locally.
But the important part is it's all backed up inside of S3.
Now, from here, you can easily create
those EBS snapshots and restore volumes inside of AWS.
So what we can think of this as is an easy way to migrate
those volumes from on-prem to become EBS volumes
inside of AWS.
Once again, this is another solution
that's great for backing up content,
or if you're looking for the migration,
this can help bridge that gap.
Now the last, and what I'd say is my favorite,
is Tape Gateway.
Tape Gateway allows you to skip
having to physically deal with tapes.
Yeah, I've been there.
If you have too, you can probably relate.
Swapping out tapes every week in your backup devices
is not very much fun.
Now, we can set up Tape Gateway and basically trick
those backup devices into thinking
that they're backing up to physical tapes, but they're not.
They're backing up to Tape Gateway.
That takes your data and stores it inside of AWS.
Stores it inside of S3 Glacier, Glacier Deep Archive,
depending on where you'd like to put it.
You don't have to change any of your current settings
or any of your current backup workflows, right?
This is directly integrated as a VM on-prem
and just like I said,
tricks those backup services into thinking
that they're backing up to a tape.
It's encrypted, it's safe,
so you don't have to worry about transmitting
all of that sensitive data in plain text over the internet.
So on the exam, what specifically do you have to know?
Well, we want to focus on Storage Gateway
being a hybrid solution.
It's good for data migration,
but it really excels for when you're going to have
that data center and the cloud stapled together.
So when you're looking at migration questions,
think: which version of Storage Gateway
is going to be the right answer?
Do I need to keep my content cached?
Do I need to migrate all of it?
So when these hybrid storage scenarios come up,
keep Storage Gateway in mind.
You want to be thinking through all of the things
we've talked about, which version of Storage Gateway
makes sense in the given situation.
Let's take a look at a few more exam tips.
You just need to know the general use cases,
the high-level version of,
oh, the NFS service is out of storage? Great.
We're going to set up File Gateway.
Oh, I'm daring to deal with tape backups?
We're going to use Tape Gateway.
Out of space on-prem?
Which Storage Gateway solves this issue?
This is going to be a common problem that you run into,
and based on the type of storage that it is,
that will point you to the right Storage Gateway solution.
While I love Tape Gateway,
it's probably not going to come up as often.
I'm not saying that you can just skip it
or somehow erase the last 20 seconds out of your mind
where we talked about it.
It's good to know that it's there.
It might be a distractor,
but the primary focus on the exam
is going to be on File Gateway and Volume Gateway.
And in the many times that I've taken the exam,
I've primarily seen the focus on File Gateway.
Now that doesn't mean you have to know how to configure
the VM or set it up or troubleshoot it.
You just need to know the general scenarios.
As this sort of topic is going to be pretty high level.
And finally, filling your car tires
with nitrogen can help them from deflating as quickly.
Alright folks, thanks for going
through Storage Gateway with me,
and I can't wait to see you in the next video.


AWS DataSync
============

Hello, Cloud Gurus, and welcome back.
In this lesson,
we're going to take a quick look at the AWS DataSync
service. So we're going to kick this off
with the very important question:
What is DataSync?
Where does it fit inside of my architecture?
And then,
what does it look like to actually use this service?
And then we'll end it all out
with some of those handy exam tips.
Now, as you can probably tell,
this is going to be a little bit of a quicker lesson,
because we really only need
to understand a general high-level utilization
of this particular service for the solutions architect exam.
So let's go ahead and dive right on in.
So what is DataSync?
Well, it's officially defined as an agent-based solution
for migrating on-prem storage to AWS.
It allows us to easily move data between NFS and SMB shares
into the cloud.
So why do I want to use this service?
Well, this is going to be a migration tool.
You've got a whole lot of data on-prem?
You got an NFS mount?
You've got a large chunk of storage
that you want to move to AWS?
DataSync is going to be the setup that you'll want to use
if this is more along the lines of a one-time migration.
So what does it actually look like to configure
and set up DataSync?
Well, here we have our on-prem server.
And what you're going to have to do is install
and set up the DataSync agent,
as this is an agent-based solution.
Now, from here,
you're going to configure the DataSync service to show,
where is this data going to go?
Now, this is all encrypted
as it's transferred over the internet.
So you don't have to worry about folks being able to spy
and see what you're up to.
That's an important thing to remember for the exam.
It provides secure transmission of your data.
You get to decide,
do I want my data to end up in an S3 Bucket?
In EFS File System for that shared block level storage?
Or for the Windows folks out there, in FSx.
The big takeaway from this is that we need to know
it's an agent-based solution
and it supports S3, EFS, and FSx.
While actually setting it up
does take a little bit more work
than just looking at a diagram like this,
this is about as much as we're going to have to know
to pass the exam.
Now, speaking of the exam,
let's take a look at some of those exam tips.
We have a couple of tools that might seem like they overlap.
We've got DataSync, we've got Storage Gateway.
When do I pick one versus the other?
Well, we need to remember
that DataSync is great for a one-time migration.
If you want to have a more continuous setup,
think Storage Gateway.
Now, that doesn't mean that Storage Gateway can't be used
for a one-time migration,
but generally, they each have their own lane.
Storage Gateway really excels
when we want to set up that hybrid architecture,
where we want to take the on-prem architecture
that we know and love and the AWS cloud
and staple them together.
Where DataSync is great for that lift and shift,
and then shut down to that data center.
So a few more tips here for us.
The first one is going to be, primarily one-time migrations.
That's the use case.
Now, like I said, in the actual world,
it's not as cut-and-dry.
However, on the exam, generally,
we want to focus on that one-time use.
It is agent based.
So you will have to install architecture on your end.
You can't just magically point it to the DataSync endpoint,
because that wouldn't work.
You have to have that agent properly set up
and given permissions to then copy that data into AWS.
You will need to be familiar with the general endpoints
that DataSync can send information to:
S3, EFS, Fsx.
That's going to be our supported locations
for where that data can end up.
And then finally, high level.
This walkthrough has been pretty high level,
and it's about as much as you're going to need to know
for the test.
DataSync is not going to pop up in detail,
you're not going to see very in-depth questions
or troubleshooting.
In fact, it might only show up as a distractor.
Just know what we've talked about here so far,
and you should be good to go.
Now, finally, just a quick note for everybody:
nearly 11% of all bank card pin numbers are 1234.
That's actually kind of scary.
It's best to use a random number generator
to create your own,
as that gives it much higher levels of security
and a much lower chance
that somebody is just going to randomly guess it.
Alright, folks,
thanks for quickly going through DataSync with me
and I can't wait to see you in the next lesson.


AWS Transfer Family
===================

Hello, Cloud Gurus, and welcome back.
In this lesson we're going to take a look
at the AWS Transfer Family.
We have to start off by defining:
What exactly is the Transfer Family? What do they do,
and when do we want to use them?
Then we'll take a look at how we can implement
the Transfer Family inside of
maybe some of our older legacy applications.
And then we'll wrap it all up with some of those exam tips.
So let's go ahead and dive right on in.
Now, what is the AWS Transfer Family?
Well, fun fact, that photo you see on the left
is actually a picture of the Transfer Family.
They might look a little strange,
but I promise they have some pretty cool features.
Specifically, the Transfer Family allows you
to easily move files in and out of S3 or EFS
using Secure File Transfer Protocol,
SFTP, FTPS, or FTP.
Now, if you've never worked with these protocols before,
I wouldn't be too surprised.
They're generally kind of aging out of the tech space
and probably not something that we're going to see
on a regular basis now that we're moving to the cloud
and we have all of those wonderful AWS API calls.
Now, let's take a look and see
how this can work in practice.
So gather round, children, Old Man Galvin
has a story for you.
Back in the day, I worked with a customer
who had tons and tons of legacy users.
Namely, these were different hospitals all around the U.S.
And these hospitals didn't really want to
change their application or change their code.
What they would do is they would take their data
and upload it using SFTP to the customer's endpoint.
Now, the problem for that customer was
they were migrating to AWS.
They're migrating to S3 for all of their storage.
How do they go back and update these thousands of hospitals
without changing every single piece of code,
without changing out everybody's application?
Well, thankfully Transfer Family can handle this for us.
What it allows us to do is basically swap out endpoints.
We can still keep that endpoint
that the legacy users are connecting to,
but basically trick the applications
to read and write data from S3 and from EFS.
Now it is good to know--Transfer Family supports SFTP
and FTPS from outside of your AWS environment
into S3 and into EFS,
but it only supports FTP transfer internally,
inside of your VPC, and that's for security reasons,
because FTP is, well, just not secure.
Alright, well, that was a very quick lecture
because we don't really need to know a ton
about Transfer Family in this particular exam.
All we need to know is the high level.
So on the test, we just want to keep in mind
that the easiest way to change nothing
is with the Transfer Family.
So this really excels when you have
a collection of older applications.
If you see the test talking about anything FTP related,
you should automatically think Transfer Family.
We don't have to change the underlying endpoint,
we don't have to update our application,
we basically just swap out the technology
behind that DNS entry that users are reading
and writing from to be our new Transfer Family endpoint.
And we then trick those applications into talking to S3.
Now, a few more exam tips here for us.
Legacy: Transfer Family excels when you see that word.
When you have an application that you can't change,
but you need to swap out the storage medium
to be something that's S3 or EFS,
this is what you're going to want to set up.
Now it is important to keep in mind those protocols.
They're pretty simple though.
FTPS, SFTP--that's from outside of your AWS environment in,
and then inside of your VPC--FTP.
Now for the DNS entry, those actually stay the same.
We're just swapping out where that DNS entry writes to,
namely, S3, through the magic of the Transfer Family.
And then we want to keep it pretty high level.
In all honesty, you're not going to see a lot of questions
about this service.
Maybe it'll pop up in passing.
It could be a distractor,
so you need to know what it's used for,
but I wouldn't spend too much time diving
into the intricacies of setting this up,
or troubleshooting, or any of those things.
And finally, during an interview, bring a bottle of water.
Sipping water is a good way to stall and give yourself
more time to think about the question.
Alright, folks, thanks for taking a look
at the Transfer Family with me,
and I can't wait to see you in the next lesson.


Moving To Cloud with Migration Hub
===================================

Hello, Cloud Gurus, and welcome back.
In this lesson, we're going to take a look
at moving to the cloud using Migration Hub.
We'll start off with: What is Migration Hub?
When do we want to use it,
and what kind of migrations is it going to help us with?
Then we'll take a look at the Server Migration Service,
commonly known as SMS.
We'll then pivot to database town
and take a quick peek at the Database Migration Service.
From here, we'll get a view
of what does the Migration Hub look like inside
of my console.
And then we'll wrap it all out
with some of those handy exam tips.
So let's go ahead and dive right on in.
So what is the AWS Migration Hub?
Well, essentially, it's a single pane of glass
that we can look through to track the progress
of our application migration to AWS.
It's more of a GUI-based service
that allows me to record and schedule,
and it integrates with 2 of the actual tools
that do the heavy lifting, the SMS and the DMS service.
At first glance, you might say this is just a GUI and it
is--it's an organizational tool,
but it really helps us to track and manage the 2 services
that are doing the heavy lifting, SMS and DMS.
So let's take a look at those tools
and see how they can help us.
So the first service we're going to take a quick peek at
is the Server Migration Service.
So in this imaginary situation,
you have your very expensive,
very robust VMware architecture up and running
inside of your data center.
And you say, "Alex, I'd like to move this with me."
Okay, great.
You don't have to just throw everything out the window
and build it new inside of AWS.
Using SMS, what you can do is schedule when you'd like
these copies to happen.
So what it's going to do is during the scheduled time window,
it's going to take a copy of your underlying vSphere Volume.
It's going to bring that data into S3.
From here, it's going to convert that into an EBS snapshot
and then create an AMI from that.
So in a roundabout way,
what it allows you to do is just take
your VMware architecture and convert it to be an AMI.
And that's really the summary that you want to have
when you go sit the exam.
Now, the second tool that we're going to see here
is the DMS service, SMS's best friend.
So the Database Migration Service kind of does
the same thing, but with the database.
So what is it can do is it can pick up
that ugly, old Oracle database
and run it through the Schema Conversion Tool
and output that data into an Amazon Aurora database.
Now the Schema Conversion Tool has a variety
of different kinds of jobs that it can do,
but its primary purpose is to take Oracle databases
and SQL Server databases and convert them to Aurora.
Now that is a gross simplification of this tool,
but that's what we really need to know for the exam.
The second thing it can do is it can take existing on-prem
or standard RDS or EC2 architecture--
basically a database anywhere--and move it into AWS.
You can use the DMS service
to consolidate multiple databases
into 1 single piece of architecture.
So this helps us as we don't have as many things to manage,
and it also helps us during the migration process.
So you just want to keep a high-level view of SMS and DMS
in your head when you're looking at exam questions
that talk about migrations.
Now let's take a quick switch over to my console
and take a look at the Migration Hub inside of my account.
Alright. So in front of us here is the AWS Migration Hub.
Let's take a look at some of its features here.
Now, when we scroll down a little bit,
we have 2 steps that we have to go through.
We either get started with a migration if we already know
what's happening inside of our environment,
or we get started with the discovery,
because it is vitally important for you to understand
what's going on before you try to migrate it.
So there's a few steps
that it's going to try to walk us through here:
discover, migrate, and eventually track the status
of that migration.
And there's quite a few discovery tools
that we have available here to us.
We have the Discovery Agent here,
and what it's going to do is, well,
you install it on your VMs, your physical architecture,
and it just maps out what's going on inside of that OS--
what packages you have, what services are running--
giving you all the details.
Here, we can deploy this Discovery Connector into
our VMware vCenter to summarize information about our VMs.
That can help us with recommendations when it comes time
to spin up new EC2 architecture, create load balancers,
all of those fun steps.
Now if we take a quick step back to the dashboard here
and click on the button for Migration,
let's go through the process of
at least quickly looking at these tools.
We have the Server Migration Service
that we just talked about that helps migrate our VMs
into AWS as AMIs.
There is integration with CloudEndure,
with some third-party solutions as well.
So it's not just AWS architecture.
And then finally, DMS down here.
Now, you don't have to know these tools in depth,
or really much past just the general overview
of what they do.
I'm going to go ahead and leave it here.
And let's hop back into the slides for some final exam tips.
What we want to take away from this lecture is
that we don't need to do it ourselves.
If you're looking at an exam question
that talks about migration,
just understand which tool you're going to want to use.
We want to keep in mind, DMS is going to be our go-to
for moving databases
and crucially migrating off of SQL Server and off of Oracle
to something on RDS, namely Aurora.
And SMS is going to be the best option for helping us take
those existing VMs and moving them as AMIs into AWS.
Alright. A few more tips here for us.
Now, these tools are magic. as far as the exam is concerned.
I know in the real world,
there's a lot of configuration that you have to do.
There is a ton of steps that we just glossed over,
but those don't really matter
as far as the exam is concerned.
You just get to magically wave your migration wand,
and everything happens for you.
On the exam, remember the Schema Conversion Tool can help us
migrate to Aurora or RDS.
Hint, hint, hint.
We want to migrate off of Oracle, off of SQL Server.
Anytime you see those words pop up on the test,
start thinking about how we can get off of
those database engines.
Now in the real world, you might have more
of a hybrid environment where you leave some things
in your physical environment
and you take some stuff to the cloud.
On the exam,
generally favor scenarios where you're going to migrate
everything into AWS.
And once again, just keep it high level.
Great to know what these tools are.
If this interests you,
please do a deep dive on this content.
Go through, set it up, walk through this yourself.
But as far as the test is concerned,
we just need to know the general overview.
And finally, if your router supports it,
try creating a separate network for your IoT devices.
Things like your thermostat, your light switches,
these have far more security issues and should be isolated
from your primary computing architecture.
Alright, folks,
thanks for taking a look at these migration tools with me,
and I can't wait to see you in the next video.


Migration Exam Tips
===================

Hello, Cloud Gurus and welcome back.
In this lesson, we're going to take a look
at some of the exam tips for the migration section.
So let's start this off with 4 questions
to ask ourselves in the exam.
The first question is where are we going?
Which region in AWS?
What does the architecture need to look like in AWS?
And do we need to be able to look back
at any point in time at that physical data center,
or are we just plowing full steam ahead
to migrate everything?
The second question is how do we get there?
Do we take everything all at once?
Is this a slow but steady migration?
Or do we want to set up some sort
of permanent hybrid solution?
Do we do it all at once?
That's a really important question
because all it wants is simpler,
but it might have downtime.
In fact, it probably will have downtime.
So we need to watch for questions
and make sure that we're answering them
and keeping this in mind.
Are we meeting that SLA?
Are we meeting that uptime?
Is this a partial migration?
Am I moving everything besides Active Directory,
everything besides the database?
You're going to have to read through the question
and based on what's presented, craft a solution
that really answers all 4 of these questions.
Now, let's start off with some tips
for the Snow Family.
Now, remember with Snowball,
it's really based on how much data we're moving.
Snowballs are really perfect for terabytes.
We're talking 20, 30, 40 TB of data
that we have to migrate.
If we're thinking smaller than that, something like gigs,
well, we might want to be moving that just
using our standard internet connection.
Now, Snowcone and Snowmobile won't really be on the exam.
Now, don't quote me on that
because just as I say that, it might pop up
as a distractor, but I would be very skeptical
of you having an in-depth question
or really any question
that utilizes Snowcone or Snowmobile
for anything more than a distractor.
Focus your study time, focus your review time, on Snowball.
When do we use Snowball?
Well, the answer is in situations
where you have slow internet,
or in situations where it has to be secure,
or both of those.
Snowball is going to be faster than just migrating data
over your standard ISP if that ISP is rather slow.
Keep in mind that it does encrypt all
of your data by default.
Now, the general turnaround time
that we think about on the exam is around a week.
Alright, let's see some more tips here.
This time for Storage Gateway.
The first one is for hybrid architecture.
Anytime we think about hybrid architecture,
think: Which version of Storage Gateway
could complement this?
Is it going to be File Gateway, Tape Gateway,
or Volume Gateway?
Are you out of space?
This is a common exam question.
Your physical file server in your data center
or in your office just doesn't have enough storage.
File Gateway to the rescue.
Remember, we can keep all of our data backed up
and keep that cached copy, the most used data, on-prem.
And then Storage Gateway--it's a VM.
This is good to know
because you'll see questions that talk about setting this up
at a very high level.
Just make sure that you include hey,
we have to have a VM installed on-prem.
Now, AWS does sell a physical appliance
if you do google it,
but that's really just a set of physical architecture
running the pre-installed VM.
Let's take a quick look at DataSync and Transfer Family.
So remember, DataSync is that agent-based solution,
and if you're presented with a question on the exam
that compares DataSync to Storage Gateway,
well, remember, DataSync is really for one-time migrations
and it's really just for file shares.
So it has kind of a smaller use case here.
I wouldn't expect to see a question
that really compares and contrasts the 2.
So just keep that one-time in mind
when you're talking about DataSync.
EFS and FSx are both viable locations for DataSync
to dump stuff on.
Now, it's a little bit more elegant than that,
but it's really about taking that file share
from on-prem and migrating it to AWS.
And Transfer Family, remember,
this is that awkward-looking family
that has that superpower of FTP protocols:
FTPS, SFTP, and for inside of the VPC only, FTP.
Basically tricking those legacy applications
to be able to read and write from S3 and EFS.
And lastly, let's take a look at Migration Hub.
So Migration Hub, it's our organizational tool
that allows us to organize things.
That's what we really need to know for the exam,
just at the high level that allows us to discover
what's going on and then track everything
as the migration happens.
But the heavy lifting comes from its 2 buddies.
The first one is DMS, the Database Migration Service.
And this is our go-to tool
for anything database migration related.
It's also good to point out
that it has that schema conversion tool,
which allows us to migrate Oracle and SQL Servers
into any other database engine
that has ever existed and namely for the exam,
that's going to be Aurora.
And then lastly here,
do you have VMs that you need to move,
and you don't want to recreate everything?
SMS, or the Server Migration Service,
is going to be our tool to help us migrate off
of our data center into AWS.
It can take those VMware VMs
and make the transition very simple
by creating that AMI and placing it in our account.
Alright, folks, well that wraps up the migration section.
Thanks for going through all of these services with me,
and I can't wait to see you in the next one.




